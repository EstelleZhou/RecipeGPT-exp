{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 8.11 Âµs\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "%load_ext autotime\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "\n",
    "# if cannot import the modules, add the parent directory to system path might help\n",
    "\n",
    "import os, tqdm, sys\n",
    "parent_dir = os.path.abspath(os.getcwd()+'/..')+'/'\n",
    "sys.path.append(parent_dir)\n",
    "\n",
    "from utils.save import make_dir, save_pickle, load_pickle, save\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import tqdm\n",
    "import json\n",
    "\n",
    "import random\n",
    "random_seed = 2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load data, usually takes 2 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1min 57s\n"
     ]
    }
   ],
   "source": [
    "data = load_pickle('../big_data/recipe1M_ny.pickle')\n",
    "layer1 = json.load(open('/data/yueliu/RecipeAnalytics_201906/raw_data/recipe1M/layer1.json','r'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "use rule-based methods to clean the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 37.6 ms\n"
     ]
    }
   ],
   "source": [
    "start_with = ['tbsp','pkt','g','tsp','x','cups','oz','mrs','can',\n",
    "              'lb', 'pkg','tbsp','lbs','qt','lrg','grams','sm',\n",
    "              'cans','bottle','and','cubes','o',',','handful',\n",
    "              'container','t','bag',\n",
    "              'gram','jar','c','lg','ml','ounces','ounce','box']\n",
    "\n",
    "remove = ['%s.' %str(i) for i in range(30)]\n",
    "\n",
    "def clean_line(line):\n",
    "    '''\n",
    "    Args:\n",
    "        line: a string, such as food name, sentences...\n",
    "    '''\n",
    "    assert type(line) == str\n",
    "    \n",
    "    # all lowercase\n",
    "    line = line.lower()\n",
    "    \n",
    "    # only reserve number and alphabets\n",
    "    line = re.sub(r'[^a-z0-9+()-/?!.,]', ' ', line)\n",
    "    \n",
    "    # replace things in brace\n",
    "    line = re.sub(r'\\([^)]*\\)', '', line)\n",
    "    \n",
    "    # remove extra spaces\n",
    "    line = re.sub(' +',' ',line).strip()\n",
    "    \n",
    "    line = line.replace(' .', '.')\n",
    "    line = line.replace(' !', '!')\n",
    "    line = line.replace(')', '')\n",
    "    line = line.replace('*', '')\n",
    "    line = line.replace('..', '.')\n",
    "    \n",
    "    return line\n",
    "\n",
    "def clean_prefix(ingr):\n",
    "    cleaned = []\n",
    "    for ans in ingr:\n",
    "        # strip\n",
    "        ans = re.sub(' +',' ',ans).strip()\n",
    "        # remove number\n",
    "        ans = re.sub(r'\\d+', '', ans)\n",
    "        # remove period\n",
    "        ans = ans.replace('.', '')\n",
    "        \n",
    "        # remove prefixes\n",
    "        for prefix in start_with:\n",
    "            ans = re.sub('^'+prefix+'\\s', '', ans)\n",
    "        # strip again\n",
    "        ans = re.sub(' +',' ',ans).strip()\n",
    "        \n",
    "        if ans:\n",
    "            cleaned.append(ans)\n",
    "            \n",
    "    return cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this processs usually takes <1 hr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1029720it [41:53, 409.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 41min 53s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "lst_undetectable = []\n",
    "new_data = []\n",
    "\n",
    "for i, v in tqdm.tqdm(enumerate(data)):\n",
    "    '''\n",
    "    1. dealing with undetectable cases\n",
    "    '''\n",
    "    ingr = []\n",
    "    for ny_full_ingredients in v['ny_full_ingredients']:\n",
    "        if 'half and half' in ny_full_ingredients['input']:\n",
    "            ingr.append('half and half')\n",
    "        elif 'purpose flour' in ny_full_ingredients['input']:\n",
    "            ingr.append('all purpose flour')\n",
    "            \n",
    "        elif type(ny_full_ingredients['name'])==float:\n",
    "            ans = ''\n",
    "            for word in ['salt', 'sugar', 'oil','mustard','water',\n",
    "                         'steak','nuts','butter','garnish','ketchup',\n",
    "                         'milk','mayonnaise','pepper','cumin', 'rice',\n",
    "                         'seasoning','grated parmesan','raisin','olive oil',\n",
    "                         'stuffing mix', 'sauce','syrup','mushroom soup',\n",
    "                         'white sugar','brown sugar',\n",
    "                         'chopped onions','sour cream','lean ground beef','tortilla',\n",
    "                         'cayenne','paprika','corn', 'egg yolks', 'egg whites'\n",
    "                         'condensed milk',\n",
    "                         'crumb crust','jell o vanilla flavor instant pudding']:\n",
    "                if word in ny_full_ingredients['input']:\n",
    "                    ans = word\n",
    "                    \n",
    "            if ',' in ny_full_ingredients['input'] and not ans:\n",
    "                ans = ny_full_ingredients['input'].split(',')[0]\n",
    "                ans = ans if ans.count(' ') ==0 else ''\n",
    "            \n",
    "            if ans:\n",
    "                ingr.append(ans)\n",
    "            else:\n",
    "                lst_undetectable.append(ny_full_ingredients['input'])\n",
    "        \n",
    "        elif 'recipe' not in ny_full_ingredients['name']:\n",
    "            ans = ny_full_ingredients['name']\n",
    "            ingr.append(ans)\n",
    "    '''\n",
    "    2. cleaning instruction\n",
    "    '''\n",
    "    instr = ''\n",
    "    # drop numbered list\n",
    "    instr = [line['text'] for line in layer1[i]['instructions'] if line['text'] not in remove]\n",
    "    instr = [line[:-2] if line[-2:] in remove else line for line in instr]\n",
    "    instr = [line[2:] if line[:2] in remove else line for line in instr]\n",
    "    instr = [line for line in instr if line]\n",
    "    # add period for certain sentences\n",
    "    instr = [line+'.' if line[-1] not in ['!', '.', ';',','] else line for line in instr]\n",
    "    # clean braces\n",
    "    instr = ' '.join(instr)\n",
    "    instr = clean_line(instr)\n",
    "    \n",
    "    tit = clean_line(v['title'])\n",
    "    if (len(ingr) == len(v['ny_full_ingredients'])) and instr.count('.')+ instr.count('!') >=2:\n",
    "        '''\n",
    "        3. cleaning mistakes of ny-times-parser\n",
    "        '''\n",
    "        ingr = clean_prefix(ingr)\n",
    "        ingr = clean_prefix(ingr)\n",
    "        ingr = list(set(ingr))\n",
    "        \n",
    "        if len(ingr)>=2:\n",
    "            recipe = {'ingredients':ingr, 'title':v['title'], 'instructions': instr, 'recipe1m_idx': i}\n",
    "            new_data.append(recipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_pickle(obj = new_data, filename='../big_data/data_1209.pickle',overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8833294487821932"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 16.8 s\n"
     ]
    }
   ],
   "source": [
    "data = new_data\n",
    "len(data)/len(layer1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make data loading faster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/yueliu/RecipeAnalytics_201906\n",
      "time: 29.4 ms\n"
     ]
    }
   ],
   "source": [
    "cd /data/yueliu/RecipeAnalytics_201906"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved gpt-2/src/path.py\n",
      "time: 27.3 ms\n"
     ]
    }
   ],
   "source": [
    "import os, importlib\n",
    "save = importlib.import_module(\"gpt-2.src.save\")\n",
    "to_write = \"path = '/data/yueliu/RecipeAnalytics_201906/gpt-2/'\"+'\\n'+\"path_to_model = path + 'models/'\"\n",
    "save.save('gpt-2/src/path.py', to_write, overwrite = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/yueliu/RecipeAnalytics_201906/gpt-2\n",
      "time: 34.6 ms\n"
     ]
    }
   ],
   "source": [
    "cd gpt-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 203 ms\n"
     ]
    }
   ],
   "source": [
    "from src import encoder\n",
    "enc = encoder.get_encoder('117M')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 47.8 ms\n"
     ]
    }
   ],
   "source": [
    "def txt(v, fields, mode = 'train'):\n",
    "    '''\n",
    "    fields: an order list, the last is the field to predict\n",
    "    mode: test/train, return string X, y or X+y\n",
    "    '''\n",
    "    to_write = ''\n",
    "    for field in fields:\n",
    "        if field == 'title':\n",
    "            name = v['title']\n",
    "            to_write += ' <start-title>'+name+' <end-title>'\n",
    "        if field == 'ingredients':\n",
    "            ingredients = v['ingredients']\n",
    "            to_write += ' <start-ingredients>'+'$'.join(ingredients)+'$ <end-ingredients>'\n",
    "        if field == 'directions':\n",
    "            directions = v['instructions']\n",
    "            to_write += ' <start-directions>'+ directions +' <end-directions>'\n",
    "            \n",
    "    if mode == 'train':\n",
    "        return to_write\n",
    "                                                     \n",
    "    elif mode == 'test':\n",
    "        field_to_predict = '<start-%s>'%fields[-1]\n",
    "        to_X, to_y = to_write.split(field_to_predict)\n",
    "        return to_X+field_to_predict, to_y\n",
    "\n",
    "class to_gpt2:\n",
    "    def __init__(self, data, ls = None):\n",
    "        if not ls:\n",
    "            ls = list(range(len(data)))\n",
    "        random.seed(random_seed)\n",
    "        random.shuffle(ls)\n",
    "        self.ls_test = ls[:4000] \n",
    "        self.ls_val = ls[4000:8000]\n",
    "        self.ls_train = ls[8000:]\n",
    "        self.data = data\n",
    "        \n",
    "    def train(self, ls, filename, overwrite = False, is_val = False):\n",
    "        to_write = ''\n",
    "        for i, v in tqdm.tqdm(enumerate(self.data)):\n",
    "            if i in ls:      \n",
    "                self.save(filename+'%d'%(i)+'d.txt', txt(v, ['title','ingredients','directions']), overwrite)\n",
    "                self.save(filename+'%d'%(i)+'i.txt', txt(v, ['title','directions','ingredients']), overwrite)\n",
    "                self.save(filename+'%d'%(i)+'t.txt', txt(v, ['ingredients','directions','title']), overwrite)\n",
    "                \n",
    "    def train_reduce(self, ls, filename, overwrite = False, is_val = False):\n",
    "        '''\n",
    "        corresponding to the changed gpt-2:\n",
    "        it reduces the memory usuage by changing the sampling of training data\n",
    "        '''\n",
    "        to_write = ''\n",
    "        for i, v in tqdm.tqdm(enumerate(self.data)):\n",
    "            if i in ls:      \n",
    "                self.save(filename+'%d'%(i)+'d.txt', txt(v, ['title','ingredients','directions']), overwrite)\n",
    "                \n",
    "    def test(self, ls, filename, overwrite = False, is_val = False):\n",
    "        to_write = ''\n",
    "        for i, v in tqdm.tqdm(enumerate(self.data)):\n",
    "            if i in ls:\n",
    "                self.save(filename+'X/%d'%(i)+'d.txt', txt(v, ['title','ingredients','directions'], mode = 'test')[0], overwrite)\n",
    "                self.save(filename+'X/%d'%(i)+'i.txt', txt(v, ['title','directions','ingredients'], mode = 'test')[0], overwrite)\n",
    "                self.save(filename+'X/%d'%(i)+'t.txt', txt(v, ['ingredients','directions','title'], mode = 'test')[0], overwrite)\n",
    "                \n",
    "                self.save(filename+'y/%d'%(i)+'d.txt', txt(v, ['title','ingredients','directions'], mode = 'test')[1], overwrite)\n",
    "                self.save(filename+'y/%d'%(i)+'i.txt', txt(v, ['title','directions','ingredients'], mode = 'test')[1], overwrite)\n",
    "                self.save(filename+'y/%d'%(i)+'t.txt', txt(v, ['ingredients','directions','title'], mode = 'test')[1], overwrite)\n",
    "        \n",
    "    def save(self, filename, to_write, overwrite = False):\n",
    "        make_dir(filename)\n",
    "        if os.path.isfile(filename) == True and overwrite == False:\n",
    "            print('already exists'+filename)\n",
    "        else:    \n",
    "            with open(filename,'w') as f:\n",
    "                f.write('%s' % to_write)\n",
    "                \n",
    "    def make_chunk(self, ls, filename, tag, overwrite = False):\n",
    "        chunk = []\n",
    "        for i, v in tqdm.tqdm(enumerate(self.data)):\n",
    "            if i in ls:      \n",
    "                chunk.append(self.encode_recipe(self.data[i]))\n",
    "        make_dir(filename)\n",
    "        print(filename+tag)\n",
    "        save_pickle(filename+tag, chunk)\n",
    "        \n",
    "    def encode_recipe(self,recipe):\n",
    "        return enc.encode(txt(recipe, ['title','ingredients','directions']))\n",
    "    \n",
    "    def fast_chunk(self, filename, overwrite = False):\n",
    "        self.make_chunk(self.ls_train, filename, tag = 'chunk.train')\n",
    "        self.make_chunk(self.ls_val, filename, tag = 'chunk.val')\n",
    "        self.make_chunk(self.ls_test, filename, tag = 'chunk.test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.76 s\n"
     ]
    }
   ],
   "source": [
    "model = to_gpt2(data)\n",
    "filename = '../to_gpt2/recipe1M_1209/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "445it [00:00, 4449.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "make dir\n",
      "make dir\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "909582it [03:20, 4542.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 3min 20s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model.test(model.ls_val, filename+'val/', overwrite = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "usually takes 15 hr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "909582it [14:13:31, 17.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../to_gpt2/recipe1M_1209/chunk.train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "909582it [03:25, 4416.55it/s]\n",
      "440it [00:00, 4396.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../to_gpt2/recipe1M_1209/chunk.val\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "909582it [03:22, 4500.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../to_gpt2/recipe1M_1209/chunk.test\n",
      "time: 14h 20min 30s\n"
     ]
    }
   ],
   "source": [
    "model.fast_chunk(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "examine the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 17.5 s\n"
     ]
    }
   ],
   "source": [
    "X = load_pickle('../../to_gpt2/recipe1M_1209/chunk.train')\n",
    "distr_length = pd.Series([len(recipe) for recipe in X])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max 3251 min 47\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f1bcec6b3c8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.03 s\n"
     ]
    }
   ],
   "source": [
    "print('max',distr_length.max(),'min', distr_length.min())\n",
    "display(distr_length.hist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9788283262088197"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 286 ms\n"
     ]
    }
   ],
   "source": [
    "1 - sum(distr_length>512)/ len(distr_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(213.47197592675985, 188.0, 110.44966961207903)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 71.4 ms\n"
     ]
    }
   ],
   "source": [
    "distr_length.mean(), distr_length.median(), distr_length.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
