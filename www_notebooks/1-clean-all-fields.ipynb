{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In 0-clean-ingredients notebook, we use NYtimes-parser to process all the ingredients in the data. Here, we use many rule-based approaches to clean all the fields (e.g. title, ingredients, instructions) we want to use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though the code is a little bit messy, this processing can be finished in 30min."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dependency import parent_dir\n",
    "from common.basics import *\n",
    "from common.save import save_pickle, load_pickle "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load two files: <br>\n",
    "layer1 is provided by http://pic2recipe.csail.mit.edu/; <br>\n",
    "recipe1M_ny.pickle is from  0-clean-recipe1m.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer1 = json.load(open('../big_data/layer1.json','r'))\n",
    "ny_data = load_pickle('../big_data/recipe1M_ny.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define rule-based methods to clean the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_with = ['tbsp','pkt','g','tsp','x','cups','oz','mrs','can',\n",
    "              'lb', 'pkg','tbsp','lbs','qt','lrg','grams','sm',\n",
    "              'cans','bottle','and','cubes','o',',','handful',\n",
    "              'container','t','bag','gram','jar','c','lg','ml','ounces','ounce','box']\n",
    "\n",
    "remove = ['%s.' %str(i) for i in range(30)]\n",
    "\n",
    "def clean_line(line):\n",
    "    '''\n",
    "    Args:\n",
    "        line: a string, such as food name, sentences...\n",
    "    '''\n",
    "    assert type(line) == str\n",
    "    \n",
    "    # all lowercase\n",
    "    line = line.lower()\n",
    "    line = line.replace(' .', '.')\n",
    "    line = line.replace(' !', '!')\n",
    "    line = line.replace(')', '')\n",
    "    line = line.replace('*', '')\n",
    "    line = line.replace('..', '.')\n",
    "    line = line.replace(' - ', '')\n",
    "    \n",
    "    # only reserve number and alphabets\n",
    "    line = re.sub(r\"[^a-z0-9+()-/?&'!.,]\", ' ', line)\n",
    "    \n",
    "    # replace things in brace\n",
    "    line = re.sub(r'\\([^)]*\\)', '', line)\n",
    "    \n",
    "    # remove extra spaces\n",
    "    line = re.sub(' +',' ',line).strip()\n",
    "    return line\n",
    "\n",
    "def clean_prefix(ingr):\n",
    "    cleaned = []\n",
    "    for ans in ingr:\n",
    "        \n",
    "        # strip\n",
    "        ans = re.sub(' +',' ',ans).strip()\n",
    "        \n",
    "        # remove number\n",
    "        ans = re.sub(r'\\d+', '', ans)\n",
    "        \n",
    "        # remove period\n",
    "        ans = ans.replace('.', '')\n",
    "        \n",
    "        # remove prefixes\n",
    "        for prefix in start_with:\n",
    "            ans = re.sub('^'+prefix+'\\s', '', ans)\n",
    "            \n",
    "        # strip again\n",
    "        ans = re.sub(' +',' ',ans).strip()\n",
    "\n",
    "        if ans:\n",
    "            cleaned.append(ans)\n",
    "            \n",
    "    return cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1029720it [20:28, 837.96it/s]\n"
     ]
    }
   ],
   "source": [
    "lst_undetectable = []\n",
    "new_data = []\n",
    "\n",
    "for i, v in tqdm.tqdm(enumerate(ny_data)):\n",
    "    '''\n",
    "    1. dealing with undetectable cases\n",
    "    '''\n",
    "    ingr = []\n",
    "    for ny_full_ingredients in v['ny_full_ingredients']:\n",
    "        if 'half and half' in ny_full_ingredients['input']:\n",
    "            ingr.append('half and half')\n",
    "        elif 'purpose flour' in ny_full_ingredients['input']:\n",
    "            ingr.append('all purpose flour')\n",
    "            \n",
    "        elif type(ny_full_ingredients['name'])==float:\n",
    "            ans = ''\n",
    "            for word in ['salt', 'sugar', 'oil','mustard','water',\n",
    "                         'steak','nuts','butter','garnish','ketchup',\n",
    "                         'milk','mayonnaise','pepper','cumin', 'rice',\n",
    "                         'seasoning','grated parmesan','raisin','olive oil',\n",
    "                         'stuffing mix', 'sauce','syrup','mushroom soup',\n",
    "                         'white sugar','brown sugar',\n",
    "                         'chopped onions','sour cream','lean ground beef','tortilla',\n",
    "                         'cayenne','paprika','corn', 'egg yolks', 'egg whites'\n",
    "                         'condensed milk',\n",
    "                         'crumb crust','jell o vanilla flavor instant pudding']:\n",
    "                if word in ny_full_ingredients['input']:\n",
    "                    ans = word\n",
    "                    \n",
    "            if ',' in ny_full_ingredients['input'] and not ans:\n",
    "                ans = ny_full_ingredients['input'].split(',')[0]\n",
    "                ans = ans if ans.count(' ') ==0 else ''\n",
    "            \n",
    "            if ans:\n",
    "                ingr.append(ans)\n",
    "            else:\n",
    "                lst_undetectable.append(ny_full_ingredients['input'])\n",
    "        \n",
    "        elif 'recipe' not in ny_full_ingredients['name']:\n",
    "            ans = ny_full_ingredients['name']\n",
    "            ingr.append(ans)\n",
    "    '''\n",
    "    2. cleaning instruction\n",
    "    '''\n",
    "    # drop numbered list\n",
    "    instr = ''\n",
    "    instr = [line['text'] for line in layer1[i]['instructions'] if line['text'] not in remove]\n",
    "    instr = [line[:-2] if line[-2:] in remove else line for line in instr]\n",
    "    instr = [line[2:] if line[:2] in remove else line for line in instr]\n",
    "    instr = [line for line in instr if line]\n",
    "    \n",
    "    # add period for certain sentences\n",
    "    instr = [line+'.' if line[-1] not in ['!', '.', ';',','] else line for line in instr]\n",
    "    \n",
    "    # clean braces\n",
    "    instr = ' '.join(instr)\n",
    "    instr = clean_line(instr)\n",
    "    \n",
    "    # contain calorie info\n",
    "    if 'calorie' in instr:\n",
    "        for term in ['per serving','nutrition information','servings','each serving has','per slice','calories']:\n",
    "            instr, sep, tail = instr.partition(term)\n",
    "        if not instr.endswith('.'):\n",
    "            instr, sep, tail = instr.rpartition('.')\n",
    "            instr = instr+sep\n",
    "            \n",
    "    # contain author info\n",
    "    instr, sep, tail = instr.partition('recipe from new new')\n",
    "    tit = clean_line(layer1[i]['title'])\n",
    "    \n",
    "    # long enough\n",
    "    cond1 = (len(ingr) == len(v['ny_full_ingredients']))\n",
    "    cond2 = (instr.count('.') + instr.count('!') >=2)\n",
    "    cond3 = (len(instr.split(' '))>10)\n",
    "    cond4 = (i!=8219)\n",
    "\n",
    "    '''\n",
    "    3. cleaning mistakes of ny-times-parser\n",
    "    '''   \n",
    "    if cond1 and cond2 and cond3 and cond4:\n",
    "        ingr = clean_prefix(ingr)\n",
    "        ingr = clean_prefix(ingr)\n",
    "        ingr = list(set(ingr))\n",
    "        \n",
    "        if len(ingr)>=2 and '. 1 tablespoon' not in instr and '. 2 tablespoons' not in instr:\n",
    "            \n",
    "            recipe = {'ingredients':ingr, 'title':tit, \n",
    "                      'instructions': instr, 'recipe1m_idx': i, 'url': layer1[i]['url']\n",
    "                     }\n",
    "            new_data.append(recipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save_pickle(obj = new_data, filename='../big_data/data_1218.pickle',overwrite=True)\n",
    "save_pickle(obj = new_data, filename='../big_data/data.pickle',overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "904401"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(new_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8782979839179583"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = new_data\n",
    "len(data)/len(layer1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
