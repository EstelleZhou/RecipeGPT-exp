{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 9.3 Âµs\n",
      "The autotime extension is already loaded. To reload it, use:\n",
      "  %reload_ext autotime\n",
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "exist\n",
      "time: 12.4 s\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "%load_ext autotime\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "'''\n",
    "# if cannot import the modules, add the parent directory to system path might help\n",
    "\n",
    "import os\n",
    "import sys\n",
    "parent_dir = os.path.abspath(os.getcwd()+'/..')+'/'\n",
    "sys.path.append(parent_dir)\n",
    "'''\n",
    "\n",
    "import os\n",
    "from utils.path import dir_HugeFiles\n",
    "from utils.words import make_corpus_0, clean_wordcount, get_wordcount, replace_UNK, parse_section\n",
    "from utils.preprocessing import load, preprocessing, clean_ny\n",
    "from utils.save import make_dir, save_pickle, load_pickle, auto_save_csv, print_time\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "dir_save = os.path.normpath(dir_HugeFiles+'dph/dic_20190607.pickle')\n",
    "dic = load(dir_save)\n",
    "random_seed = 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drop 46 recipes with less than 2 ingredients\n",
      "time: 139 ms\n"
     ]
    }
   ],
   "source": [
    "ls = [i for i,v in dic.items() if len(v['ingredients'])>1]\n",
    "print('drop %d recipes with less than 2 ingredients' %(len(dic)-len(ls)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "furthur drop 1026 recipes with less than 2 instructions\n",
      "time: 88.7 ms\n"
     ]
    }
   ],
   "source": [
    "ls = [i for i in ls if len(dic[i]['directions'])>1]\n",
    "print('furthur drop %d recipes with less than 2 instructions' %(len(dic)-len(ls)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 82.2 ms\n"
     ]
    }
   ],
   "source": [
    "# split data\n",
    "train_ls, test_ls = train_test_split(ls, test_size = 0.2, shuffle = True, random_state = random_seed)\n",
    "train_ls, val_ls = train_test_split(train_ls, test_size = 0.25, shuffle = True, random_state = random_seed + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/2 cup ketchup $ 1/4 cup prepared yellow mustard $ 1 cup cornflake crumbs $ 1 ( 16 ounce ) package all beef hot dogs\n",
      "\n",
      "preheat oven to 350 degrees f ( 175 degrees c ) . line a baking sheet with a sheet of aluminum foil . stir together ketchup and mustard on a plate until mixed . place the cornflake crumbs in a shallow bowl . roll each hot dog in the ketchup mixture , then roll in the cornflake crumbs to coat . place onto prepared baking sheet . bake in preheated oven until the hot dogs are hot on the inside , and crispy on the outside , 15 to 20 minutes .\n",
      "\n",
      "time: 25.6 ms\n"
     ]
    }
   ],
   "source": [
    "# show some examples\n",
    "ingredients_list = dic[0]['ingredients_list']\n",
    "to_write = ' $ '.join([' '.join(ele) for ele in ingredients_list])\n",
    "print(to_write)\n",
    "print()\n",
    "directions_list = dic[0]['directions_list']\n",
    "to_write = ' '.join([' '.join(ele) for ele in directions_list])\n",
    "print(to_write)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 32.1 ms\n"
     ]
    }
   ],
   "source": [
    "def to_opennmt(key, ls, filename, delimiter = ' $ ', overwrite = False):\n",
    "    '''\n",
    "    params: key: string, the key of data, e.g. dic[0]['ingredients_list']\n",
    "    params: ls: list, he index of recipe, from train_test_split\n",
    "    params: filename, string, the filename of txt file\n",
    "    '''\n",
    "    make_dir(filename)\n",
    "    if os.path.isfile(filename) == True and overwrite == False:\n",
    "        print('already exists'+filename)\n",
    "    else:    \n",
    "        with open(filename,'w') as f:\n",
    "            for i, v in dic.items():\n",
    "                if i in ls:\n",
    "                    to_write = delimiter.join([' '.join(ele) for ele in v[key]])\n",
    "                    f.write(\"%s\\n\" % to_write)\n",
    "        print('saved '+filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size after train test split\n",
      "[32445, 10815, 10816]\n",
      "time: 34.2 ms\n"
     ]
    }
   ],
   "source": [
    "print('size after train test split')\n",
    "print([len(ls) for ls in [train_ls, val_ls, test_ls]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 2min 14s\n"
     ]
    }
   ],
   "source": [
    "# takes about 3 min\n",
    "to_opennmt(key = 'ingredients_list', \n",
    "           ls = train_ls, \n",
    "           filename = dir_HugeFiles+'to_opennmt/train.ingred.atok'\n",
    "          )\n",
    "to_opennmt(key = 'ingredients_list', \n",
    "           ls = val_ls, \n",
    "           filename = dir_HugeFiles+'to_opennmt/val.ingred.atok'\n",
    "          )\n",
    "to_opennmt(key = 'ingredients_list', \n",
    "           ls = test_ls, \n",
    "           filename = dir_HugeFiles+'to_opennmt/test.ingred.atok'\n",
    "          )\n",
    "to_opennmt(key = 'directions_list', \n",
    "           ls = train_ls, \n",
    "           filename = dir_HugeFiles+'to_opennmt/train.instr.atok',\n",
    "           delimiter = ' '\n",
    "          )\n",
    "to_opennmt(key = 'directions_list', \n",
    "           ls = val_ls, \n",
    "           filename = dir_HugeFiles+'to_opennmt/val.instr.atok'\n",
    "          )\n",
    "to_opennmt(key = 'directions_list', \n",
    "           ls = test_ls, \n",
    "           filename = dir_HugeFiles+'to_opennmt/test.instr.atok'\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49.09452736318415\n",
      "99.99999999999997\n",
      "time: 64 ms\n"
     ]
    }
   ],
   "source": [
    "corpus =\"\"\"\n",
    "Monty Python (sometimes known as The Pythons) were a British surreal comedy group who created the sketch comedy show Monty Python's Flying Circus,\n",
    "that first aired on the BBC on October 5, 1969. Forty-five episodes were made over four series. The Python phenomenon developed from the television series\n",
    "into something larger in scope and impact, spawning touring stage shows, films, numerous albums, several books, and a stage musical.\n",
    "The group's influence on comedy has been compared to The Beatles' influence on music.\"\"\"\n",
    "import collections, nltk\n",
    "# we first tokenize the text corpus\n",
    "tokens = nltk.word_tokenize(corpus)\n",
    "\n",
    "#here you construct the unigram language model \n",
    "def unigram(tokens):    \n",
    "    model = collections.defaultdict(lambda: 0.01)\n",
    "    for f in tokens:\n",
    "        try:\n",
    "            model[f] += 1\n",
    "        except KeyError:\n",
    "            model [f] = 1\n",
    "            continue\n",
    "    N = float(sum(model.values()))\n",
    "    for word in model:\n",
    "        model[word] = model[word]/N\n",
    "    return model\n",
    "\n",
    "#computes perplexity of the unigram model on a testset  \n",
    "def perplexity(testset, model):\n",
    "    testset = testset.split()\n",
    "    perplexity = 1\n",
    "    N = 0\n",
    "    for word in testset:\n",
    "        N += 1\n",
    "        perplexity = perplexity * (1/model[word])\n",
    "    perplexity = pow(perplexity, 1/float(N)) \n",
    "    return perplexity\n",
    "\n",
    "testset1 = \"Monty\"\n",
    "testset2 = \"abracadabra gobbledygook rubbish\"\n",
    "\n",
    "model = unigram(tokens)\n",
    "print(perplexity(testset1, model))\n",
    "print(perplexity(testset2, model))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
