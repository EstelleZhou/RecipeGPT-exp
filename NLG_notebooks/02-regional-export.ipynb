{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 6.91 Âµs\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "%load_ext autotime\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "\n",
    "# if cannot import the modules, add the parent directory to system path might help\n",
    "\n",
    "import os\n",
    "import sys\n",
    "parent_dir = os.path.abspath(os.getcwd()+'/..')+'/'\n",
    "sys.path.append(parent_dir)\n",
    "\n",
    "\n",
    "import os\n",
    "from utils.path import dir_HugeFiles\n",
    "from utils.preprocessing import load\n",
    "from utils.words import make_corpus_0, get_wordcount_list\n",
    "from utils.save import make_dir, save_pickle, load_pickle, auto_save_csv, print_time\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "import random\n",
    "import re\n",
    "\n",
    "#dir_save = os.path.normpath(dir_HugeFiles+'dph/dic_20190607.pickle')\n",
    "#dic = load(dir_save)\n",
    "random_seed = 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exist\n",
      "drop 46 recipes with less than 2 ingredients\n",
      "furthur drop 1026 recipes with less than 2 instructions\n",
      "drop 0 recipes with no description\n",
      "now we are using recipe54k 54076\n",
      "time: 9.83 s\n"
     ]
    }
   ],
   "source": [
    "dic = load(dir_save = '../big_data/dic_regional_20190925.pickle')\n",
    "\n",
    "ls = [i for i,v in dic.items() if len(v['ingredients'])>1]\n",
    "print('drop %d recipes with less than 2 ingredients' %(len(dic)-len(ls)))\n",
    "ls = [i for i in ls if len(dic[i]['directions'])>1]\n",
    "print('furthur drop %d recipes with less than 2 instructions' %(len(dic)-len(ls)))\n",
    "desc = [i for i in ls if len(dic[i]['description'])<1]\n",
    "print('drop %d recipes with no description' %(len(desc)))\n",
    "print('now we are using recipe54k %d' % len(ls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 480 ms\n"
     ]
    }
   ],
   "source": [
    "styles = {'chinese':[],'general western':[],\n",
    "          'indian':[], 'italian':[],'latin american':[]}\n",
    "\n",
    "for i, v in dic.items():\n",
    "    if 'regional_tag3' in v.keys():\n",
    "        # filter out the ' recipe'\n",
    "        recipe = set([t[:-8] if t[-8:]==' recipes' else t for t in v['tags']])\n",
    "        if 'main dish' in recipe and 'desserts' in recipe:\n",
    "            dic[i]['meal_type'] = 'dual_type'\n",
    "        elif 'main dish' in recipe:\n",
    "            dic[i]['meal_type'] = 'main'\n",
    "        elif 'main dishes' in recipe:\n",
    "            dic[i]['meal_type'] = 'main'\n",
    "        elif 'desserts' in recipe:\n",
    "            dic[i]['meal_type'] = 'desserts'\n",
    "        else:\n",
    "            dic[i]['meal_type'] = 'others'\n",
    "            \n",
    "        if dic[i]['meal_type'] == 'main':\n",
    "            if v['regional_predict'] in styles.keys():\n",
    "                styles[v['regional_predict']].append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 22.5 ms\n"
     ]
    }
   ],
   "source": [
    "len(styles['chinese'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 48.8 ms\n"
     ]
    }
   ],
   "source": [
    "def reverse(text):\n",
    "    # replace things in brace\n",
    "    text = re.sub(r'\\([^)]*\\)', '', text)\n",
    "\n",
    "    # remove space before punct\n",
    "    text = re.sub(r'\\s([?.!,\"](?:\\s|$))', r'\\1', text)\n",
    "    \n",
    "    # remove consecutive spaces\n",
    "    text = re.sub(' +',' ',text).strip()\n",
    "    return text\n",
    "\n",
    "def txt(v, fields, mode = 'train'):\n",
    "    '''\n",
    "    fields: an order list, the last is the field to predict\n",
    "    mode: test/train, return string X, y or X+y\n",
    "    '''\n",
    "    to_write = ''\n",
    "    for field in fields:\n",
    "        if field == 'style':\n",
    "            to_write += ' <start-style>'+v['regional_predict']+' <end-style>'\n",
    "        if field == 'ingredients':\n",
    "            ingredients = [reverse(sent) for sent in v['ny_ingredients']['ny'] if type(sent)==str]\n",
    "            to_write += ' <start-ingredients>'+'$'.join(ingredients)+'$ <end-ingredients>'\n",
    "        if field == 'directions':\n",
    "            directions = [reverse(sent) for sent in v['directions']]\n",
    "            to_write += ' <start-directions>'+' '.join(directions)+' <end-directions>'\n",
    "                                                     \n",
    "    if mode == 'train':\n",
    "        return to_write\n",
    "                                                     \n",
    "    elif mode == 'test':\n",
    "        field_to_predict = '<start-%s>'%fields[-1]\n",
    "        to_X, to_y = to_write.split(field_to_predict)\n",
    "        return to_X + field_to_predict, to_y\n",
    "\n",
    "class to_gpt2:\n",
    "    def __init__(self, dic, styles):\n",
    "        self.random = random\n",
    "        self.random.seed(2019)\n",
    "        ls = sum([self.random.sample(ls_style,400) for i, ls_style in styles.items()], [])\n",
    "        classes = [dic[i]['regional_predict'] for i in ls]\n",
    "        ls_train, self.ls_test, class_train , __ = train_test_split(ls, classes, \n",
    "                                                         test_size = 0.2, \n",
    "                                                         random_state = random_seed, \n",
    "                                                         shuffle = True, \n",
    "                                                         stratify = classes)\n",
    "        self.ls_train, self.ls_val, _, __ = train_test_split(ls_train, \n",
    "                                                             class_train, \n",
    "                                                             test_size = 0.25, \n",
    "                                                             random_state = random_seed ,\n",
    "                                                             shuffle = True,\n",
    "                                                             stratify = class_train)\n",
    "        self.dic = dic\n",
    "    \n",
    "    def train(self, ls, filename, overwrite = False, is_val = False):\n",
    "        to_write = ''\n",
    "        for i, v in self.dic.items():\n",
    "            if i in ls: \n",
    "                self.save(filename+'%d'%(i)+'d.txt', txt(v, ['style','ingredients','directions']), overwrite)\n",
    "                \n",
    "    def test(self, ls, filename, overwrite = False, is_val = False):\n",
    "        to_write = ''\n",
    "        for i, v in self.dic.items():\n",
    "            if i in ls:\n",
    "                self.save(filename+'X/%d'%(i)+'d.txt', txt(v, ['style','ingredients','directions'], mode = 'test')[0], overwrite)                \n",
    "                self.save(filename+'y/%d'%(i)+'d.txt', txt(v, ['style','ingredients','directions'], mode = 'test')[1], overwrite)\n",
    "                self.save(filename+'Xy/%d'%(i)+'d.txt', txt(v, ['style','ingredients','directions']),overwrite)\n",
    "                \n",
    "    def fast_export(self, filename, overwrite = False):\n",
    "        self.train(self.ls_train, filename+'train/', overwrite = overwrite)\n",
    "        self.train(self.ls_val, filename+'val/',overwrite = overwrite)\n",
    "        self.test(self.ls_test, filename+'test/', overwrite = overwrite)\n",
    "    def save(self, filename, to_write, overwrite = False):\n",
    "        make_dir(filename)\n",
    "        if os.path.isfile(filename) == True and overwrite == False:\n",
    "            print('already exists'+filename)\n",
    "        else:    \n",
    "            with open(filename,'w') as f:\n",
    "                f.write('%s' % to_write)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 28.2 ms\n"
     ]
    }
   ],
   "source": [
    "model = to_gpt2(dic, styles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "make dir\n",
      "make dir\n",
      "make dir\n",
      "make dir\n",
      "make dir\n",
      "time: 4.55 s\n"
     ]
    }
   ],
   "source": [
    "model.fast_export('../../to_gpt2/recipe54k_0925/',overwrite = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### delexicalized the sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "origin: length of data: 4294\n",
      "drop some fields: length of data: 2682\n",
      "drop duplicates: length of data: 2582\n",
      "before processing: length of data: 2582\n",
      "add some rows: length of data: 2685\n",
      "drop duplicates: length of data: 2684\n",
      "time: 1.64 s\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('../big_data/food_taxonomy.txt',delimiter='\\t', header = None)\n",
    "\n",
    "def cleaning(df):\n",
    "    '''\n",
    "    eliminate the row if it contains the following non-ingredient words\n",
    "    '''\n",
    "    print('origin: length of data: %d' % len(df))\n",
    "    eliminate = ['Snack brand', 'Preparation', 'Fast food', 'Dietary Supplement', 'Dessert']\n",
    "    for i in range(2):\n",
    "        df  = df[df.apply(lambda x: x[i] not in eliminate, axis = 1)]\n",
    "    # uncased\n",
    "    for i in range(3):\n",
    "        df[i]  = df.apply(lambda x: x[i].lower(), axis = 1)\n",
    "    print('drop some fields: length of data: %d' % len(df))    \n",
    "    \n",
    "    # drop the duplicates\n",
    "    df = df[~df[2].duplicated()]\n",
    "    df = df.reset_index(drop = True)\n",
    "    print('drop duplicates: length of data: %d' % len(df))\n",
    "        \n",
    "    return df\n",
    "\n",
    "df = cleaning(df)\n",
    "additional_ingr=\\\n",
    "[\n",
    "['Condiment', 'Sweet', 'sugar'],\n",
    "['Condiment', 'Sweet', 'applesauce'],\n",
    "['Flour', 'Flour','flour'],\n",
    "['Baking powder','Baking powder','baking powder'],\n",
    "['Water','Water','water'],\n",
    "['Water','Water','iced'],\n",
    "['Water','Water','ice'],\n",
    "['Herb and spice','spices','jalapeno'],\n",
    "['Condiment','Condiment', 'oil'],\n",
    "['Beverage','Fruit juice', 'juice'],\n",
    "['Staple','Maize','enchilada'],\n",
    "['Condiment','Condiment', 'dressing mix'],\n",
    "['Condiment','Condiment', 'cheese mix'],\n",
    "['Baking powder','Baking powder','baking soda'],\n",
    "['Condiment','Condiment', 'dry mix'],\n",
    "['Condiment','Condiment','chocolate'],\n",
    "['Vegetable','Bulb and stem vegetables','tapioca'],\n",
    "['Flour', 'Flour','xanthan gum'],\n",
    "['Flour', 'Flour','starch'],\n",
    "['Egg and dairy', 'Dairy product','buttermilk'],\n",
    "['Condiment','Condiment','chili'],\n",
    "['Condiment','Condiment','chile'],\n",
    "['Condiment','Condiment','chilis'],\n",
    "['Condiment','Condiment','chiles'],\n",
    "['Condiment','Condiment','chilies'],\n",
    "['Flour', 'Flour','corn muffin mix'],\n",
    "['Beverage','Chocolate','chocolate mix'],\n",
    "['Meat','dumpling','dumpling'],\n",
    "['Meat','dumpling','wonton'],\n",
    "['Staple','Wheat','pizza dough'],\n",
    "['Staple','Wheat','dough'],\n",
    "['Condiment','Condiment', 'pizza sauce'],\n",
    "['Flour', 'Flour','yeast'],\n",
    "['Condiment','Sweet','cocoa'],\n",
    "['Staple','Maize','chip'],\n",
    "['Egg and dairy','Dairy product','ricotta'],\n",
    "['Condiment','Condiment','seasoning'],\n",
    "['Beverage','Alcohol','sherry'],\n",
    "['Staple','Rice','grain rice'], \n",
    "['Staple','Wheat','shell'],\n",
    "['Meat','Beef','fillet'],\n",
    "['Staple','Maize','cornmeal'],\n",
    "['Condiment','Condiment','seed oil'],\n",
    "['Nut and seed','Other','seed'],\n",
    "['Condiment', 'Sweet', 'sugar blend'],\n",
    "['Soup','Soup','broth'],\n",
    "['Soup','Soup','stock'],\n",
    "['Condiment', 'Sweet', 'marshmallow'],\n",
    "['Condiment', 'Dry Condiment', 'dried vegetable flakes'],\n",
    "['Condiment', 'Dry Condiment', 'dried celery flakes'],\n",
    "['Flour', 'Flour','cornstarch'],\n",
    "['Staple','Wheat','double crust'],\n",
    "['Staple','Wheat','crust'],\n",
    "['Staple','Wheat','pastry crust'],\n",
    "['Egg and dairy','Dairy product','gorgonzola'],\n",
    "['Beverage','juice','drink mix'],\n",
    "['Egg and dairy','Egg','Egg whites'],\n",
    "['Baking powder','Baking powder','baking mix'],\n",
    "['Staple','Rice','brown rice'],\n",
    "['Condiment','Condiment','five spice'],\n",
    "['Meat','Beef','tenderloin'],\n",
    "['Meat','Pork','prosciutto'],\n",
    "['Condiment', 'Sweet', 'whipped topping'],\n",
    "['Condiment', 'Sweet', 'topping'],\n",
    "['Beverage','Alcohol','cider'],\n",
    "['Meat','Shellfish','crabmeat'],\n",
    "['Condiment', 'Sweet', 'candy'],\n",
    "['Condiment', 'Sweet', 'caramel'],\n",
    "['Condiment', 'Sweet', 'molasses'],\n",
    "['Vegetable','Podded vegetables','cannellini'],\n",
    "['Vegetable','Fruits','fruit'],\n",
    "['Staple','Wheat','saltine'],\n",
    "['Condiment','Condiment', 'habanero'],\n",
    "['Beverage','Juice','jell o'],\n",
    "['Beverage','Juice','jelly'],\n",
    "['Beverage','Soft drink','carbonated beverage'],\n",
    "['Egg and dairy','Dairy product','gruyere'],\n",
    "['Vegetable','Leafy and Salad','beet'],\n",
    "['Water','Water','icing'],\n",
    "['Egg and dairy','Dairy product','parmigiano'],\n",
    "['Beverage','Alcohol','liqueur'],\n",
    "['Condiment','Condiment', 'lard'],\n",
    "['Staple','Wheat','crumb'],\n",
    "['Herb and spice','Herb','peppermint'],\n",
    "['Beverage','Alcohol','marsala'],\n",
    "['Side dish','Potatoes','hash brown'],\n",
    "['Meat','Beef','steak'],\n",
    "['Condiment','Condiment','gelatin'],\n",
    "['Meat','Beef','chuck'],\n",
    "['Egg and dairy','Dairy product','colby'],\n",
    "['Condiment', 'Sweet', 'jam'],\n",
    "['Condiment', 'Sweet', 'cool whip'],\n",
    "['Condiment', 'Sweet', 'stevia'],\n",
    "['Staple','Wheat','bran'],\n",
    "['Condiment','Condiment','pimento'],\n",
    "['Condiment','Condiment','food coloring'],\n",
    "['Meat','Meat','rib'],\n",
    "['Condiment','Condiment','shortening'],\n",
    "['Vegetable','Fruits','sweet pickles'],\n",
    "['Condiment', 'Sweet', 'white confectioner'],\n",
    "['Condiment', 'Sweet', 'confectioner'],  \n",
    "['Vegetable','Root and tuberous vegetabless','rhubarb'],\n",
    "['Condiment', 'Condiment', 'cooking spray']\n",
    "]\n",
    "\n",
    "def add_rows(df, additional_ingr):\n",
    "    add = pd.DataFrame(additional_ingr)\n",
    "    print('before processing: length of data: %d' % len(df))\n",
    "    df = pd.concat([df,add]).reset_index(drop =True)\n",
    "\n",
    "    # uncased\n",
    "    for i in range(3):\n",
    "        df[i]  = df.apply(lambda x: x[i].lower().strip(), axis = 1)\n",
    "    print('add some rows: length of data: %d' % len(df))    \n",
    "    \n",
    "    # drop the duplicates\n",
    "    df = df[~df[2].duplicated()]\n",
    "    df = df.reset_index(drop = True)\n",
    "    print('drop duplicates: length of data: %d' % len(df))\n",
    "    return df\n",
    "\n",
    "df = add_rows(df, additional_ingr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 45.7 s\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import spacy\n",
    "class delexicalization:\n",
    "    def __init__(self, df):\n",
    "        ### \n",
    "        self.spacy = spacy.load('en_core_web_lg')  # 10 sec\n",
    "        df[3] = df[2].apply(lambda x: self.lemma_ingr(x)) # 30 sec\n",
    "        self.level3 = df.groupby([3])[1].apply(list).index.tolist()\n",
    "        self.gb = df.groupby([3])[1].apply(list)\n",
    "        self.level3_set = [set(ingr.split(' ')) for ingr in self.level3]\n",
    "        self.random = random\n",
    "        self.random.seed(2019)\n",
    "        \n",
    "    def lemma_ingr(self, ingr):\n",
    "        '''\n",
    "        Args: ingr: a str\n",
    "        Return: str: the first exact match or no change\n",
    "        '''\n",
    "        assert type(ingr) == str\n",
    "        phrases_to_sentences = 'Mix the %s and water.'%(ingr)\n",
    "        doc = self.spacy(phrases_to_sentences)\n",
    "        exact_match, root_match = [],[]\n",
    "        for chunk in doc.noun_chunks:\n",
    "            if chunk.text != 'water':\n",
    "                root_lemma = [token.lemma_ for token in doc if token.text == chunk.root.text][0]\n",
    "                exact_match.append(chunk.lemma_.replace('the ',''))\n",
    "                root_match.append(root_lemma)\n",
    "        if exact_match:\n",
    "            return exact_match[0]\n",
    "        else:\n",
    "            return ingr\n",
    "        \n",
    "    def delexicalized(self, exact_ingr):\n",
    "        '''\n",
    "        input: dic[9]['ny_ingredients']['exact'][0]\n",
    "        e.g. 'onion'\n",
    "        '''\n",
    "        matched = []\n",
    "        for ingr, ingr_set in zip(self.level3, self.level3_set):\n",
    "            if len(ingr_set) == len(ingr_set & set(exact_ingr.split(' '))):\n",
    "                matched += self.gb[ingr]\n",
    "        if matched:\n",
    "            return '%s#%s' % (self.random.sample(matched, 1)[0], exact_ingr)\n",
    "        \n",
    "    def ingr(self, exact_ingr):\n",
    "        matched = self.delexicalized(exact_ingr)\n",
    "        if matched:\n",
    "            return matched\n",
    "        else:\n",
    "            return exact_ingr\n",
    "        \n",
    "    def sent(self, sent):\n",
    "        '''\n",
    "        For example\n",
    "        sent = 'Mix the porks, chicken, white sugar with the grain rice in a pot.'\n",
    "        delexicalized_sent(sent) = \n",
    "        'Mix the pork#pork, poultry#chicken, sweet#white sugar with the rice#grain rice in a pot.'\n",
    "        '''\n",
    "        output = sent\n",
    "        doc = self.spacy(sent)\n",
    "        for chunk in doc.noun_chunks:\n",
    "            food_matching = self.delexicalized(chunk.lemma_.replace('the ',''))\n",
    "            if food_matching:\n",
    "                output = output.replace(chunk.text.replace('the ',''), food_matching)\n",
    "        return output\n",
    "# takes 1 min\n",
    "food_taxonomy = delexicalization(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 33.4 ms\n"
     ]
    }
   ],
   "source": [
    "def txt(v, fields, mode = 'train'):\n",
    "    '''\n",
    "    fields: an order list, the last is the field to predict\n",
    "    mode: test/train, return string X, y or X+y\n",
    "    '''\n",
    "    to_write = ''\n",
    "    for field in fields:\n",
    "        if field == 'style':\n",
    "            to_write += ' <start-style>'+v['regional_predict']+' <end-style>'\n",
    "        if field == 'ingredients':\n",
    "            ingredients = [food_taxonomy.ingr(reverse(sent)) \\\n",
    "                           for sent in v['ny_ingredients']['exact'] if type(sent)==str]\n",
    "            \n",
    "            to_write += ' <start-ingredients>'+'$'.join(ingredients)+'$ <end-ingredients>'\n",
    "        if field == 'directions':\n",
    "            directions = [food_taxonomy.sent(reverse(sent)) for sent in v['directions']]\n",
    "            to_write += ' <start-directions>'+' '.join(directions)+' <end-directions>'\n",
    "                                                     \n",
    "    if mode == 'train':\n",
    "        return to_write\n",
    "                                                     \n",
    "    elif mode == 'test':\n",
    "        field_to_predict = '<start-%s>'%fields[-1]\n",
    "        to_X, to_y = to_write.split(field_to_predict)\n",
    "        return to_X + field_to_predict, to_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 47.8 ms\n"
     ]
    }
   ],
   "source": [
    "model = to_gpt2(dic, styles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "make dir\n",
      "make dir\n",
      "make dir\n",
      "make dir\n",
      "time: 10min 5s\n"
     ]
    }
   ],
   "source": [
    "model.fast_export('../../to_gpt2/recipe54k_0926/',overwrite = True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
