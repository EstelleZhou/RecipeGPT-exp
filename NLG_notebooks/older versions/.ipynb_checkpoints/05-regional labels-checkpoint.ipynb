{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 5.01 Âµs\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "%load_ext autotime\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "\n",
    "# if cannot import the modules, add the parent directory to system path might help\n",
    "\n",
    "import os\n",
    "import sys\n",
    "parent_dir = os.path.abspath(os.getcwd()+'/..')+'/'\n",
    "sys.path.append(parent_dir)\n",
    "\n",
    "\n",
    "import os\n",
    "from utils.path import dir_HugeFiles\n",
    "from utils.preprocessing import load\n",
    "from utils.words import make_corpus_0, get_wordcount_list\n",
    "from utils.save import make_dir, save_pickle, load_pickle, auto_save_csv, print_time\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#dir_save = os.path.normpath(dir_HugeFiles+'dph/dic_20190607.pickle')\n",
    "#dic = load(dir_save)\n",
    "random_seed = 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic = load(dir_save = '../big_data/dic_20190830.pickle')\n",
    "\n",
    "ls = [i for i,v in dic.items() if len(v['ingredients'])>1]\n",
    "print('drop %d recipes with less than 2 ingredients' %(len(dic)-len(ls)))\n",
    "ls = [i for i in ls if len(dic[i]['directions'])>1]\n",
    "print('furthur drop %d recipes with less than 2 instructions' %(len(dic)-len(ls)))\n",
    "desc = [i for i in ls if len(dic[i]['description'])<1]\n",
    "print('drop %d recipes with no description' %(len(desc)))\n",
    "print('now we are using recipe54k %d' % len(ls))\n",
    "\n",
    "corpus_llist, corpus_list, corpus = make_corpus_0(dic, ['name','ingredients','directions'], ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1min 28s\n"
     ]
    }
   ],
   "source": [
    "X, knowns = get_wordcount_list(corpus_list, drop = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1min 20s\n"
     ]
    }
   ],
   "source": [
    "def add_UNK(string, key):\n",
    "    listt = string.split(' ')\n",
    "    listt = [l if l in knowns else 'UNK' for l in listt]\n",
    "    return listt\n",
    "def rm_UNK(string, key):\n",
    "    listt = string.split(' ')\n",
    "    listt = [l for l in listt if l in knowns]\n",
    "    return listt\n",
    "\n",
    "for i, v in dic.items():\n",
    "    if i in ls:\n",
    "        for key in ['name','ingredients','directions']:\n",
    "            dic[i][key+'_UNK'] = [rm_UNK(string, key) for string in v[key]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 24 s\n"
     ]
    }
   ],
   "source": [
    "stats = []\n",
    "# find world cuisine\n",
    "world = set(['all world cuisine', 'world cuisine'])\n",
    "# check the region tag\n",
    "region = set(['latin american','european', 'asian', 'u . s .', 'african', 'australian and new zealander'])\n",
    "for i, v in dic.items():\n",
    "    if i in ls:\n",
    "        # filter out the ' recipe'\n",
    "        recipe = set([t[:-8] if t[-8:]==' recipes' else t for t in v['tags']])\n",
    "\n",
    "        # if not world cusine\n",
    "        if len( recipe & world ) == 0:\n",
    "            regional_tag = 'unknown'\n",
    "        else:\n",
    "            check_region = recipe & region\n",
    "            # if region not in the list\n",
    "            if len(check_region) == 0:\n",
    "                if 'italian' in recipe:\n",
    "                    regional_tag = 'european'\n",
    "                elif 'mexican' in recipe:\n",
    "                    regional_tag = 'latin american'\n",
    "                elif 'canadian' in recipe:\n",
    "                    regional_tag = 'north american'\n",
    "                elif 'middle eastern' in recipe:\n",
    "                    regional_tag = 'asian'\n",
    "                elif 'indian' in recipe:\n",
    "                    regional_tag = 'asian'\n",
    "                else:\n",
    "                    regional_tag = 'unknown_region'\n",
    "            # exact match\n",
    "            elif len(check_region) == 1: \n",
    "                regional_tag = list(check_region)[0]\n",
    "                if regional_tag == 'u . s .':\n",
    "                    regional_tag = 'north american'\n",
    "            # dual region\n",
    "            else:\n",
    "                regional_tag = 'dual_region'\n",
    "        \n",
    "        dic[i]['regional_tag'] = regional_tag\n",
    "        stats.append(regional_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'unknown': 38098,\n",
       "         'european': 5059,\n",
       "         'latin american': 2590,\n",
       "         'dual_region': 447,\n",
       "         'north american': 3795,\n",
       "         'asian': 3299,\n",
       "         'australian and new zealander': 328,\n",
       "         'african': 205,\n",
       "         'unknown_region': 255})"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 26.1 ms\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "Counter(stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = {'european': 0, 'latin american': 1, 'north american': 2,'asian': 3,'australian and new zealander': 4, 'african': 5}\n",
    "ls_region = [i for i in ls if not 'unknown' in dic[i]['regional_tag'] and not 'dual_region' in dic[i]['regional_tag']]\n",
    "reviews = [v['name_UNK']+v['ingredients_UNK']+v['directions_UNK'] for i, v in dic.items() if i in ls_region]\n",
    "labels = [classes[v['regional_tag']] for i, v in dic.items() if i in ls_region]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" Because the performance of HAAT is similar to BoW, I will use BoW \\nclasses = {'european': 0, 'latin american': 1, 'north american': 2,'asian': 3,'australian and new zealander': 4, 'african': 5}\\nls_region = [i for i in ls if not 'unknown' in dic[i]['regional_tag'] and not 'dual_region' in dic[i]['regional_tag']]\\nreviews = [v['name_UNK']+v['ingredients_UNK']+v['directions_UNK'] for i, v in dic.items() if i in ls_region]\\nlabels = [classes[v['regional_tag']] for i, v in dic.items() if i in ls_region]\\nfull_datasets = reviews, labels\\nsave_pickle(dir_HugeFiles+'to_HATT/'+'regional_20190717.pickle', full_datasets, overwrite = True)\\n\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 21.2 ms\n"
     ]
    }
   ],
   "source": [
    "''' Because the performance of HAAT is similar to BoW, I will use BoW \n",
    "\n",
    "full_datasets = reviews, labels\n",
    "save_pickle(dir_HugeFiles+'to_HATT/'+'regional_20190717.pickle', full_datasets, overwrite = True)\n",
    "X_require_predict = [v['name_UNK']+v['ingredients_UNK']+v['directions_UNK'] for i, v in dic.items() if i in ls_region and i not in ls]\n",
    "save_pickle(dir_HugeFiles+'to_HATT/'+'X_require_predict_20190717.pickle', X_require_predict, overwrite = True)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 23.5 ms\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### use the whole recipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/helena/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/home/helena/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/home/helena/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/home/helena/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/home/helena/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/home/helena/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/home/helena/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/home/helena/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.80      0.79      1012\n",
      "           1       0.79      0.77      0.78       518\n",
      "           2       0.66      0.70      0.68       759\n",
      "           3       0.84      0.83      0.84       660\n",
      "           4       0.24      0.12      0.16        66\n",
      "           5       0.54      0.34      0.42        41\n",
      "\n",
      "   micro avg       0.76      0.76      0.76      3056\n",
      "   macro avg       0.64      0.59      0.61      3056\n",
      "weighted avg       0.75      0.76      0.75      3056\n",
      "\n",
      "time: 2min 2s\n"
     ]
    }
   ],
   "source": [
    "X, fn = get_wordcount_list([sum(recipe, []) for recipe in reviews])\n",
    "y = np.array(labels)\n",
    "config = {'fold':1}\n",
    "ss = StratifiedKFold(n_splits = 5, shuffle = True, random_state = 12)\n",
    "fold = 0\n",
    "for train_index, test_index in ss.split(X, y):\n",
    "    X_test, y_test = X[test_index], y[test_index]\n",
    "    X_train, y_train = X[train_index], y[train_index]\n",
    "    if fold == config['fold']:\n",
    "        break\n",
    "    fold += 1\n",
    "    \n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, stratify = y_train, test_size = 0.25)\n",
    "\n",
    "best_f1 = 0\n",
    "best_c = 1\n",
    "for c in [0.1, 0.5, 1,3, 10, 100, 500]:\n",
    "    model = ''\n",
    "    model = LogisticRegression(C= c, multi_class='auto', solver = 'lbfgs')\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_val)\n",
    "    f1 =  round(classification_report(y_val, y_pred, output_dict=True)['weighted avg']['f1-score'] , 3)\n",
    "    best_f1 = f1 if best_f1 < f1 else best_f1\n",
    "    best_c = c if best_f1 < f1 else best_c\n",
    "model = ''\n",
    "model = LogisticRegression(C= best_c, multi_class='auto', solver = 'lbfgs')\n",
    "model.fit(np.concatenate([X_train, X_val]), np.concatenate([y_train, y_val]))\n",
    "y_pred = model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### use the recipe title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/helena/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/helena/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/home/helena/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/home/helena/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/home/helena/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/home/helena/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/home/helena/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.86      0.81      1012\n",
      "           1       0.87      0.79      0.83       518\n",
      "           2       0.71      0.72      0.72       759\n",
      "           3       0.84      0.80      0.82       660\n",
      "           4       0.19      0.05      0.07        66\n",
      "           5       0.92      0.59      0.72        41\n",
      "\n",
      "   micro avg       0.78      0.78      0.78      3056\n",
      "   macro avg       0.72      0.63      0.66      3056\n",
      "weighted avg       0.77      0.78      0.77      3056\n",
      "\n",
      "time: 30.8 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/helena/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "X, fn = get_wordcount_list([recipe[0] for recipe in reviews])\n",
    "y = np.array(labels)\n",
    "\n",
    "config = {'fold':1}\n",
    "ss = StratifiedKFold(n_splits = 5, shuffle = True, random_state = 12)\n",
    "fold = 0\n",
    "for train_index, test_index in ss.split(X, y):\n",
    "    X_test, y_test = X[test_index], y[test_index]\n",
    "    X_train, y_train = X[train_index], y[train_index]\n",
    "    if fold == config['fold']:\n",
    "        break\n",
    "    fold += 1\n",
    "    \n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, stratify = y_train, test_size = 0.25)\n",
    "\n",
    "best_f1 = 0\n",
    "best_c = 1\n",
    "for c in [0.1, 0.5, 1, 3, 10, 100, 500]:\n",
    "    model = ''\n",
    "    model = LogisticRegression(C= c, multi_class='auto', solver = 'lbfgs')\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_val)\n",
    "    f1 =  round(classification_report(y_val, y_pred, output_dict=True)['weighted avg']['f1-score'] , 3)\n",
    "    best_f1 = f1 if best_f1 < f1 else best_f1\n",
    "    best_c = c if best_f1 < f1 else best_c\n",
    "model = ''\n",
    "model = LogisticRegression(C= best_c, multi_class='auto', solver = 'lbfgs')\n",
    "model.fit(np.concatenate([X_train, X_val]), np.concatenate([y_train, y_val]))\n",
    "y_pred = model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 18 ms\n"
     ]
    }
   ],
   "source": [
    "ls = range(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Diet and Health->Healthy->Low-Carb Recipes;Dish Type->Main Dishes->Pork Main Dishes;World Cuisine->All World Cuisine->Italian Recipes;Dish Type->Main Dishes->Italian Main Dishes;Ingredient->Pork Recipes->Ground Pork;World Cuisine->Italian->Italian Main Dishes;Ingredient->Pork Recipes->Pork Shoulder;World Cuisine->Italian->Authentic Italian Recipes;Diet and Health->Low Carb Recipes->Low-Carb Main Dishes;Ingredient->Poultry->Sausage Recipes;Cooking Style->Gourmet->Gourmet Main Dishes;Ingredient->Poultry->Pork Recipes;Dish Type->Main Dishes->Gourmet Main Dishes;World Cuisine->All World Cuisine->European Recipes;Diet and Health->Weight-Loss Recipes->Low-Carb Recipes;Ingredient->Pork Recipes->Pork Main Dishes'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 26.6 ms\n"
     ]
    }
   ],
   "source": [
    "dic[2]['sections']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'chinese', 'japanese', 'korean','indian','pakistani','bangladeshi','persian','filipino','indonesian','vietnamese','thai', 'korean', 'middle eastern','indian'\n",
    "'italian', 'german', 'uk and ireland','french','mediterranean'\n",
    "\n",
    "'canadian','indian','southern', 'u.s. recipes'\n",
    "'mexican','south american','caribbean','latin american'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 13.4 s\n"
     ]
    }
   ],
   "source": [
    "stats = []\n",
    "\n",
    "# check the region tag\n",
    "regional_tag2 = {'asian':set(['chinese', 'japanese', 'korean','indian','pakistani','bangladeshi',\n",
    "                            'persian','filipino','indonesian','vietnamese','thai', 'korean', \n",
    "                            'middle eastern','indian']),\n",
    "                 'european':set(['italian', 'german', 'uk and ireland','french']),\n",
    "                 'north american':set(['canadian','southern','u . s .']),\n",
    "                 'latin american': set(['mexican','south american','caribbean'])\n",
    "              }\n",
    "\n",
    "for i, v in dic.items():\n",
    "    if i in ls_region:\n",
    "        # filter out the ' recipe'\n",
    "        recipe = set([t[:-8] if t[-8:]==' recipes' else t for t in v['tags']])\n",
    "        if v['regional_tag'] in regional_tag2.keys():\n",
    "            compare = regional_tag2[v['regional_tag']] & recipe\n",
    "            if len(compare) == 1:\n",
    "                dic[i]['regional_tag2'] = compare.pop()\n",
    "            elif len(compare) > 1:\n",
    "                dic[i]['regional_tag2'] = 'dual_region'\n",
    "            else:\n",
    "                dic[i]['regional_tag2'] = 'unknown'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 3.31 s\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame.from_dict(dic, orient = 'index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "regional_tag    regional_tag2 \n",
       "asian           bangladeshi          3\n",
       "                chinese            454\n",
       "                dual_region         79\n",
       "                filipino            95\n",
       "                indian             637\n",
       "                indonesian          21\n",
       "                japanese           188\n",
       "                korean             136\n",
       "                middle eastern     281\n",
       "                pakistani           17\n",
       "                thai               203\n",
       "                unknown           1157\n",
       "                vietnamese          28\n",
       "european        dual_region         10\n",
       "                french             309\n",
       "                german             117\n",
       "                italian           2838\n",
       "                uk and ireland     181\n",
       "                unknown           1604\n",
       "latin american  caribbean          146\n",
       "                dual_region          2\n",
       "                mexican           2024\n",
       "                south american     107\n",
       "                unknown            311\n",
       "north american  canadian           770\n",
       "                dual_region        819\n",
       "                u . s .           2206\n",
       "Name: regional_tag2, dtype: int64"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 39.3 ms\n"
     ]
    }
   ],
   "source": [
    "df.groupby(['regional_tag','regional_tag2'])['regional_tag2'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 108 ms\n"
     ]
    }
   ],
   "source": [
    "df = df[df['regional_tag']!='unknown']\n",
    "df = df[df['regional_tag2']!='unknown']\n",
    "df = df[df['regional_tag2']!='dual_region']\n",
    "df = df[~df['regional_tag2'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 28.6 ms\n"
     ]
    }
   ],
   "source": [
    "others = ['bangladeshi','filipino','indonesian','pakistani','vietnamese']\n",
    "df['regional_tag2_'] = ['others' if v in others else v for v in df.regional_tag2]\n",
    "df['regional_tag2_'] = ['north american' if v in ['canadian','u . s .'] else v for v in df.regional_tag2_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "regional_tag    regional_tag2_\n",
       "asian           chinese           [[[chinese, chicken, salad, iii]], [[chinese, ...\n",
       "                indian            [[[easy, peasy, pea, salad]], [[besan]], [[ind...\n",
       "                japanese          [[[traditional, beef, sukiyaki]], [[japanese, ...\n",
       "                korean            [[[quick, and, easy, kimchi, salad]], [[kimchi...\n",
       "                middle eastern    [[[eggplants, in, red, sauce]], [[tahini, dres...\n",
       "                others            [[[(, chicken, cooked, in, coconut, milk, )]],...\n",
       "                thai              [[[thai, green, mango, salad]], [[shrimp, and,...\n",
       "european        french            [[[french, apple, pie, with, cream, cheese, to...\n",
       "                german            [[[german, rhubarb, streusel, cake]], [[kraut,...\n",
       "                italian           [[[italian, sausage, tuscan, style]], [[easy, ...\n",
       "                uk and ireland    [[[sweet, scottish, tablet]], [[scottish, minc...\n",
       "latin american  caribbean         [[[cuban, ropa, vieja]], [[carne, iii]], [[car...\n",
       "                mexican           [[[cream, cheese, jalapeno, dip]], [[karen, s,...\n",
       "                south american    [[[brazilian, milk, pudding]], [[brazilian, pa...\n",
       "north american  north american    [[[s, maid, sandwiches]], [[memphis, rub]], [[...\n",
       "Name: name_UNK, dtype: object"
      ]
     },
     "execution_count": 348,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 384 ms\n"
     ]
    }
   ],
   "source": [
    "gb = df.groupby(['regional_tag','regional_tag2_']).name_UNK.apply(list)\n",
    "gb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 26.5 ms\n"
     ]
    }
   ],
   "source": [
    "datasets = {'asian':[], 'european':[], 'latin american':[],'north american':[]}\n",
    "labels = {'asian':[], 'european':[], 'latin american':[],'north american':[]}\n",
    "for (tag, tag2), row in gb.iteritems():\n",
    "    datasets[tag] += row\n",
    "    labels[tag] += [tag2]*len(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 173 ms\n"
     ]
    }
   ],
   "source": [
    "X, fn = get_wordcount_list([recipe[0] for recipe in datasets['north american']])\n",
    "y = np.array(labels['north american'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 23.4 ms\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    canadian       0.69      0.31      0.43       154\n",
      "     u . s .       0.80      0.95      0.87       442\n",
      "\n",
      "   micro avg       0.79      0.79      0.79       596\n",
      "   macro avg       0.74      0.63      0.65       596\n",
      "weighted avg       0.77      0.79      0.75       596\n",
      "\n",
      "time: 618 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/helena/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/home/helena/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify = y, test_size = 0.2)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, stratify = y_train, test_size = 0.25)\n",
    "\n",
    "best_f1 = 0\n",
    "best_c = 1\n",
    "for c in [0.1, 0.5, 1, 3, 10, 100, 500]:\n",
    "    model = ''\n",
    "    model = LogisticRegression(C= c, multi_class='auto', solver = 'lbfgs')\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_val)\n",
    "    f1 =  f1_score(y_val, y_pred, average = 'micro')\n",
    "    best_f1 = f1 if best_f1 < f1 else best_f1\n",
    "    best_c = c if best_f1 < f1 else best_c\n",
    "model = ''\n",
    "model = LogisticRegression(C= best_c, multi_class='auto', solver = 'lbfgs')\n",
    "model.fit(np.concatenate([X_train, X_val]), np.concatenate([y_train, y_val]))\n",
    "y_pred = model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### everything classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 30.4 ms\n"
     ]
    }
   ],
   "source": [
    "datasets = []\n",
    "labels = []\n",
    "for (tag, tag2), row in gb.iteritems():\n",
    "    datasets += row\n",
    "    labels += [tag2]*len(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.97 s\n"
     ]
    }
   ],
   "source": [
    "X, fn = get_wordcount_list([recipe[0] for recipe in datasets])\n",
    "y = np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/helena/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/helena/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/home/helena/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                precision    recall  f1-score   support\n",
      "\n",
      "     caribbean       1.00      0.45      0.62        29\n",
      "       chinese       0.82      0.67      0.74        91\n",
      "        french       0.79      0.37      0.51        62\n",
      "        german       1.00      0.39      0.56        23\n",
      "        indian       0.82      0.70      0.76       128\n",
      "       italian       0.86      0.85      0.85       568\n",
      "      japanese       0.95      0.47      0.63        38\n",
      "        korean       1.00      0.70      0.83        27\n",
      "       mexican       0.89      0.83      0.86       405\n",
      "middle eastern       0.85      0.41      0.55        56\n",
      "north american       0.62      0.87      0.72       595\n",
      "        others       0.94      0.52      0.67        33\n",
      "south american       1.00      0.38      0.55        21\n",
      "          thai       0.97      0.71      0.82        41\n",
      "uk and ireland       0.75      0.42      0.54        36\n",
      "\n",
      "     micro avg       0.77      0.77      0.77      2153\n",
      "     macro avg       0.88      0.58      0.68      2153\n",
      "  weighted avg       0.80      0.77      0.77      2153\n",
      "\n",
      "time: 30.4 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/helena/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify = y, test_size = 0.2)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, stratify = y_train, test_size = 0.25)\n",
    "\n",
    "best_f1 = 0\n",
    "best_c = 1\n",
    "for c in [0.1, 0.5, 1, 3, 10, 100, 500]:\n",
    "    model = ''\n",
    "    model = LogisticRegression(C= c, multi_class='auto', solver = 'lbfgs', max_iter = 300)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_val)\n",
    "    f1 =  f1_score(y_val, y_pred, average = 'weighted')\n",
    "    best_f1 = f1 if best_f1 < f1 else best_f1\n",
    "    best_c = c if best_f1 < f1 else best_c\n",
    "model = ''\n",
    "model = LogisticRegression(C= best_c, multi_class='auto', solver = 'lbfgs')\n",
    "model.fit(np.concatenate([X_train, X_val]), np.concatenate([y_train, y_val]))\n",
    "y_pred = model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 36.6 ms\n"
     ]
    }
   ],
   "source": [
    "from lightgbm import LGBMClassifier"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
