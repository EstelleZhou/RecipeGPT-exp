{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 11.4 Âµs\n",
      "The autotime extension is already loaded. To reload it, use:\n",
      "  %reload_ext autotime\n",
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "time: 31.9 ms\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "%load_ext autotime\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "\n",
    "# if cannot import the modules, add the parent directory to system path might help\n",
    "\n",
    "import os\n",
    "import sys\n",
    "parent_dir = os.path.abspath(os.getcwd()+'/..')+'/'\n",
    "sys.path.append(parent_dir)\n",
    "\n",
    "\n",
    "import os\n",
    "from utils.path import dir_HugeFiles\n",
    "from utils.preprocessing_0716 import load\n",
    "from utils.words import make_corpus_0, get_wordcount_list\n",
    "from utils.save import make_dir, save_pickle, load_pickle, auto_save_csv, print_time\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#dir_save = os.path.normpath(dir_HugeFiles+'dph/dic_20190607.pickle')\n",
    "#dic = load(dir_save)\n",
    "random_seed = 2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of BLEU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU = 100.00, 100.0/100.0/100.0/100.0 (BP=1.000, ratio=1.000, hyp_len=6, ref_len=6)\n",
      "It is not advisable to publish scores from multi-bleu.perl.  The scores depend on your tokenizer, which is unlikely to be reproducible from your paper or consistent across research groups.  Instead you should detokenize then use mteval-v14.pl, which has a standard tokenization.  Scores from multi-bleu.perl can still be used for internal purposes when you have a consistent tokenizer.\n",
      "time: 160 ms\n"
     ]
    }
   ],
   "source": [
    "# !perl tools/multi-bleu.perl path_to_test_file < path_to_pred_file\n",
    "!perl tools/multi-bleu.perl data/sample_test.txt < data/sample_pred.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Format of sample_text.txt: <br>\n",
    "line\\n line\\n "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 25.4 ms\n"
     ]
    }
   ],
   "source": [
    "#read txt \n",
    "filename = dir_HugeFiles+'to_gpt2/recipe54k_0724.X_test.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 99.3 ms\n"
     ]
    }
   ],
   "source": [
    "with open(filename, 'r') as f:\n",
    "    for i, raw_text in enumerate(f):\n",
    "        if not i%2: \n",
    "            context_tokens = enc.encode(raw_text)\n",
    "            # may be useful if we want to evaluate the fields respectively\n",
    "            #last_token = raw_text.split(' ')[-1].replace('\\n','')\n",
    "            \n",
    "            generated = 0\n",
    "            for _ in range(nsamples // batch_size):\n",
    "                out = sess.run(output, feed_dict={\n",
    "                    context: [context_tokens for _ in range(batch_size)]\n",
    "                })[:, len(context_tokens):]\n",
    "                \n",
    "                for i in range(batch_size):\n",
    "                    generated += 1\n",
    "                    text = enc.decode(out[i])\n",
    "                    # not interested in the words after '<'\n",
    "                    text = text.split('<')[0] \n",
    "                    \n",
    "        # filter out \\n only sentences\n",
    "        else:\n",
    "            text = '\\n'\n",
    "        to_write += text\n",
    "    save(filename.replace('test','pred'), to_write)\n",
    "        \n",
    "def save(filename, to_write, overwrite = False):\n",
    "    make_dir(filename)\n",
    "    if os.path.isfile(filename) == True and overwrite == False:\n",
    "        print('already exists'+filename)\n",
    "    else:    \n",
    "        with open(filename,'w') as f:\n",
    "            f.write('%s' % to_write)\n",
    "        print('saved '+filename)\n",
    "        \n",
    "def make_dir(filename):\n",
    "    dir_path = os.path.dirname(filename)\n",
    "    if not os.path.exists(dir_path):\n",
    "        os.makedirs(dir_path)\n",
    "        print('make dir')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### good tutorial for perplexity\n",
    "https://stackoverflow.com/questions/33266956/nltk-package-to-estimate-the-unigram-perplexity <br>\n",
    "in short, we neeed the prediction probability of the language model to derive the perplexity, <br>\n",
    "so it must be computed while running the mode. <br> it is irrelevant to either top k or top p sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
