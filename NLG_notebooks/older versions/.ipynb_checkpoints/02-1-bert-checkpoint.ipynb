{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### please use python 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-06-12 02:10:53]: bert train_bert_multi_label.py[line:26] INFO  seed is 2018\n",
      "[2019-06-12 02:10:53]: bert train_bert_multi_label.py[line:29] INFO  starting load data from disk\n",
      "100%|██████████| 742/742 [00:00<00:00, 9787.54it/s]\n",
      "[2019-06-12 02:10:53]: bert data_transformer.py[line:83] INFO  train val split\n",
      "Merge: 742it [00:00, 1193318.09it/s]\n",
      "write data to disk: 100%|██████████| 557/557 [00:00<00:00, 202305.80it/s]\n",
      "write data to disk: 100%|██████████| 185/185 [00:00<00:00, 151168.17it/s]\n",
      "[2019-06-12 02:10:54]: bert train_bert_multi_label.py[line:76] INFO  initializing model\n",
      "[2019-06-12 02:11:06]: bert train_bert_multi_label.py[line:99] INFO  initializing callbacks\n",
      "[2019-06-12 02:11:06]: bert train_bert_multi_label.py[line:117] INFO  training model....\n",
      "[2019-06-12 02:11:12]: bert utils.py[line:43] INFO  current 2 GPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model summary info: \n",
      "-----------------------------------------------------------------------\n",
      "             Layer (type)                Input Shape         Param #\n",
      "=======================================================================\n",
      "               BertFine-1                  [-1, 512]               0\n",
      "              BertModel-2                  [-1, 512]               0\n",
      "               BertFine-3                  [-1, 512]               0\n",
      "              BertModel-4                  [-1, 512]               0\n",
      "         BertEmbeddings-5                  [-1, 512]               0\n",
      "         BertEmbeddings-6                  [-1, 512]               0\n",
      "              Embedding-7                  [-1, 512]      23,440,896\n",
      "              Embedding-8                  [-1, 512]      23,440,896\n",
      "              Embedding-9                  [-1, 512]         393,216\n",
      "             Embedding-10                  [-1, 512]         393,216\n",
      "             Embedding-11                  [-1, 512]           1,536\n",
      "             Embedding-12                  [-1, 512]           1,536\n",
      "        FusedLayerNorm-13             [-1, 512, 768]           1,536\n",
      "        FusedLayerNorm-14             [-1, 512, 768]           1,536\n",
      "               Dropout-15             [-1, 512, 768]               0\n",
      "               Dropout-16             [-1, 512, 768]               0\n",
      "           BertEncoder-17             [-1, 512, 768]               0\n",
      "             BertLayer-18             [-1, 512, 768]               0\n",
      "         BertAttention-19             [-1, 512, 768]               0\n",
      "     BertSelfAttention-20             [-1, 512, 768]               0\n",
      "                Linear-21             [-1, 512, 768]         590,592\n",
      "           BertEncoder-22             [-1, 512, 768]               0\n",
      "             BertLayer-23             [-1, 512, 768]               0\n",
      "         BertAttention-24             [-1, 512, 768]               0\n",
      "     BertSelfAttention-25             [-1, 512, 768]               0\n",
      "                Linear-26             [-1, 512, 768]         590,592\n",
      "                Linear-27             [-1, 512, 768]         590,592\n",
      "                Linear-28             [-1, 512, 768]         590,592\n",
      "                Linear-29             [-1, 512, 768]         590,592\n",
      "                Linear-30             [-1, 512, 768]         590,592\n",
      "               Dropout-31         [-1, 12, 512, 512]               0\n",
      "               Dropout-32         [-1, 12, 512, 512]               0\n",
      "        BertSelfOutput-33             [-1, 512, 768]               0\n",
      "                Linear-34             [-1, 512, 768]         590,592\n",
      "               Dropout-35             [-1, 512, 768]               0\n",
      "        BertSelfOutput-36             [-1, 512, 768]               0\n",
      "                Linear-37             [-1, 512, 768]         590,592\n",
      "        FusedLayerNorm-38             [-1, 512, 768]           1,536\n",
      "      BertIntermediate-39             [-1, 512, 768]               0\n",
      "                Linear-40             [-1, 512, 768]       2,362,368\n",
      "               Dropout-41             [-1, 512, 768]               0\n",
      "        FusedLayerNorm-42             [-1, 512, 768]           1,536\n",
      "      BertIntermediate-43             [-1, 512, 768]               0\n",
      "                Linear-44             [-1, 512, 768]       2,362,368\n",
      "            BertOutput-45            [-1, 512, 3072]               0\n",
      "                Linear-46            [-1, 512, 3072]       2,360,064\n",
      "            BertOutput-47            [-1, 512, 3072]               0\n",
      "                Linear-48            [-1, 512, 3072]       2,360,064\n",
      "               Dropout-49             [-1, 512, 768]               0\n",
      "        FusedLayerNorm-50             [-1, 512, 768]           1,536\n",
      "               Dropout-51             [-1, 512, 768]               0\n",
      "             BertLayer-52             [-1, 512, 768]               0\n",
      "         BertAttention-53             [-1, 512, 768]               0\n",
      "     BertSelfAttention-54             [-1, 512, 768]               0\n",
      "                Linear-55             [-1, 512, 768]         590,592\n",
      "        FusedLayerNorm-56             [-1, 512, 768]           1,536\n",
      "             BertLayer-57             [-1, 512, 768]               0\n",
      "         BertAttention-58             [-1, 512, 768]               0\n",
      "     BertSelfAttention-59             [-1, 512, 768]               0\n",
      "                Linear-60             [-1, 512, 768]         590,592\n",
      "                Linear-61             [-1, 512, 768]         590,592\n",
      "                Linear-62             [-1, 512, 768]         590,592\n",
      "                Linear-63             [-1, 512, 768]         590,592\n",
      "                Linear-64             [-1, 512, 768]         590,592\n",
      "               Dropout-65         [-1, 12, 512, 512]               0\n",
      "               Dropout-66         [-1, 12, 512, 512]               0\n",
      "        BertSelfOutput-67             [-1, 512, 768]               0\n",
      "                Linear-68             [-1, 512, 768]         590,592\n",
      "        BertSelfOutput-69             [-1, 512, 768]               0\n",
      "                Linear-70             [-1, 512, 768]         590,592\n",
      "               Dropout-71             [-1, 512, 768]               0\n",
      "        FusedLayerNorm-72             [-1, 512, 768]           1,536\n",
      "               Dropout-73             [-1, 512, 768]               0\n",
      "      BertIntermediate-74             [-1, 512, 768]               0\n",
      "                Linear-75             [-1, 512, 768]       2,362,368\n",
      "        FusedLayerNorm-76             [-1, 512, 768]           1,536\n",
      "      BertIntermediate-77             [-1, 512, 768]               0\n",
      "                Linear-78             [-1, 512, 768]       2,362,368\n",
      "            BertOutput-79            [-1, 512, 3072]               0\n",
      "                Linear-80            [-1, 512, 3072]       2,360,064\n",
      "            BertOutput-81            [-1, 512, 3072]               0\n",
      "                Linear-82            [-1, 512, 3072]       2,360,064\n",
      "               Dropout-83             [-1, 512, 768]               0\n",
      "               Dropout-84             [-1, 512, 768]               0\n",
      "        FusedLayerNorm-85             [-1, 512, 768]           1,536\n",
      "        FusedLayerNorm-86             [-1, 512, 768]           1,536\n",
      "             BertLayer-87             [-1, 512, 768]               0\n",
      "         BertAttention-88             [-1, 512, 768]               0\n",
      "     BertSelfAttention-89             [-1, 512, 768]               0\n",
      "                Linear-90             [-1, 512, 768]         590,592\n",
      "             BertLayer-91             [-1, 512, 768]               0\n",
      "         BertAttention-92             [-1, 512, 768]               0\n",
      "     BertSelfAttention-93             [-1, 512, 768]               0\n",
      "                Linear-94             [-1, 512, 768]         590,592\n",
      "                Linear-95             [-1, 512, 768]         590,592\n",
      "                Linear-96             [-1, 512, 768]         590,592\n",
      "                Linear-97             [-1, 512, 768]         590,592\n",
      "                Linear-98             [-1, 512, 768]         590,592\n",
      "               Dropout-99         [-1, 12, 512, 512]               0\n",
      "              Dropout-100         [-1, 12, 512, 512]               0\n",
      "       BertSelfOutput-101             [-1, 512, 768]               0\n",
      "               Linear-102             [-1, 512, 768]         590,592\n",
      "       BertSelfOutput-103             [-1, 512, 768]               0\n",
      "               Linear-104             [-1, 512, 768]         590,592\n",
      "              Dropout-105             [-1, 512, 768]               0\n",
      "              Dropout-106             [-1, 512, 768]               0\n",
      "       FusedLayerNorm-107             [-1, 512, 768]           1,536\n",
      "       FusedLayerNorm-108             [-1, 512, 768]           1,536\n",
      "     BertIntermediate-109             [-1, 512, 768]               0\n",
      "               Linear-110             [-1, 512, 768]       2,362,368\n",
      "     BertIntermediate-111             [-1, 512, 768]               0\n",
      "               Linear-112             [-1, 512, 768]       2,362,368\n",
      "           BertOutput-113            [-1, 512, 3072]               0\n",
      "               Linear-114            [-1, 512, 3072]       2,360,064\n",
      "           BertOutput-115            [-1, 512, 3072]               0\n",
      "               Linear-116            [-1, 512, 3072]       2,360,064\n",
      "              Dropout-117             [-1, 512, 768]               0\n",
      "       FusedLayerNorm-118             [-1, 512, 768]           1,536\n",
      "              Dropout-119             [-1, 512, 768]               0\n",
      "            BertLayer-120             [-1, 512, 768]               0\n",
      "        BertAttention-121             [-1, 512, 768]               0\n",
      "    BertSelfAttention-122             [-1, 512, 768]               0\n",
      "               Linear-123             [-1, 512, 768]         590,592\n",
      "       FusedLayerNorm-124             [-1, 512, 768]           1,536\n",
      "            BertLayer-125             [-1, 512, 768]               0\n",
      "        BertAttention-126             [-1, 512, 768]               0\n",
      "    BertSelfAttention-127             [-1, 512, 768]               0\n",
      "               Linear-128             [-1, 512, 768]         590,592\n",
      "               Linear-129             [-1, 512, 768]         590,592\n",
      "               Linear-130             [-1, 512, 768]         590,592\n",
      "               Linear-131             [-1, 512, 768]         590,592\n",
      "               Linear-132             [-1, 512, 768]         590,592\n",
      "              Dropout-133         [-1, 12, 512, 512]               0\n",
      "              Dropout-134         [-1, 12, 512, 512]               0\n",
      "       BertSelfOutput-135             [-1, 512, 768]               0\n",
      "               Linear-136             [-1, 512, 768]         590,592\n",
      "       BertSelfOutput-137             [-1, 512, 768]               0\n",
      "               Linear-138             [-1, 512, 768]         590,592\n",
      "              Dropout-139             [-1, 512, 768]               0\n",
      "       FusedLayerNorm-140             [-1, 512, 768]           1,536\n",
      "              Dropout-141             [-1, 512, 768]               0\n",
      "     BertIntermediate-142             [-1, 512, 768]               0\n",
      "               Linear-143             [-1, 512, 768]       2,362,368\n",
      "       FusedLayerNorm-144             [-1, 512, 768]           1,536\n",
      "     BertIntermediate-145             [-1, 512, 768]               0\n",
      "               Linear-146             [-1, 512, 768]       2,362,368\n",
      "           BertOutput-147            [-1, 512, 3072]               0\n",
      "               Linear-148            [-1, 512, 3072]       2,360,064\n",
      "           BertOutput-149            [-1, 512, 3072]               0\n",
      "               Linear-150            [-1, 512, 3072]       2,360,064\n",
      "              Dropout-151             [-1, 512, 768]               0\n",
      "              Dropout-152             [-1, 512, 768]               0\n",
      "       FusedLayerNorm-153             [-1, 512, 768]           1,536\n",
      "       FusedLayerNorm-154             [-1, 512, 768]           1,536\n",
      "            BertLayer-155             [-1, 512, 768]               0\n",
      "        BertAttention-156             [-1, 512, 768]               0\n",
      "    BertSelfAttention-157             [-1, 512, 768]               0\n",
      "               Linear-158             [-1, 512, 768]         590,592\n",
      "            BertLayer-159             [-1, 512, 768]               0\n",
      "        BertAttention-160             [-1, 512, 768]               0\n",
      "    BertSelfAttention-161             [-1, 512, 768]               0\n",
      "               Linear-162             [-1, 512, 768]         590,592\n",
      "               Linear-163             [-1, 512, 768]         590,592\n",
      "               Linear-164             [-1, 512, 768]         590,592\n",
      "               Linear-165             [-1, 512, 768]         590,592\n",
      "               Linear-166             [-1, 512, 768]         590,592\n",
      "              Dropout-167         [-1, 12, 512, 512]               0\n",
      "              Dropout-168         [-1, 12, 512, 512]               0\n",
      "       BertSelfOutput-169             [-1, 512, 768]               0\n",
      "               Linear-170             [-1, 512, 768]         590,592\n",
      "       BertSelfOutput-171             [-1, 512, 768]               0\n",
      "               Linear-172             [-1, 512, 768]         590,592\n",
      "              Dropout-173             [-1, 512, 768]               0\n",
      "              Dropout-174             [-1, 512, 768]               0\n",
      "       FusedLayerNorm-175             [-1, 512, 768]           1,536\n",
      "       FusedLayerNorm-176             [-1, 512, 768]           1,536\n",
      "     BertIntermediate-177             [-1, 512, 768]               0\n",
      "               Linear-178             [-1, 512, 768]       2,362,368\n",
      "     BertIntermediate-179             [-1, 512, 768]               0\n",
      "               Linear-180             [-1, 512, 768]       2,362,368\n",
      "           BertOutput-181            [-1, 512, 3072]               0\n",
      "               Linear-182            [-1, 512, 3072]       2,360,064\n",
      "           BertOutput-183            [-1, 512, 3072]               0\n",
      "               Linear-184            [-1, 512, 3072]       2,360,064\n",
      "              Dropout-185             [-1, 512, 768]               0\n",
      "              Dropout-186             [-1, 512, 768]               0\n",
      "       FusedLayerNorm-187             [-1, 512, 768]           1,536\n",
      "       FusedLayerNorm-188             [-1, 512, 768]           1,536\n",
      "            BertLayer-189             [-1, 512, 768]               0\n",
      "        BertAttention-190             [-1, 512, 768]               0\n",
      "    BertSelfAttention-191             [-1, 512, 768]               0\n",
      "               Linear-192             [-1, 512, 768]         590,592\n",
      "            BertLayer-193             [-1, 512, 768]               0\n",
      "        BertAttention-194             [-1, 512, 768]               0\n",
      "    BertSelfAttention-195             [-1, 512, 768]               0\n",
      "               Linear-196             [-1, 512, 768]         590,592\n",
      "               Linear-197             [-1, 512, 768]         590,592\n",
      "               Linear-198             [-1, 512, 768]         590,592\n",
      "               Linear-199             [-1, 512, 768]         590,592\n",
      "               Linear-200             [-1, 512, 768]         590,592\n",
      "              Dropout-201         [-1, 12, 512, 512]               0\n",
      "              Dropout-202         [-1, 12, 512, 512]               0\n",
      "       BertSelfOutput-203             [-1, 512, 768]               0\n",
      "               Linear-204             [-1, 512, 768]         590,592\n",
      "       BertSelfOutput-205             [-1, 512, 768]               0\n",
      "               Linear-206             [-1, 512, 768]         590,592\n",
      "              Dropout-207             [-1, 512, 768]               0\n",
      "              Dropout-208             [-1, 512, 768]               0\n",
      "       FusedLayerNorm-209             [-1, 512, 768]           1,536\n",
      "       FusedLayerNorm-210             [-1, 512, 768]           1,536\n",
      "     BertIntermediate-211             [-1, 512, 768]               0\n",
      "               Linear-212             [-1, 512, 768]       2,362,368\n",
      "     BertIntermediate-213             [-1, 512, 768]               0\n",
      "               Linear-214             [-1, 512, 768]       2,362,368\n",
      "           BertOutput-215            [-1, 512, 3072]               0\n",
      "               Linear-216            [-1, 512, 3072]       2,360,064\n",
      "           BertOutput-217            [-1, 512, 3072]               0\n",
      "               Linear-218            [-1, 512, 3072]       2,360,064\n",
      "              Dropout-219             [-1, 512, 768]               0\n",
      "              Dropout-220             [-1, 512, 768]               0\n",
      "       FusedLayerNorm-221             [-1, 512, 768]           1,536\n",
      "       FusedLayerNorm-222             [-1, 512, 768]           1,536\n",
      "            BertLayer-223             [-1, 512, 768]               0\n",
      "        BertAttention-224             [-1, 512, 768]               0\n",
      "    BertSelfAttention-225             [-1, 512, 768]               0\n",
      "               Linear-226             [-1, 512, 768]         590,592\n",
      "            BertLayer-227             [-1, 512, 768]               0\n",
      "        BertAttention-228             [-1, 512, 768]               0\n",
      "    BertSelfAttention-229             [-1, 512, 768]               0\n",
      "               Linear-230             [-1, 512, 768]         590,592\n",
      "               Linear-231             [-1, 512, 768]         590,592\n",
      "               Linear-232             [-1, 512, 768]         590,592\n",
      "               Linear-233             [-1, 512, 768]         590,592\n",
      "               Linear-234             [-1, 512, 768]         590,592\n",
      "              Dropout-235         [-1, 12, 512, 512]               0\n",
      "              Dropout-236         [-1, 12, 512, 512]               0\n",
      "       BertSelfOutput-237             [-1, 512, 768]               0\n",
      "               Linear-238             [-1, 512, 768]         590,592\n",
      "       BertSelfOutput-239             [-1, 512, 768]               0\n",
      "               Linear-240             [-1, 512, 768]         590,592\n",
      "              Dropout-241             [-1, 512, 768]               0\n",
      "       FusedLayerNorm-242             [-1, 512, 768]           1,536\n",
      "              Dropout-243             [-1, 512, 768]               0\n",
      "     BertIntermediate-244             [-1, 512, 768]               0\n",
      "               Linear-245             [-1, 512, 768]       2,362,368\n",
      "       FusedLayerNorm-246             [-1, 512, 768]           1,536\n",
      "     BertIntermediate-247             [-1, 512, 768]               0\n",
      "               Linear-248             [-1, 512, 768]       2,362,368\n",
      "           BertOutput-249            [-1, 512, 3072]               0\n",
      "               Linear-250            [-1, 512, 3072]       2,360,064\n",
      "           BertOutput-251            [-1, 512, 3072]               0\n",
      "               Linear-252            [-1, 512, 3072]       2,360,064\n",
      "              Dropout-253             [-1, 512, 768]               0\n",
      "       FusedLayerNorm-254             [-1, 512, 768]           1,536\n",
      "              Dropout-255             [-1, 512, 768]               0\n",
      "            BertLayer-256             [-1, 512, 768]               0\n",
      "        BertAttention-257             [-1, 512, 768]               0\n",
      "    BertSelfAttention-258             [-1, 512, 768]               0\n",
      "               Linear-259             [-1, 512, 768]         590,592\n",
      "       FusedLayerNorm-260             [-1, 512, 768]           1,536\n",
      "            BertLayer-261             [-1, 512, 768]               0\n",
      "        BertAttention-262             [-1, 512, 768]               0\n",
      "    BertSelfAttention-263             [-1, 512, 768]               0\n",
      "               Linear-264             [-1, 512, 768]         590,592\n",
      "               Linear-265             [-1, 512, 768]         590,592\n",
      "               Linear-266             [-1, 512, 768]         590,592\n",
      "               Linear-267             [-1, 512, 768]         590,592\n",
      "               Linear-268             [-1, 512, 768]         590,592\n",
      "              Dropout-269         [-1, 12, 512, 512]               0\n",
      "              Dropout-270         [-1, 12, 512, 512]               0\n",
      "       BertSelfOutput-271             [-1, 512, 768]               0\n",
      "               Linear-272             [-1, 512, 768]         590,592\n",
      "       BertSelfOutput-273             [-1, 512, 768]               0\n",
      "               Linear-274             [-1, 512, 768]         590,592\n",
      "              Dropout-275             [-1, 512, 768]               0\n",
      "              Dropout-276             [-1, 512, 768]               0\n",
      "       FusedLayerNorm-277             [-1, 512, 768]           1,536\n",
      "       FusedLayerNorm-278             [-1, 512, 768]           1,536\n",
      "     BertIntermediate-279             [-1, 512, 768]               0\n",
      "               Linear-280             [-1, 512, 768]       2,362,368\n",
      "     BertIntermediate-281             [-1, 512, 768]               0\n",
      "               Linear-282             [-1, 512, 768]       2,362,368\n",
      "           BertOutput-283            [-1, 512, 3072]               0\n",
      "               Linear-284            [-1, 512, 3072]       2,360,064\n",
      "           BertOutput-285            [-1, 512, 3072]               0\n",
      "               Linear-286            [-1, 512, 3072]       2,360,064\n",
      "              Dropout-287             [-1, 512, 768]               0\n",
      "              Dropout-288             [-1, 512, 768]               0\n",
      "       FusedLayerNorm-289             [-1, 512, 768]           1,536\n",
      "       FusedLayerNorm-290             [-1, 512, 768]           1,536\n",
      "            BertLayer-291             [-1, 512, 768]               0\n",
      "        BertAttention-292             [-1, 512, 768]               0\n",
      "    BertSelfAttention-293             [-1, 512, 768]               0\n",
      "               Linear-294             [-1, 512, 768]         590,592\n",
      "            BertLayer-295             [-1, 512, 768]               0\n",
      "        BertAttention-296             [-1, 512, 768]               0\n",
      "    BertSelfAttention-297             [-1, 512, 768]               0\n",
      "               Linear-298             [-1, 512, 768]         590,592\n",
      "               Linear-299             [-1, 512, 768]         590,592\n",
      "               Linear-300             [-1, 512, 768]         590,592\n",
      "               Linear-301             [-1, 512, 768]         590,592\n",
      "               Linear-302             [-1, 512, 768]         590,592\n",
      "              Dropout-303         [-1, 12, 512, 512]               0\n",
      "              Dropout-304         [-1, 12, 512, 512]               0\n",
      "       BertSelfOutput-305             [-1, 512, 768]               0\n",
      "               Linear-306             [-1, 512, 768]         590,592\n",
      "       BertSelfOutput-307             [-1, 512, 768]               0\n",
      "               Linear-308             [-1, 512, 768]         590,592\n",
      "              Dropout-309             [-1, 512, 768]               0\n",
      "              Dropout-310             [-1, 512, 768]               0\n",
      "       FusedLayerNorm-311             [-1, 512, 768]           1,536\n",
      "       FusedLayerNorm-312             [-1, 512, 768]           1,536\n",
      "     BertIntermediate-313             [-1, 512, 768]               0\n",
      "               Linear-314             [-1, 512, 768]       2,362,368\n",
      "     BertIntermediate-315             [-1, 512, 768]               0\n",
      "               Linear-316             [-1, 512, 768]       2,362,368\n",
      "           BertOutput-317            [-1, 512, 3072]               0\n",
      "               Linear-318            [-1, 512, 3072]       2,360,064\n",
      "           BertOutput-319            [-1, 512, 3072]               0\n",
      "               Linear-320            [-1, 512, 3072]       2,360,064\n",
      "              Dropout-321             [-1, 512, 768]               0\n",
      "              Dropout-322             [-1, 512, 768]               0\n",
      "       FusedLayerNorm-323             [-1, 512, 768]           1,536\n",
      "       FusedLayerNorm-324             [-1, 512, 768]           1,536\n",
      "            BertLayer-325             [-1, 512, 768]               0\n",
      "        BertAttention-326             [-1, 512, 768]               0\n",
      "    BertSelfAttention-327             [-1, 512, 768]               0\n",
      "               Linear-328             [-1, 512, 768]         590,592\n",
      "            BertLayer-329             [-1, 512, 768]               0\n",
      "        BertAttention-330             [-1, 512, 768]               0\n",
      "    BertSelfAttention-331             [-1, 512, 768]               0\n",
      "               Linear-332             [-1, 512, 768]         590,592\n",
      "               Linear-333             [-1, 512, 768]         590,592\n",
      "               Linear-334             [-1, 512, 768]         590,592\n",
      "               Linear-335             [-1, 512, 768]         590,592\n",
      "               Linear-336             [-1, 512, 768]         590,592\n",
      "              Dropout-337         [-1, 12, 512, 512]               0\n",
      "              Dropout-338         [-1, 12, 512, 512]               0\n",
      "       BertSelfOutput-339             [-1, 512, 768]               0\n",
      "               Linear-340             [-1, 512, 768]         590,592\n",
      "       BertSelfOutput-341             [-1, 512, 768]               0\n",
      "               Linear-342             [-1, 512, 768]         590,592\n",
      "              Dropout-343             [-1, 512, 768]               0\n",
      "              Dropout-344             [-1, 512, 768]               0\n",
      "       FusedLayerNorm-345             [-1, 512, 768]           1,536\n",
      "       FusedLayerNorm-346             [-1, 512, 768]           1,536\n",
      "     BertIntermediate-347             [-1, 512, 768]               0\n",
      "               Linear-348             [-1, 512, 768]       2,362,368\n",
      "     BertIntermediate-349             [-1, 512, 768]               0\n",
      "               Linear-350             [-1, 512, 768]       2,362,368\n",
      "           BertOutput-351            [-1, 512, 3072]               0\n",
      "               Linear-352            [-1, 512, 3072]       2,360,064\n",
      "           BertOutput-353            [-1, 512, 3072]               0\n",
      "               Linear-354            [-1, 512, 3072]       2,360,064\n",
      "              Dropout-355             [-1, 512, 768]               0\n",
      "       FusedLayerNorm-356             [-1, 512, 768]           1,536\n",
      "              Dropout-357             [-1, 512, 768]               0\n",
      "            BertLayer-358             [-1, 512, 768]               0\n",
      "        BertAttention-359             [-1, 512, 768]               0\n",
      "    BertSelfAttention-360             [-1, 512, 768]               0\n",
      "               Linear-361             [-1, 512, 768]         590,592\n",
      "       FusedLayerNorm-362             [-1, 512, 768]           1,536\n",
      "            BertLayer-363             [-1, 512, 768]               0\n",
      "        BertAttention-364             [-1, 512, 768]               0\n",
      "    BertSelfAttention-365             [-1, 512, 768]               0\n",
      "               Linear-366             [-1, 512, 768]         590,592\n",
      "               Linear-367             [-1, 512, 768]         590,592\n",
      "               Linear-368             [-1, 512, 768]         590,592\n",
      "               Linear-369             [-1, 512, 768]         590,592\n",
      "               Linear-370             [-1, 512, 768]         590,592\n",
      "              Dropout-371         [-1, 12, 512, 512]               0\n",
      "              Dropout-372         [-1, 12, 512, 512]               0\n",
      "       BertSelfOutput-373             [-1, 512, 768]               0\n",
      "               Linear-374             [-1, 512, 768]         590,592\n",
      "       BertSelfOutput-375             [-1, 512, 768]               0\n",
      "               Linear-376             [-1, 512, 768]         590,592\n",
      "              Dropout-377             [-1, 512, 768]               0\n",
      "              Dropout-378             [-1, 512, 768]               0\n",
      "       FusedLayerNorm-379             [-1, 512, 768]           1,536\n",
      "     BertIntermediate-380             [-1, 512, 768]               0\n",
      "               Linear-381             [-1, 512, 768]       2,362,368\n",
      "       FusedLayerNorm-382             [-1, 512, 768]           1,536\n",
      "     BertIntermediate-383             [-1, 512, 768]               0\n",
      "               Linear-384             [-1, 512, 768]       2,362,368\n",
      "           BertOutput-385            [-1, 512, 3072]               0\n",
      "               Linear-386            [-1, 512, 3072]       2,360,064\n",
      "           BertOutput-387            [-1, 512, 3072]               0\n",
      "               Linear-388            [-1, 512, 3072]       2,360,064\n",
      "              Dropout-389             [-1, 512, 768]               0\n",
      "              Dropout-390             [-1, 512, 768]               0\n",
      "       FusedLayerNorm-391             [-1, 512, 768]           1,536\n",
      "            BertLayer-392             [-1, 512, 768]               0\n",
      "        BertAttention-393             [-1, 512, 768]               0\n",
      "    BertSelfAttention-394             [-1, 512, 768]               0\n",
      "               Linear-395             [-1, 512, 768]         590,592\n",
      "       FusedLayerNorm-396             [-1, 512, 768]           1,536\n",
      "            BertLayer-397             [-1, 512, 768]               0\n",
      "        BertAttention-398             [-1, 512, 768]               0\n",
      "    BertSelfAttention-399             [-1, 512, 768]               0\n",
      "               Linear-400             [-1, 512, 768]         590,592\n",
      "               Linear-401             [-1, 512, 768]         590,592\n",
      "               Linear-402             [-1, 512, 768]         590,592\n",
      "               Linear-403             [-1, 512, 768]         590,592\n",
      "               Linear-404             [-1, 512, 768]         590,592\n",
      "              Dropout-405         [-1, 12, 512, 512]               0\n",
      "              Dropout-406         [-1, 12, 512, 512]               0\n",
      "       BertSelfOutput-407             [-1, 512, 768]               0\n",
      "               Linear-408             [-1, 512, 768]         590,592\n",
      "       BertSelfOutput-409             [-1, 512, 768]               0\n",
      "               Linear-410             [-1, 512, 768]         590,592\n",
      "              Dropout-411             [-1, 512, 768]               0\n",
      "              Dropout-412             [-1, 512, 768]               0\n",
      "       FusedLayerNorm-413             [-1, 512, 768]           1,536\n",
      "       FusedLayerNorm-414             [-1, 512, 768]           1,536\n",
      "     BertIntermediate-415             [-1, 512, 768]               0\n",
      "               Linear-416             [-1, 512, 768]       2,362,368\n",
      "     BertIntermediate-417             [-1, 512, 768]               0\n",
      "               Linear-418             [-1, 512, 768]       2,362,368\n",
      "           BertOutput-419            [-1, 512, 3072]               0\n",
      "               Linear-420            [-1, 512, 3072]       2,360,064\n",
      "           BertOutput-421            [-1, 512, 3072]               0\n",
      "               Linear-422            [-1, 512, 3072]       2,360,064\n",
      "              Dropout-423             [-1, 512, 768]               0\n",
      "              Dropout-424             [-1, 512, 768]               0\n",
      "       FusedLayerNorm-425             [-1, 512, 768]           1,536\n",
      "       FusedLayerNorm-426             [-1, 512, 768]           1,536\n",
      "           BertPooler-427             [-1, 512, 768]               0\n",
      "               Linear-428                  [-1, 768]         590,592\n",
      "           BertPooler-429             [-1, 512, 768]               0\n",
      "               Linear-430                  [-1, 768]         590,592\n",
      "                 Tanh-431                  [-1, 768]               0\n",
      "                 Tanh-432                  [-1, 768]               0\n",
      "              Dropout-433                  [-1, 768]               0\n",
      "              Dropout-434                  [-1, 768]               0\n",
      "               Linear-435                  [-1, 768]             769\n",
      "               Linear-436                  [-1, 768]             769\n",
      "=======================================================================\n",
      "Total params: 218,966,018\n",
      "Trainable params: 218,966,018\n",
      "Non-trainable params: 0\n",
      "-----------------------------------------------------------------------\n",
      "--------------------Epoch {epoch}/{self.epochs}------------------------\n",
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>]100% -0.5s/step  accuracy: 0.5385 - loss: 0.6836 \n",
      "------------------------- train result ------------------------------\n",
      "Best thresh: 0.0000 - F1 Score: 0.6817\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-06-12 02:11:58]: bert trainer.py[line:188] INFO  \n",
      "Epoch: 1 -  loss: 0.6766 - f1: 0.6817 - valid_loss: 0.6670 - valid_f1: 0.7745 \n",
      "[2019-06-12 02:11:58]: bert modelcheckpoint.py[line:48] INFO  \n",
      "Epoch 1: valid_loss improved from inf to 0.66699\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------------valid result ------------------------------\n",
      "Best thresh: 0.4300 - F1 Score: 0.7745\n",
      "-----------------------------------------------------------------------\n",
      "--------------------Epoch {epoch}/{self.epochs}------------------------\n",
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>]100% -0.5s/step  accuracy: 0.6923 - loss: 0.6092 \n",
      "------------------------- train result ------------------------------\n",
      "Best thresh: 0.4400 - F1 Score: 0.7279\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-06-12 02:12:52]: bert trainer.py[line:188] INFO  \n",
      "Epoch: 2 -  loss: 0.5666 - f1: 0.7279 - valid_loss: 0.4947 - valid_f1: 0.7812 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------------valid result ------------------------------\n",
      "Best thresh: 0.1600 - F1 Score: 0.7812\n",
      "-----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-06-12 02:12:53]: bert modelcheckpoint.py[line:48] INFO  \n",
      "Epoch 2: valid_loss improved from 0.66699 to 0.49469\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------Epoch {epoch}/{self.epochs}------------------------\n",
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>]100% -0.5s/step  accuracy: 1.0000 - loss: 0.1758 \n",
      "------------------------- train result ------------------------------\n",
      "Best thresh: 0.3400 - F1 Score: 0.8308\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-06-12 02:13:50]: bert trainer.py[line:188] INFO  \n",
      "Epoch: 3 -  loss: 0.4049 - f1: 0.8308 - valid_loss: 0.4627 - valid_f1: 0.8306 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------------valid result ------------------------------\n",
      "Best thresh: 0.2500 - F1 Score: 0.8306\n",
      "-----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-06-12 02:13:50]: bert modelcheckpoint.py[line:48] INFO  \n",
      "Epoch 3: valid_loss improved from 0.49469 to 0.46268\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------Epoch {epoch}/{self.epochs}------------------------\n",
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>]100% -0.5s/step  accuracy: 0.8462 - loss: 0.2141 \n",
      "------------------------- train result ------------------------------\n",
      "Best thresh: 0.5100 - F1 Score: 0.9046\n",
      "\n",
      "--------------------------valid result ------------------------------\n",
      "Best thresh: 0.2500 - F1 Score: 0.8111\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-06-12 02:14:48]: bert trainer.py[line:188] INFO  \n",
      "Epoch: 4 -  loss: 0.2968 - f1: 0.9046 - valid_loss: 0.5145 - valid_f1: 0.8111 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------\n",
      "--------------------Epoch {epoch}/{self.epochs}------------------------\n",
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>]100% -0.5s/step  accuracy: 1.0000 - loss: 0.0586 \n",
      "------------------------- train result ------------------------------\n",
      "Best thresh: 0.2900 - F1 Score: 0.9450\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-06-12 02:15:23]: bert trainer.py[line:188] INFO  \n",
      "Epoch: 5 -  loss: 0.1885 - f1: 0.9450 - valid_loss: 0.6059 - valid_f1: 0.8000 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------------valid result ------------------------------\n",
      "Best thresh: 0.7700 - F1 Score: 0.8000\n",
      "-----------------------------------------------------------------------\n",
      "--------------------Epoch {epoch}/{self.epochs}------------------------\n",
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>]100% -0.5s/step  accuracy: 1.0000 - loss: 0.0247 \n",
      "------------------------- train result ------------------------------\n",
      "Best thresh: 0.3600 - F1 Score: 0.9775\n",
      "\n",
      "--------------------------valid result ------------------------------\n",
      "Best thresh: 0.6900 - F1 Score: 0.7978\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-06-12 02:15:57]: bert trainer.py[line:188] INFO  \n",
      "Epoch: 6 -  loss: 0.1023 - f1: 0.9775 - valid_loss: 0.6635 - valid_f1: 0.7978 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------\n",
      "--------------------Epoch {epoch}/{self.epochs}------------------------\n",
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>]100% -0.5s/step  accuracy: 1.0000 - loss: 0.0247 \n",
      "------------------------- train result ------------------------------\n",
      "Best thresh: 0.3300 - F1 Score: 0.9862\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-06-12 02:16:31]: bert trainer.py[line:188] INFO  \n",
      "Epoch: 7 -  loss: 0.0665 - f1: 0.9862 - valid_loss: 0.6868 - valid_f1: 0.8021 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------------valid result ------------------------------\n",
      "Best thresh: 0.1200 - F1 Score: 0.8021\n",
      "-----------------------------------------------------------------------\n",
      "--------------------Epoch {epoch}/{self.epochs}------------------------\n",
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>]100% -0.5s/step  accuracy: 1.0000 - loss: 0.0144 \n",
      "------------------------- train result ------------------------------\n",
      "Best thresh: 0.4200 - F1 Score: 0.9914\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-06-12 02:17:05]: bert trainer.py[line:188] INFO  \n",
      "Epoch: 8 -  loss: 0.0496 - f1: 0.9914 - valid_loss: 0.7409 - valid_f1: 0.8045 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------------valid result ------------------------------\n",
      "Best thresh: 0.3900 - F1 Score: 0.8045\n",
      "-----------------------------------------------------------------------\n",
      "--------------------Epoch {epoch}/{self.epochs}------------------------\n",
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>]100% -0.5s/step  accuracy: 1.0000 - loss: 0.0169 \n",
      "------------------------- train result ------------------------------\n",
      "Best thresh: 0.2900 - F1 Score: 0.9914\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-06-12 02:17:39]: bert trainer.py[line:188] INFO  \n",
      "Epoch: 9 -  loss: 0.0436 - f1: 0.9914 - valid_loss: 0.7592 - valid_f1: 0.7978 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------------valid result ------------------------------\n",
      "Best thresh: 0.0600 - F1 Score: 0.7978\n",
      "-----------------------------------------------------------------------\n",
      "--------------------Epoch {epoch}/{self.epochs}------------------------\n",
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>]100% -0.5s/step  accuracy: 1.0000 - loss: 0.0090 \n",
      "------------------------- train result ------------------------------\n",
      "Best thresh: 0.1300 - F1 Score: 0.9931\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-06-12 02:18:14]: bert trainer.py[line:188] INFO  \n",
      "Epoch: 10 -  loss: 0.0375 - f1: 0.9931 - valid_loss: 0.7666 - valid_f1: 0.7978 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------------valid result ------------------------------\n",
      "Best thresh: 0.0600 - F1 Score: 0.7978\n",
      "-----------------------------------------------------------------------\n",
      "--------------------Epoch {epoch}/{self.epochs}------------------------\n",
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>]100% -0.5s/step  accuracy: 1.0000 - loss: 0.0153 \n",
      "------------------------- train result ------------------------------\n",
      "Best thresh: 0.0600 - F1 Score: 0.9931\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-06-12 02:18:47]: bert trainer.py[line:188] INFO  \n",
      "Epoch: 11 -  loss: 0.0339 - f1: 0.9931 - valid_loss: 0.7792 - valid_f1: 0.7978 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------------valid result ------------------------------\n",
      "Best thresh: 0.0600 - F1 Score: 0.7978\n",
      "-----------------------------------------------------------------------\n",
      "--------------------Epoch {epoch}/{self.epochs}------------------------\n",
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>]100% -0.6s/step  accuracy: 1.0000 - loss: 0.0110 \n",
      "------------------------- train result ------------------------------\n",
      "Best thresh: 0.2100 - F1 Score: 0.9931\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-06-12 02:19:22]: bert trainer.py[line:188] INFO  \n",
      "Epoch: 12 -  loss: 0.0311 - f1: 0.9931 - valid_loss: 0.7898 - valid_f1: 0.7978 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------------valid result ------------------------------\n",
      "Best thresh: 0.0500 - F1 Score: 0.7978\n",
      "-----------------------------------------------------------------------\n",
      "--------------------Epoch {epoch}/{self.epochs}------------------------\n",
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>]100% -0.5s/step  accuracy: 1.0000 - loss: 0.0089 \n",
      "------------------------- train result ------------------------------\n",
      "Best thresh: 0.1600 - F1 Score: 0.9931\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-06-12 02:19:56]: bert trainer.py[line:188] INFO  \n",
      "Epoch: 13 -  loss: 0.0296 - f1: 0.9931 - valid_loss: 0.7938 - valid_f1: 0.7978 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------------valid result ------------------------------\n",
      "Best thresh: 0.0500 - F1 Score: 0.7978\n",
      "-----------------------------------------------------------------------\n",
      "--------------------Epoch {epoch}/{self.epochs}------------------------\n",
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>]100% -0.5s/step  accuracy: 1.0000 - loss: 0.0080 \n",
      "------------------------- train result ------------------------------\n",
      "Best thresh: 0.3300 - F1 Score: 0.9931\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-06-12 02:20:30]: bert trainer.py[line:188] INFO  \n",
      "Epoch: 14 -  loss: 0.0307 - f1: 0.9931 - valid_loss: 0.7946 - valid_f1: 0.8023 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------------valid result ------------------------------\n",
      "Best thresh: 0.4000 - F1 Score: 0.8023\n",
      "-----------------------------------------------------------------------\n",
      "--------------------Epoch {epoch}/{self.epochs}------------------------\n",
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>]100% -0.5s/step  accuracy: 1.0000 - loss: 0.0331 \n",
      "------------------------- train result ------------------------------\n",
      "Best thresh: 0.5200 - F1 Score: 0.9948\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-06-12 02:21:04]: bert trainer.py[line:188] INFO  \n",
      "Epoch: 15 -  loss: 0.0286 - f1: 0.9948 - valid_loss: 0.7953 - valid_f1: 0.8023 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------------valid result ------------------------------\n",
      "Best thresh: 0.4000 - F1 Score: 0.8023\n",
      "-----------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "train = \"bert_recipe_lonepatient/train_bert_multi_label.py\"\n",
    "%run $train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-06-12 07:35:45]: bert train_bert_multi_label.py[line:28] INFO  seed is 2018\n",
      "[2019-06-12 07:35:45]: bert train_bert_multi_label.py[line:31] INFO  starting load data from disk\n",
      "100%|██████████| 742/742 [00:00<00:00, 9504.56it/s]\n",
      "[2019-06-12 07:35:45]: bert data_transformer.py[line:83] INFO  train val split\n",
      "Merge: 742it [00:00, 993035.60it/s]\n",
      "write data to disk: 100%|██████████| 557/557 [00:00<00:00, 130537.37it/s]\n",
      "write data to disk: 100%|██████████| 185/185 [00:00<00:00, 129670.16it/s]\n",
      "[2019-06-12 07:35:46]: bert train_bert_multi_label.py[line:79] INFO  initializing model\n",
      "[2019-06-12 07:35:58]: bert train_bert_multi_label.py[line:101] INFO  initializing callbacks\n",
      "[2019-06-12 07:35:58]: bert train_bert_multi_label.py[line:119] INFO  training model....\n",
      "[2019-06-12 07:36:03]: bert utils.py[line:43] INFO  current 2 GPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model summary info: \n",
      "-----------------------------------------------------------------------\n",
      "             Layer (type)                Input Shape         Param #\n",
      "=======================================================================\n",
      "               BertFine-1                  [-1, 512]               0\n",
      "              BertModel-2                  [-1, 512]               0\n",
      "         BertEmbeddings-3                  [-1, 512]               0\n",
      "               BertFine-4                  [-1, 512]               0\n",
      "              BertModel-5                  [-1, 512]               0\n",
      "              Embedding-6                  [-1, 512]      23,440,896\n",
      "         BertEmbeddings-7                  [-1, 512]               0\n",
      "              Embedding-8                  [-1, 512]      23,440,896\n",
      "              Embedding-9                  [-1, 512]         393,216\n",
      "             Embedding-10                  [-1, 512]         393,216\n",
      "             Embedding-11                  [-1, 512]           1,536\n",
      "             Embedding-12                  [-1, 512]           1,536\n",
      "        FusedLayerNorm-13             [-1, 512, 768]           1,536\n",
      "        FusedLayerNorm-14             [-1, 512, 768]           1,536\n",
      "               Dropout-15             [-1, 512, 768]               0\n",
      "               Dropout-16             [-1, 512, 768]               0\n",
      "           BertEncoder-17             [-1, 512, 768]               0\n",
      "             BertLayer-18             [-1, 512, 768]               0\n",
      "         BertAttention-19             [-1, 512, 768]               0\n",
      "     BertSelfAttention-20             [-1, 512, 768]               0\n",
      "                Linear-21             [-1, 512, 768]         590,592\n",
      "           BertEncoder-22             [-1, 512, 768]               0\n",
      "             BertLayer-23             [-1, 512, 768]               0\n",
      "         BertAttention-24             [-1, 512, 768]               0\n",
      "     BertSelfAttention-25             [-1, 512, 768]               0\n",
      "                Linear-26             [-1, 512, 768]         590,592\n",
      "                Linear-27             [-1, 512, 768]         590,592\n",
      "                Linear-28             [-1, 512, 768]         590,592\n",
      "                Linear-29             [-1, 512, 768]         590,592\n",
      "                Linear-30             [-1, 512, 768]         590,592\n",
      "               Dropout-31         [-1, 12, 512, 512]               0\n",
      "               Dropout-32         [-1, 12, 512, 512]               0\n",
      "        BertSelfOutput-33             [-1, 512, 768]               0\n",
      "                Linear-34             [-1, 512, 768]         590,592\n",
      "        BertSelfOutput-35             [-1, 512, 768]               0\n",
      "                Linear-36             [-1, 512, 768]         590,592\n",
      "               Dropout-37             [-1, 512, 768]               0\n",
      "               Dropout-38             [-1, 512, 768]               0\n",
      "        FusedLayerNorm-39             [-1, 512, 768]           1,536\n",
      "        FusedLayerNorm-40             [-1, 512, 768]           1,536\n",
      "      BertIntermediate-41             [-1, 512, 768]               0\n",
      "                Linear-42             [-1, 512, 768]       2,362,368\n",
      "      BertIntermediate-43             [-1, 512, 768]               0\n",
      "                Linear-44             [-1, 512, 768]       2,362,368\n",
      "            BertOutput-45            [-1, 512, 3072]               0\n",
      "                Linear-46            [-1, 512, 3072]       2,360,064\n",
      "            BertOutput-47            [-1, 512, 3072]               0\n",
      "                Linear-48            [-1, 512, 3072]       2,360,064\n",
      "               Dropout-49             [-1, 512, 768]               0\n",
      "               Dropout-50             [-1, 512, 768]               0\n",
      "        FusedLayerNorm-51             [-1, 512, 768]           1,536\n",
      "        FusedLayerNorm-52             [-1, 512, 768]           1,536\n",
      "             BertLayer-53             [-1, 512, 768]               0\n",
      "         BertAttention-54             [-1, 512, 768]               0\n",
      "     BertSelfAttention-55             [-1, 512, 768]               0\n",
      "                Linear-56             [-1, 512, 768]         590,592\n",
      "             BertLayer-57             [-1, 512, 768]               0\n",
      "         BertAttention-58             [-1, 512, 768]               0\n",
      "     BertSelfAttention-59             [-1, 512, 768]               0\n",
      "                Linear-60             [-1, 512, 768]         590,592\n",
      "                Linear-61             [-1, 512, 768]         590,592\n",
      "                Linear-62             [-1, 512, 768]         590,592\n",
      "                Linear-63             [-1, 512, 768]         590,592\n",
      "                Linear-64             [-1, 512, 768]         590,592\n",
      "               Dropout-65         [-1, 12, 512, 512]               0\n",
      "               Dropout-66         [-1, 12, 512, 512]               0\n",
      "        BertSelfOutput-67             [-1, 512, 768]               0\n",
      "                Linear-68             [-1, 512, 768]         590,592\n",
      "        BertSelfOutput-69             [-1, 512, 768]               0\n",
      "                Linear-70             [-1, 512, 768]         590,592\n",
      "               Dropout-71             [-1, 512, 768]               0\n",
      "               Dropout-72             [-1, 512, 768]               0\n",
      "        FusedLayerNorm-73             [-1, 512, 768]           1,536\n",
      "        FusedLayerNorm-74             [-1, 512, 768]           1,536\n",
      "      BertIntermediate-75             [-1, 512, 768]               0\n",
      "                Linear-76             [-1, 512, 768]       2,362,368\n",
      "      BertIntermediate-77             [-1, 512, 768]               0\n",
      "                Linear-78             [-1, 512, 768]       2,362,368\n",
      "            BertOutput-79            [-1, 512, 3072]               0\n",
      "                Linear-80            [-1, 512, 3072]       2,360,064\n",
      "            BertOutput-81            [-1, 512, 3072]               0\n",
      "                Linear-82            [-1, 512, 3072]       2,360,064\n",
      "               Dropout-83             [-1, 512, 768]               0\n",
      "               Dropout-84             [-1, 512, 768]               0\n",
      "        FusedLayerNorm-85             [-1, 512, 768]           1,536\n",
      "        FusedLayerNorm-86             [-1, 512, 768]           1,536\n",
      "             BertLayer-87             [-1, 512, 768]               0\n",
      "         BertAttention-88             [-1, 512, 768]               0\n",
      "     BertSelfAttention-89             [-1, 512, 768]               0\n",
      "                Linear-90             [-1, 512, 768]         590,592\n",
      "             BertLayer-91             [-1, 512, 768]               0\n",
      "         BertAttention-92             [-1, 512, 768]               0\n",
      "     BertSelfAttention-93             [-1, 512, 768]               0\n",
      "                Linear-94             [-1, 512, 768]         590,592\n",
      "                Linear-95             [-1, 512, 768]         590,592\n",
      "                Linear-96             [-1, 512, 768]         590,592\n",
      "                Linear-97             [-1, 512, 768]         590,592\n",
      "                Linear-98             [-1, 512, 768]         590,592\n",
      "               Dropout-99         [-1, 12, 512, 512]               0\n",
      "              Dropout-100         [-1, 12, 512, 512]               0\n",
      "       BertSelfOutput-101             [-1, 512, 768]               0\n",
      "               Linear-102             [-1, 512, 768]         590,592\n",
      "       BertSelfOutput-103             [-1, 512, 768]               0\n",
      "               Linear-104             [-1, 512, 768]         590,592\n",
      "              Dropout-105             [-1, 512, 768]               0\n",
      "              Dropout-106             [-1, 512, 768]               0\n",
      "       FusedLayerNorm-107             [-1, 512, 768]           1,536\n",
      "       FusedLayerNorm-108             [-1, 512, 768]           1,536\n",
      "     BertIntermediate-109             [-1, 512, 768]               0\n",
      "               Linear-110             [-1, 512, 768]       2,362,368\n",
      "     BertIntermediate-111             [-1, 512, 768]               0\n",
      "               Linear-112             [-1, 512, 768]       2,362,368\n",
      "           BertOutput-113            [-1, 512, 3072]               0\n",
      "               Linear-114            [-1, 512, 3072]       2,360,064\n",
      "           BertOutput-115            [-1, 512, 3072]               0\n",
      "               Linear-116            [-1, 512, 3072]       2,360,064\n",
      "              Dropout-117             [-1, 512, 768]               0\n",
      "              Dropout-118             [-1, 512, 768]               0\n",
      "       FusedLayerNorm-119             [-1, 512, 768]           1,536\n",
      "       FusedLayerNorm-120             [-1, 512, 768]           1,536\n",
      "            BertLayer-121             [-1, 512, 768]               0\n",
      "        BertAttention-122             [-1, 512, 768]               0\n",
      "    BertSelfAttention-123             [-1, 512, 768]               0\n",
      "               Linear-124             [-1, 512, 768]         590,592\n",
      "            BertLayer-125             [-1, 512, 768]               0\n",
      "        BertAttention-126             [-1, 512, 768]               0\n",
      "    BertSelfAttention-127             [-1, 512, 768]               0\n",
      "               Linear-128             [-1, 512, 768]         590,592\n",
      "               Linear-129             [-1, 512, 768]         590,592\n",
      "               Linear-130             [-1, 512, 768]         590,592\n",
      "               Linear-131             [-1, 512, 768]         590,592\n",
      "               Linear-132             [-1, 512, 768]         590,592\n",
      "              Dropout-133         [-1, 12, 512, 512]               0\n",
      "              Dropout-134         [-1, 12, 512, 512]               0\n",
      "       BertSelfOutput-135             [-1, 512, 768]               0\n",
      "               Linear-136             [-1, 512, 768]         590,592\n",
      "       BertSelfOutput-137             [-1, 512, 768]               0\n",
      "               Linear-138             [-1, 512, 768]         590,592\n",
      "              Dropout-139             [-1, 512, 768]               0\n",
      "       FusedLayerNorm-140             [-1, 512, 768]           1,536\n",
      "              Dropout-141             [-1, 512, 768]               0\n",
      "     BertIntermediate-142             [-1, 512, 768]               0\n",
      "               Linear-143             [-1, 512, 768]       2,362,368\n",
      "       FusedLayerNorm-144             [-1, 512, 768]           1,536\n",
      "     BertIntermediate-145             [-1, 512, 768]               0\n",
      "               Linear-146             [-1, 512, 768]       2,362,368\n",
      "           BertOutput-147            [-1, 512, 3072]               0\n",
      "               Linear-148            [-1, 512, 3072]       2,360,064\n",
      "           BertOutput-149            [-1, 512, 3072]               0\n",
      "               Linear-150            [-1, 512, 3072]       2,360,064\n",
      "              Dropout-151             [-1, 512, 768]               0\n",
      "       FusedLayerNorm-152             [-1, 512, 768]           1,536\n",
      "              Dropout-153             [-1, 512, 768]               0\n",
      "            BertLayer-154             [-1, 512, 768]               0\n",
      "        BertAttention-155             [-1, 512, 768]               0\n",
      "    BertSelfAttention-156             [-1, 512, 768]               0\n",
      "               Linear-157             [-1, 512, 768]         590,592\n",
      "       FusedLayerNorm-158             [-1, 512, 768]           1,536\n",
      "            BertLayer-159             [-1, 512, 768]               0\n",
      "        BertAttention-160             [-1, 512, 768]               0\n",
      "    BertSelfAttention-161             [-1, 512, 768]               0\n",
      "               Linear-162             [-1, 512, 768]         590,592\n",
      "               Linear-163             [-1, 512, 768]         590,592\n",
      "               Linear-164             [-1, 512, 768]         590,592\n",
      "               Linear-165             [-1, 512, 768]         590,592\n",
      "               Linear-166             [-1, 512, 768]         590,592\n",
      "              Dropout-167         [-1, 12, 512, 512]               0\n",
      "              Dropout-168         [-1, 12, 512, 512]               0\n",
      "       BertSelfOutput-169             [-1, 512, 768]               0\n",
      "               Linear-170             [-1, 512, 768]         590,592\n",
      "       BertSelfOutput-171             [-1, 512, 768]               0\n",
      "               Linear-172             [-1, 512, 768]         590,592\n",
      "              Dropout-173             [-1, 512, 768]               0\n",
      "              Dropout-174             [-1, 512, 768]               0\n",
      "       FusedLayerNorm-175             [-1, 512, 768]           1,536\n",
      "       FusedLayerNorm-176             [-1, 512, 768]           1,536\n",
      "     BertIntermediate-177             [-1, 512, 768]               0\n",
      "               Linear-178             [-1, 512, 768]       2,362,368\n",
      "     BertIntermediate-179             [-1, 512, 768]               0\n",
      "               Linear-180             [-1, 512, 768]       2,362,368\n",
      "           BertOutput-181            [-1, 512, 3072]               0\n",
      "               Linear-182            [-1, 512, 3072]       2,360,064\n",
      "           BertOutput-183            [-1, 512, 3072]               0\n",
      "               Linear-184            [-1, 512, 3072]       2,360,064\n",
      "              Dropout-185             [-1, 512, 768]               0\n",
      "              Dropout-186             [-1, 512, 768]               0\n",
      "       FusedLayerNorm-187             [-1, 512, 768]           1,536\n",
      "       FusedLayerNorm-188             [-1, 512, 768]           1,536\n",
      "            BertLayer-189             [-1, 512, 768]               0\n",
      "        BertAttention-190             [-1, 512, 768]               0\n",
      "    BertSelfAttention-191             [-1, 512, 768]               0\n",
      "               Linear-192             [-1, 512, 768]         590,592\n",
      "            BertLayer-193             [-1, 512, 768]               0\n",
      "        BertAttention-194             [-1, 512, 768]               0\n",
      "    BertSelfAttention-195             [-1, 512, 768]               0\n",
      "               Linear-196             [-1, 512, 768]         590,592\n",
      "               Linear-197             [-1, 512, 768]         590,592\n",
      "               Linear-198             [-1, 512, 768]         590,592\n",
      "               Linear-199             [-1, 512, 768]         590,592\n",
      "               Linear-200             [-1, 512, 768]         590,592\n",
      "              Dropout-201         [-1, 12, 512, 512]               0\n",
      "              Dropout-202         [-1, 12, 512, 512]               0\n",
      "       BertSelfOutput-203             [-1, 512, 768]               0\n",
      "               Linear-204             [-1, 512, 768]         590,592\n",
      "       BertSelfOutput-205             [-1, 512, 768]               0\n",
      "               Linear-206             [-1, 512, 768]         590,592\n",
      "              Dropout-207             [-1, 512, 768]               0\n",
      "              Dropout-208             [-1, 512, 768]               0\n",
      "       FusedLayerNorm-209             [-1, 512, 768]           1,536\n",
      "       FusedLayerNorm-210             [-1, 512, 768]           1,536\n",
      "     BertIntermediate-211             [-1, 512, 768]               0\n",
      "               Linear-212             [-1, 512, 768]       2,362,368\n",
      "     BertIntermediate-213             [-1, 512, 768]               0\n",
      "               Linear-214             [-1, 512, 768]       2,362,368\n",
      "           BertOutput-215            [-1, 512, 3072]               0\n",
      "               Linear-216            [-1, 512, 3072]       2,360,064\n",
      "           BertOutput-217            [-1, 512, 3072]               0\n",
      "               Linear-218            [-1, 512, 3072]       2,360,064\n",
      "              Dropout-219             [-1, 512, 768]               0\n",
      "              Dropout-220             [-1, 512, 768]               0\n",
      "       FusedLayerNorm-221             [-1, 512, 768]           1,536\n",
      "       FusedLayerNorm-222             [-1, 512, 768]           1,536\n",
      "            BertLayer-223             [-1, 512, 768]               0\n",
      "        BertAttention-224             [-1, 512, 768]               0\n",
      "    BertSelfAttention-225             [-1, 512, 768]               0\n",
      "               Linear-226             [-1, 512, 768]         590,592\n",
      "            BertLayer-227             [-1, 512, 768]               0\n",
      "        BertAttention-228             [-1, 512, 768]               0\n",
      "    BertSelfAttention-229             [-1, 512, 768]               0\n",
      "               Linear-230             [-1, 512, 768]         590,592\n",
      "               Linear-231             [-1, 512, 768]         590,592\n",
      "               Linear-232             [-1, 512, 768]         590,592\n",
      "               Linear-233             [-1, 512, 768]         590,592\n",
      "               Linear-234             [-1, 512, 768]         590,592\n",
      "              Dropout-235         [-1, 12, 512, 512]               0\n",
      "              Dropout-236         [-1, 12, 512, 512]               0\n",
      "       BertSelfOutput-237             [-1, 512, 768]               0\n",
      "               Linear-238             [-1, 512, 768]         590,592\n",
      "       BertSelfOutput-239             [-1, 512, 768]               0\n",
      "               Linear-240             [-1, 512, 768]         590,592\n",
      "              Dropout-241             [-1, 512, 768]               0\n",
      "              Dropout-242             [-1, 512, 768]               0\n",
      "       FusedLayerNorm-243             [-1, 512, 768]           1,536\n",
      "       FusedLayerNorm-244             [-1, 512, 768]           1,536\n",
      "     BertIntermediate-245             [-1, 512, 768]               0\n",
      "               Linear-246             [-1, 512, 768]       2,362,368\n",
      "     BertIntermediate-247             [-1, 512, 768]               0\n",
      "               Linear-248             [-1, 512, 768]       2,362,368\n",
      "           BertOutput-249            [-1, 512, 3072]               0\n",
      "               Linear-250            [-1, 512, 3072]       2,360,064\n",
      "           BertOutput-251            [-1, 512, 3072]               0\n",
      "               Linear-252            [-1, 512, 3072]       2,360,064\n",
      "              Dropout-253             [-1, 512, 768]               0\n",
      "              Dropout-254             [-1, 512, 768]               0\n",
      "       FusedLayerNorm-255             [-1, 512, 768]           1,536\n",
      "       FusedLayerNorm-256             [-1, 512, 768]           1,536\n",
      "            BertLayer-257             [-1, 512, 768]               0\n",
      "        BertAttention-258             [-1, 512, 768]               0\n",
      "    BertSelfAttention-259             [-1, 512, 768]               0\n",
      "               Linear-260             [-1, 512, 768]         590,592\n",
      "            BertLayer-261             [-1, 512, 768]               0\n",
      "        BertAttention-262             [-1, 512, 768]               0\n",
      "    BertSelfAttention-263             [-1, 512, 768]               0\n",
      "               Linear-264             [-1, 512, 768]         590,592\n",
      "               Linear-265             [-1, 512, 768]         590,592\n",
      "               Linear-266             [-1, 512, 768]         590,592\n",
      "               Linear-267             [-1, 512, 768]         590,592\n",
      "               Linear-268             [-1, 512, 768]         590,592\n",
      "              Dropout-269         [-1, 12, 512, 512]               0\n",
      "              Dropout-270         [-1, 12, 512, 512]               0\n",
      "       BertSelfOutput-271             [-1, 512, 768]               0\n",
      "               Linear-272             [-1, 512, 768]         590,592\n",
      "       BertSelfOutput-273             [-1, 512, 768]               0\n",
      "               Linear-274             [-1, 512, 768]         590,592\n",
      "              Dropout-275             [-1, 512, 768]               0\n",
      "              Dropout-276             [-1, 512, 768]               0\n",
      "       FusedLayerNorm-277             [-1, 512, 768]           1,536\n",
      "     BertIntermediate-278             [-1, 512, 768]               0\n",
      "               Linear-279             [-1, 512, 768]       2,362,368\n",
      "       FusedLayerNorm-280             [-1, 512, 768]           1,536\n",
      "     BertIntermediate-281             [-1, 512, 768]               0\n",
      "               Linear-282             [-1, 512, 768]       2,362,368\n",
      "           BertOutput-283            [-1, 512, 3072]               0\n",
      "               Linear-284            [-1, 512, 3072]       2,360,064\n",
      "           BertOutput-285            [-1, 512, 3072]               0\n",
      "               Linear-286            [-1, 512, 3072]       2,360,064\n",
      "              Dropout-287             [-1, 512, 768]               0\n",
      "       FusedLayerNorm-288             [-1, 512, 768]           1,536\n",
      "              Dropout-289             [-1, 512, 768]               0\n",
      "            BertLayer-290             [-1, 512, 768]               0\n",
      "        BertAttention-291             [-1, 512, 768]               0\n",
      "    BertSelfAttention-292             [-1, 512, 768]               0\n",
      "               Linear-293             [-1, 512, 768]         590,592\n",
      "       FusedLayerNorm-294             [-1, 512, 768]           1,536\n",
      "            BertLayer-295             [-1, 512, 768]               0\n",
      "        BertAttention-296             [-1, 512, 768]               0\n",
      "    BertSelfAttention-297             [-1, 512, 768]               0\n",
      "               Linear-298             [-1, 512, 768]         590,592\n",
      "               Linear-299             [-1, 512, 768]         590,592\n",
      "               Linear-300             [-1, 512, 768]         590,592\n",
      "               Linear-301             [-1, 512, 768]         590,592\n",
      "               Linear-302             [-1, 512, 768]         590,592\n",
      "              Dropout-303         [-1, 12, 512, 512]               0\n",
      "              Dropout-304         [-1, 12, 512, 512]               0\n",
      "       BertSelfOutput-305             [-1, 512, 768]               0\n",
      "               Linear-306             [-1, 512, 768]         590,592\n",
      "       BertSelfOutput-307             [-1, 512, 768]               0\n",
      "               Linear-308             [-1, 512, 768]         590,592\n",
      "              Dropout-309             [-1, 512, 768]               0\n",
      "              Dropout-310             [-1, 512, 768]               0\n",
      "       FusedLayerNorm-311             [-1, 512, 768]           1,536\n",
      "       FusedLayerNorm-312             [-1, 512, 768]           1,536\n",
      "     BertIntermediate-313             [-1, 512, 768]               0\n",
      "               Linear-314             [-1, 512, 768]       2,362,368\n",
      "     BertIntermediate-315             [-1, 512, 768]               0\n",
      "               Linear-316             [-1, 512, 768]       2,362,368\n",
      "           BertOutput-317            [-1, 512, 3072]               0\n",
      "               Linear-318            [-1, 512, 3072]       2,360,064\n",
      "           BertOutput-319            [-1, 512, 3072]               0\n",
      "               Linear-320            [-1, 512, 3072]       2,360,064\n",
      "              Dropout-321             [-1, 512, 768]               0\n",
      "              Dropout-322             [-1, 512, 768]               0\n",
      "       FusedLayerNorm-323             [-1, 512, 768]           1,536\n",
      "       FusedLayerNorm-324             [-1, 512, 768]           1,536\n",
      "            BertLayer-325             [-1, 512, 768]               0\n",
      "        BertAttention-326             [-1, 512, 768]               0\n",
      "    BertSelfAttention-327             [-1, 512, 768]               0\n",
      "               Linear-328             [-1, 512, 768]         590,592\n",
      "            BertLayer-329             [-1, 512, 768]               0\n",
      "        BertAttention-330             [-1, 512, 768]               0\n",
      "    BertSelfAttention-331             [-1, 512, 768]               0\n",
      "               Linear-332             [-1, 512, 768]         590,592\n",
      "               Linear-333             [-1, 512, 768]         590,592\n",
      "               Linear-334             [-1, 512, 768]         590,592\n",
      "               Linear-335             [-1, 512, 768]         590,592\n",
      "               Linear-336             [-1, 512, 768]         590,592\n",
      "              Dropout-337         [-1, 12, 512, 512]               0\n",
      "              Dropout-338         [-1, 12, 512, 512]               0\n",
      "       BertSelfOutput-339             [-1, 512, 768]               0\n",
      "               Linear-340             [-1, 512, 768]         590,592\n",
      "       BertSelfOutput-341             [-1, 512, 768]               0\n",
      "               Linear-342             [-1, 512, 768]         590,592\n",
      "              Dropout-343             [-1, 512, 768]               0\n",
      "              Dropout-344             [-1, 512, 768]               0\n",
      "       FusedLayerNorm-345             [-1, 512, 768]           1,536\n",
      "       FusedLayerNorm-346             [-1, 512, 768]           1,536\n",
      "     BertIntermediate-347             [-1, 512, 768]               0\n",
      "               Linear-348             [-1, 512, 768]       2,362,368\n",
      "     BertIntermediate-349             [-1, 512, 768]               0\n",
      "               Linear-350             [-1, 512, 768]       2,362,368\n",
      "           BertOutput-351            [-1, 512, 3072]               0\n",
      "               Linear-352            [-1, 512, 3072]       2,360,064\n",
      "           BertOutput-353            [-1, 512, 3072]               0\n",
      "               Linear-354            [-1, 512, 3072]       2,360,064\n",
      "              Dropout-355             [-1, 512, 768]               0\n",
      "              Dropout-356             [-1, 512, 768]               0\n",
      "       FusedLayerNorm-357             [-1, 512, 768]           1,536\n",
      "       FusedLayerNorm-358             [-1, 512, 768]           1,536\n",
      "            BertLayer-359             [-1, 512, 768]               0\n",
      "        BertAttention-360             [-1, 512, 768]               0\n",
      "    BertSelfAttention-361             [-1, 512, 768]               0\n",
      "               Linear-362             [-1, 512, 768]         590,592\n",
      "            BertLayer-363             [-1, 512, 768]               0\n",
      "        BertAttention-364             [-1, 512, 768]               0\n",
      "    BertSelfAttention-365             [-1, 512, 768]               0\n",
      "               Linear-366             [-1, 512, 768]         590,592\n",
      "               Linear-367             [-1, 512, 768]         590,592\n",
      "               Linear-368             [-1, 512, 768]         590,592\n",
      "               Linear-369             [-1, 512, 768]         590,592\n",
      "               Linear-370             [-1, 512, 768]         590,592\n",
      "              Dropout-371         [-1, 12, 512, 512]               0\n",
      "              Dropout-372         [-1, 12, 512, 512]               0\n",
      "       BertSelfOutput-373             [-1, 512, 768]               0\n",
      "               Linear-374             [-1, 512, 768]         590,592\n",
      "       BertSelfOutput-375             [-1, 512, 768]               0\n",
      "               Linear-376             [-1, 512, 768]         590,592\n",
      "              Dropout-377             [-1, 512, 768]               0\n",
      "              Dropout-378             [-1, 512, 768]               0\n",
      "       FusedLayerNorm-379             [-1, 512, 768]           1,536\n",
      "       FusedLayerNorm-380             [-1, 512, 768]           1,536\n",
      "     BertIntermediate-381             [-1, 512, 768]               0\n",
      "               Linear-382             [-1, 512, 768]       2,362,368\n",
      "     BertIntermediate-383             [-1, 512, 768]               0\n",
      "               Linear-384             [-1, 512, 768]       2,362,368\n",
      "           BertOutput-385            [-1, 512, 3072]               0\n",
      "               Linear-386            [-1, 512, 3072]       2,360,064\n",
      "           BertOutput-387            [-1, 512, 3072]               0\n",
      "               Linear-388            [-1, 512, 3072]       2,360,064\n",
      "              Dropout-389             [-1, 512, 768]               0\n",
      "              Dropout-390             [-1, 512, 768]               0\n",
      "       FusedLayerNorm-391             [-1, 512, 768]           1,536\n",
      "       FusedLayerNorm-392             [-1, 512, 768]           1,536\n",
      "            BertLayer-393             [-1, 512, 768]               0\n",
      "        BertAttention-394             [-1, 512, 768]               0\n",
      "    BertSelfAttention-395             [-1, 512, 768]               0\n",
      "               Linear-396             [-1, 512, 768]         590,592\n",
      "            BertLayer-397             [-1, 512, 768]               0\n",
      "        BertAttention-398             [-1, 512, 768]               0\n",
      "    BertSelfAttention-399             [-1, 512, 768]               0\n",
      "               Linear-400             [-1, 512, 768]         590,592\n",
      "               Linear-401             [-1, 512, 768]         590,592\n",
      "               Linear-402             [-1, 512, 768]         590,592\n",
      "               Linear-403             [-1, 512, 768]         590,592\n",
      "               Linear-404             [-1, 512, 768]         590,592\n",
      "              Dropout-405         [-1, 12, 512, 512]               0\n",
      "              Dropout-406         [-1, 12, 512, 512]               0\n",
      "       BertSelfOutput-407             [-1, 512, 768]               0\n",
      "               Linear-408             [-1, 512, 768]         590,592\n",
      "       BertSelfOutput-409             [-1, 512, 768]               0\n",
      "               Linear-410             [-1, 512, 768]         590,592\n",
      "              Dropout-411             [-1, 512, 768]               0\n",
      "              Dropout-412             [-1, 512, 768]               0\n",
      "       FusedLayerNorm-413             [-1, 512, 768]           1,536\n",
      "     BertIntermediate-414             [-1, 512, 768]               0\n",
      "               Linear-415             [-1, 512, 768]       2,362,368\n",
      "       FusedLayerNorm-416             [-1, 512, 768]           1,536\n",
      "     BertIntermediate-417             [-1, 512, 768]               0\n",
      "               Linear-418             [-1, 512, 768]       2,362,368\n",
      "           BertOutput-419            [-1, 512, 3072]               0\n",
      "               Linear-420            [-1, 512, 3072]       2,360,064\n",
      "           BertOutput-421            [-1, 512, 3072]               0\n",
      "               Linear-422            [-1, 512, 3072]       2,360,064\n",
      "              Dropout-423             [-1, 512, 768]               0\n",
      "              Dropout-424             [-1, 512, 768]               0\n",
      "       FusedLayerNorm-425             [-1, 512, 768]           1,536\n",
      "       FusedLayerNorm-426             [-1, 512, 768]           1,536\n",
      "           BertPooler-427             [-1, 512, 768]               0\n",
      "               Linear-428                  [-1, 768]         590,592\n",
      "           BertPooler-429             [-1, 512, 768]               0\n",
      "               Linear-430                  [-1, 768]         590,592\n",
      "                 Tanh-431                  [-1, 768]               0\n",
      "              Dropout-432                  [-1, 768]               0\n",
      "               Linear-433                  [-1, 768]             769\n",
      "                 Tanh-434                  [-1, 768]               0\n",
      "              Dropout-435                  [-1, 768]               0\n",
      "               Linear-436                  [-1, 768]             769\n",
      "=======================================================================\n",
      "Total params: 218,966,018\n",
      "Trainable params: 218,966,018\n",
      "Non-trainable params: 0\n",
      "-----------------------------------------------------------------------\n",
      "--------------------Epoch {epoch}/{self.epochs}------------------------\n",
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>]100% -0.5s/step  accuracy: 0.1538 - loss: 0.7218 \n",
      "------------------------- train result ------------------------------\n",
      "Best thresh: 0.3500 - F1 Score: 0.6683\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-06-12 07:36:50]: bert trainer.py[line:194] INFO  \n",
      "Epoch: 1 -  loss: 0.6779 - f1: 0.6683 - valid_loss: 0.6713 - valid_f1: 0.7042 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------------valid result ------------------------------\n",
      "Best thresh: 0.4800 - F1 Score: 0.7042\n",
      "-----------------------------------------------------------------------\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'valid_threshold_F1'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/workspace/AA_DPH/bert_recipe_lonepatient/train_bert_multi_label.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/workspace/AA_DPH/bert_recipe_lonepatient/train_bert_multi_label.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_configs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_configs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m     \u001b[0;31m# 拟合模型\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m     \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalid_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalid_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m     \u001b[0;31m# 释放显存\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'n_gpu'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/workspace/AA_DPH/bert_recipe_lonepatient/pybert/train/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, train_data, valid_data)\u001b[0m\n\u001b[1;32m    200\u001b[0m             \u001b[0;31m# save model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_checkpoint\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 202\u001b[0;31m                 \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_save_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalid_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'valid_loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthreshold\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'valid_threshold_F1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    203\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_checkpoint\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepoch_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_checkpoint\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmonitor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'valid_threshold_F1'"
     ]
    }
   ],
   "source": [
    "### batch size 16\n",
    "train = \"bert_recipe_lonepatient/train_bert_multi_label.py\"\n",
    "%run $train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-06-12 07:44:47]: bert train_bert_multi_label.py[line:28] INFO  seed is 2018\n",
      "[2019-06-12 07:44:47]: bert train_bert_multi_label.py[line:31] INFO  starting load data from disk\n",
      "100%|██████████| 742/742 [00:00<00:00, 7784.56it/s]\n",
      "[2019-06-12 07:44:47]: bert data_transformer.py[line:83] INFO  train val split\n",
      "Merge: 742it [00:00, 961734.72it/s]\n",
      "write data to disk: 100%|██████████| 557/557 [00:00<00:00, 209376.89it/s]\n",
      "write data to disk: 100%|██████████| 185/185 [00:00<00:00, 137823.49it/s]\n",
      "[2019-06-12 07:44:49]: bert train_bert_multi_label.py[line:79] INFO  initializing model\n",
      "[2019-06-12 07:45:00]: bert train_bert_multi_label.py[line:101] INFO  initializing callbacks\n",
      "[2019-06-12 07:45:00]: bert train_bert_multi_label.py[line:119] INFO  training model....\n",
      "[2019-06-12 07:45:05]: bert utils.py[line:43] INFO  current 2 GPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model summary info: \n",
      "-----------------------------------------------------------------------\n",
      "             Layer (type)                Input Shape         Param #\n",
      "=======================================================================\n",
      "               BertFine-1                  [-1, 512]               0\n",
      "              BertModel-2                  [-1, 512]               0\n",
      "               BertFine-3                  [-1, 512]               0\n",
      "              BertModel-4                  [-1, 512]               0\n",
      "         BertEmbeddings-5                  [-1, 512]               0\n",
      "         BertEmbeddings-6                  [-1, 512]               0\n",
      "              Embedding-7                  [-1, 512]      23,440,896\n",
      "              Embedding-8                  [-1, 512]      23,440,896\n",
      "              Embedding-9                  [-1, 512]         393,216\n",
      "             Embedding-10                  [-1, 512]         393,216\n",
      "             Embedding-11                  [-1, 512]           1,536\n",
      "             Embedding-12                  [-1, 512]           1,536\n",
      "        FusedLayerNorm-13             [-1, 512, 768]           1,536\n",
      "        FusedLayerNorm-14             [-1, 512, 768]           1,536\n",
      "               Dropout-15             [-1, 512, 768]               0\n",
      "               Dropout-16             [-1, 512, 768]               0\n",
      "           BertEncoder-17             [-1, 512, 768]               0\n",
      "             BertLayer-18             [-1, 512, 768]               0\n",
      "         BertAttention-19             [-1, 512, 768]               0\n",
      "     BertSelfAttention-20             [-1, 512, 768]               0\n",
      "                Linear-21             [-1, 512, 768]         590,592\n",
      "           BertEncoder-22             [-1, 512, 768]               0\n",
      "             BertLayer-23             [-1, 512, 768]               0\n",
      "         BertAttention-24             [-1, 512, 768]               0\n",
      "     BertSelfAttention-25             [-1, 512, 768]               0\n",
      "                Linear-26             [-1, 512, 768]         590,592\n",
      "                Linear-27             [-1, 512, 768]         590,592\n",
      "                Linear-28             [-1, 512, 768]         590,592\n",
      "                Linear-29             [-1, 512, 768]         590,592\n",
      "                Linear-30             [-1, 512, 768]         590,592\n",
      "               Dropout-31         [-1, 12, 512, 512]               0\n",
      "               Dropout-32         [-1, 12, 512, 512]               0\n",
      "        BertSelfOutput-33             [-1, 512, 768]               0\n",
      "                Linear-34             [-1, 512, 768]         590,592\n",
      "               Dropout-35             [-1, 512, 768]               0\n",
      "        FusedLayerNorm-36             [-1, 512, 768]           1,536\n",
      "      BertIntermediate-37             [-1, 512, 768]               0\n",
      "                Linear-38             [-1, 512, 768]       2,362,368\n",
      "        BertSelfOutput-39             [-1, 512, 768]               0\n",
      "                Linear-40             [-1, 512, 768]         590,592\n",
      "               Dropout-41             [-1, 512, 768]               0\n",
      "        FusedLayerNorm-42             [-1, 512, 768]           1,536\n",
      "      BertIntermediate-43             [-1, 512, 768]               0\n",
      "                Linear-44             [-1, 512, 768]       2,362,368\n",
      "            BertOutput-45            [-1, 512, 3072]               0\n",
      "                Linear-46            [-1, 512, 3072]       2,360,064\n",
      "               Dropout-47             [-1, 512, 768]               0\n",
      "        FusedLayerNorm-48             [-1, 512, 768]           1,536\n",
      "            BertOutput-49            [-1, 512, 3072]               0\n",
      "                Linear-50            [-1, 512, 3072]       2,360,064\n",
      "             BertLayer-51             [-1, 512, 768]               0\n",
      "         BertAttention-52             [-1, 512, 768]               0\n",
      "     BertSelfAttention-53             [-1, 512, 768]               0\n",
      "                Linear-54             [-1, 512, 768]         590,592\n",
      "               Dropout-55             [-1, 512, 768]               0\n",
      "                Linear-56             [-1, 512, 768]         590,592\n",
      "                Linear-57             [-1, 512, 768]         590,592\n",
      "        FusedLayerNorm-58             [-1, 512, 768]           1,536\n",
      "             BertLayer-59             [-1, 512, 768]               0\n",
      "         BertAttention-60             [-1, 512, 768]               0\n",
      "     BertSelfAttention-61             [-1, 512, 768]               0\n",
      "                Linear-62             [-1, 512, 768]         590,592\n",
      "                Linear-63             [-1, 512, 768]         590,592\n",
      "                Linear-64             [-1, 512, 768]         590,592\n",
      "               Dropout-65         [-1, 12, 512, 512]               0\n",
      "               Dropout-66         [-1, 12, 512, 512]               0\n",
      "        BertSelfOutput-67             [-1, 512, 768]               0\n",
      "                Linear-68             [-1, 512, 768]         590,592\n",
      "        BertSelfOutput-69             [-1, 512, 768]               0\n",
      "                Linear-70             [-1, 512, 768]         590,592\n",
      "               Dropout-71             [-1, 512, 768]               0\n",
      "               Dropout-72             [-1, 512, 768]               0\n",
      "        FusedLayerNorm-73             [-1, 512, 768]           1,536\n",
      "        FusedLayerNorm-74             [-1, 512, 768]           1,536\n",
      "      BertIntermediate-75             [-1, 512, 768]               0\n",
      "                Linear-76             [-1, 512, 768]       2,362,368\n",
      "      BertIntermediate-77             [-1, 512, 768]               0\n",
      "                Linear-78             [-1, 512, 768]       2,362,368\n",
      "            BertOutput-79            [-1, 512, 3072]               0\n",
      "                Linear-80            [-1, 512, 3072]       2,360,064\n",
      "            BertOutput-81            [-1, 512, 3072]               0\n",
      "                Linear-82            [-1, 512, 3072]       2,360,064\n",
      "               Dropout-83             [-1, 512, 768]               0\n",
      "               Dropout-84             [-1, 512, 768]               0\n",
      "        FusedLayerNorm-85             [-1, 512, 768]           1,536\n",
      "        FusedLayerNorm-86             [-1, 512, 768]           1,536\n",
      "             BertLayer-87             [-1, 512, 768]               0\n",
      "         BertAttention-88             [-1, 512, 768]               0\n",
      "     BertSelfAttention-89             [-1, 512, 768]               0\n",
      "                Linear-90             [-1, 512, 768]         590,592\n",
      "             BertLayer-91             [-1, 512, 768]               0\n",
      "         BertAttention-92             [-1, 512, 768]               0\n",
      "     BertSelfAttention-93             [-1, 512, 768]               0\n",
      "                Linear-94             [-1, 512, 768]         590,592\n",
      "                Linear-95             [-1, 512, 768]         590,592\n",
      "                Linear-96             [-1, 512, 768]         590,592\n",
      "                Linear-97             [-1, 512, 768]         590,592\n",
      "                Linear-98             [-1, 512, 768]         590,592\n",
      "               Dropout-99         [-1, 12, 512, 512]               0\n",
      "              Dropout-100         [-1, 12, 512, 512]               0\n",
      "       BertSelfOutput-101             [-1, 512, 768]               0\n",
      "               Linear-102             [-1, 512, 768]         590,592\n",
      "       BertSelfOutput-103             [-1, 512, 768]               0\n",
      "               Linear-104             [-1, 512, 768]         590,592\n",
      "              Dropout-105             [-1, 512, 768]               0\n",
      "              Dropout-106             [-1, 512, 768]               0\n",
      "       FusedLayerNorm-107             [-1, 512, 768]           1,536\n",
      "       FusedLayerNorm-108             [-1, 512, 768]           1,536\n",
      "     BertIntermediate-109             [-1, 512, 768]               0\n",
      "               Linear-110             [-1, 512, 768]       2,362,368\n",
      "     BertIntermediate-111             [-1, 512, 768]               0\n",
      "               Linear-112             [-1, 512, 768]       2,362,368\n",
      "           BertOutput-113            [-1, 512, 3072]               0\n",
      "               Linear-114            [-1, 512, 3072]       2,360,064\n",
      "           BertOutput-115            [-1, 512, 3072]               0\n",
      "               Linear-116            [-1, 512, 3072]       2,360,064\n",
      "              Dropout-117             [-1, 512, 768]               0\n",
      "              Dropout-118             [-1, 512, 768]               0\n",
      "       FusedLayerNorm-119             [-1, 512, 768]           1,536\n",
      "       FusedLayerNorm-120             [-1, 512, 768]           1,536\n",
      "            BertLayer-121             [-1, 512, 768]               0\n",
      "        BertAttention-122             [-1, 512, 768]               0\n",
      "    BertSelfAttention-123             [-1, 512, 768]               0\n",
      "               Linear-124             [-1, 512, 768]         590,592\n",
      "            BertLayer-125             [-1, 512, 768]               0\n",
      "        BertAttention-126             [-1, 512, 768]               0\n",
      "    BertSelfAttention-127             [-1, 512, 768]               0\n",
      "               Linear-128             [-1, 512, 768]         590,592\n",
      "               Linear-129             [-1, 512, 768]         590,592\n",
      "               Linear-130             [-1, 512, 768]         590,592\n",
      "               Linear-131             [-1, 512, 768]         590,592\n",
      "               Linear-132             [-1, 512, 768]         590,592\n",
      "              Dropout-133         [-1, 12, 512, 512]               0\n",
      "              Dropout-134         [-1, 12, 512, 512]               0\n",
      "       BertSelfOutput-135             [-1, 512, 768]               0\n",
      "               Linear-136             [-1, 512, 768]         590,592\n",
      "       BertSelfOutput-137             [-1, 512, 768]               0\n",
      "               Linear-138             [-1, 512, 768]         590,592\n",
      "              Dropout-139             [-1, 512, 768]               0\n",
      "              Dropout-140             [-1, 512, 768]               0\n",
      "       FusedLayerNorm-141             [-1, 512, 768]           1,536\n",
      "       FusedLayerNorm-142             [-1, 512, 768]           1,536\n",
      "     BertIntermediate-143             [-1, 512, 768]               0\n",
      "               Linear-144             [-1, 512, 768]       2,362,368\n",
      "     BertIntermediate-145             [-1, 512, 768]               0\n",
      "               Linear-146             [-1, 512, 768]       2,362,368\n",
      "           BertOutput-147            [-1, 512, 3072]               0\n",
      "               Linear-148            [-1, 512, 3072]       2,360,064\n",
      "           BertOutput-149            [-1, 512, 3072]               0\n",
      "               Linear-150            [-1, 512, 3072]       2,360,064\n",
      "              Dropout-151             [-1, 512, 768]               0\n",
      "              Dropout-152             [-1, 512, 768]               0\n",
      "       FusedLayerNorm-153             [-1, 512, 768]           1,536\n",
      "       FusedLayerNorm-154             [-1, 512, 768]           1,536\n",
      "            BertLayer-155             [-1, 512, 768]               0\n",
      "        BertAttention-156             [-1, 512, 768]               0\n",
      "    BertSelfAttention-157             [-1, 512, 768]               0\n",
      "               Linear-158             [-1, 512, 768]         590,592\n",
      "            BertLayer-159             [-1, 512, 768]               0\n",
      "        BertAttention-160             [-1, 512, 768]               0\n",
      "    BertSelfAttention-161             [-1, 512, 768]               0\n",
      "               Linear-162             [-1, 512, 768]         590,592\n",
      "               Linear-163             [-1, 512, 768]         590,592\n",
      "               Linear-164             [-1, 512, 768]         590,592\n",
      "               Linear-165             [-1, 512, 768]         590,592\n",
      "               Linear-166             [-1, 512, 768]         590,592\n",
      "              Dropout-167         [-1, 12, 512, 512]               0\n",
      "              Dropout-168         [-1, 12, 512, 512]               0\n",
      "       BertSelfOutput-169             [-1, 512, 768]               0\n",
      "               Linear-170             [-1, 512, 768]         590,592\n",
      "       BertSelfOutput-171             [-1, 512, 768]               0\n",
      "               Linear-172             [-1, 512, 768]         590,592\n",
      "              Dropout-173             [-1, 512, 768]               0\n",
      "       FusedLayerNorm-174             [-1, 512, 768]           1,536\n",
      "              Dropout-175             [-1, 512, 768]               0\n",
      "     BertIntermediate-176             [-1, 512, 768]               0\n",
      "               Linear-177             [-1, 512, 768]       2,362,368\n",
      "       FusedLayerNorm-178             [-1, 512, 768]           1,536\n",
      "     BertIntermediate-179             [-1, 512, 768]               0\n",
      "               Linear-180             [-1, 512, 768]       2,362,368\n",
      "           BertOutput-181            [-1, 512, 3072]               0\n",
      "               Linear-182            [-1, 512, 3072]       2,360,064\n",
      "           BertOutput-183            [-1, 512, 3072]               0\n",
      "               Linear-184            [-1, 512, 3072]       2,360,064\n",
      "              Dropout-185             [-1, 512, 768]               0\n",
      "       FusedLayerNorm-186             [-1, 512, 768]           1,536\n",
      "              Dropout-187             [-1, 512, 768]               0\n",
      "            BertLayer-188             [-1, 512, 768]               0\n",
      "        BertAttention-189             [-1, 512, 768]               0\n",
      "    BertSelfAttention-190             [-1, 512, 768]               0\n",
      "               Linear-191             [-1, 512, 768]         590,592\n",
      "       FusedLayerNorm-192             [-1, 512, 768]           1,536\n",
      "            BertLayer-193             [-1, 512, 768]               0\n",
      "        BertAttention-194             [-1, 512, 768]               0\n",
      "    BertSelfAttention-195             [-1, 512, 768]               0\n",
      "               Linear-196             [-1, 512, 768]         590,592\n",
      "               Linear-197             [-1, 512, 768]         590,592\n",
      "               Linear-198             [-1, 512, 768]         590,592\n",
      "               Linear-199             [-1, 512, 768]         590,592\n",
      "               Linear-200             [-1, 512, 768]         590,592\n",
      "              Dropout-201         [-1, 12, 512, 512]               0\n",
      "              Dropout-202         [-1, 12, 512, 512]               0\n",
      "       BertSelfOutput-203             [-1, 512, 768]               0\n",
      "               Linear-204             [-1, 512, 768]         590,592\n",
      "       BertSelfOutput-205             [-1, 512, 768]               0\n",
      "               Linear-206             [-1, 512, 768]         590,592\n",
      "              Dropout-207             [-1, 512, 768]               0\n",
      "              Dropout-208             [-1, 512, 768]               0\n",
      "       FusedLayerNorm-209             [-1, 512, 768]           1,536\n",
      "       FusedLayerNorm-210             [-1, 512, 768]           1,536\n",
      "     BertIntermediate-211             [-1, 512, 768]               0\n",
      "               Linear-212             [-1, 512, 768]       2,362,368\n",
      "     BertIntermediate-213             [-1, 512, 768]               0\n",
      "               Linear-214             [-1, 512, 768]       2,362,368\n",
      "           BertOutput-215            [-1, 512, 3072]               0\n",
      "               Linear-216            [-1, 512, 3072]       2,360,064\n",
      "           BertOutput-217            [-1, 512, 3072]               0\n",
      "               Linear-218            [-1, 512, 3072]       2,360,064\n",
      "              Dropout-219             [-1, 512, 768]               0\n",
      "              Dropout-220             [-1, 512, 768]               0\n",
      "       FusedLayerNorm-221             [-1, 512, 768]           1,536\n",
      "       FusedLayerNorm-222             [-1, 512, 768]           1,536\n",
      "            BertLayer-223             [-1, 512, 768]               0\n",
      "        BertAttention-224             [-1, 512, 768]               0\n",
      "    BertSelfAttention-225             [-1, 512, 768]               0\n",
      "               Linear-226             [-1, 512, 768]         590,592\n",
      "            BertLayer-227             [-1, 512, 768]               0\n",
      "        BertAttention-228             [-1, 512, 768]               0\n",
      "    BertSelfAttention-229             [-1, 512, 768]               0\n",
      "               Linear-230             [-1, 512, 768]         590,592\n",
      "               Linear-231             [-1, 512, 768]         590,592\n",
      "               Linear-232             [-1, 512, 768]         590,592\n",
      "               Linear-233             [-1, 512, 768]         590,592\n",
      "               Linear-234             [-1, 512, 768]         590,592\n",
      "              Dropout-235         [-1, 12, 512, 512]               0\n",
      "              Dropout-236         [-1, 12, 512, 512]               0\n",
      "       BertSelfOutput-237             [-1, 512, 768]               0\n",
      "               Linear-238             [-1, 512, 768]         590,592\n",
      "       BertSelfOutput-239             [-1, 512, 768]               0\n",
      "               Linear-240             [-1, 512, 768]         590,592\n",
      "              Dropout-241             [-1, 512, 768]               0\n",
      "       FusedLayerNorm-242             [-1, 512, 768]           1,536\n",
      "              Dropout-243             [-1, 512, 768]               0\n",
      "     BertIntermediate-244             [-1, 512, 768]               0\n",
      "               Linear-245             [-1, 512, 768]       2,362,368\n",
      "       FusedLayerNorm-246             [-1, 512, 768]           1,536\n",
      "     BertIntermediate-247             [-1, 512, 768]               0\n",
      "               Linear-248             [-1, 512, 768]       2,362,368\n",
      "           BertOutput-249            [-1, 512, 3072]               0\n",
      "               Linear-250            [-1, 512, 3072]       2,360,064\n",
      "           BertOutput-251            [-1, 512, 3072]               0\n",
      "               Linear-252            [-1, 512, 3072]       2,360,064\n",
      "              Dropout-253             [-1, 512, 768]               0\n",
      "              Dropout-254             [-1, 512, 768]               0\n",
      "       FusedLayerNorm-255             [-1, 512, 768]           1,536\n",
      "            BertLayer-256             [-1, 512, 768]               0\n",
      "        BertAttention-257             [-1, 512, 768]               0\n",
      "    BertSelfAttention-258             [-1, 512, 768]               0\n",
      "               Linear-259             [-1, 512, 768]         590,592\n",
      "       FusedLayerNorm-260             [-1, 512, 768]           1,536\n",
      "            BertLayer-261             [-1, 512, 768]               0\n",
      "        BertAttention-262             [-1, 512, 768]               0\n",
      "    BertSelfAttention-263             [-1, 512, 768]               0\n",
      "               Linear-264             [-1, 512, 768]         590,592\n",
      "               Linear-265             [-1, 512, 768]         590,592\n",
      "               Linear-266             [-1, 512, 768]         590,592\n",
      "               Linear-267             [-1, 512, 768]         590,592\n",
      "               Linear-268             [-1, 512, 768]         590,592\n",
      "              Dropout-269         [-1, 12, 512, 512]               0\n",
      "              Dropout-270         [-1, 12, 512, 512]               0\n",
      "       BertSelfOutput-271             [-1, 512, 768]               0\n",
      "               Linear-272             [-1, 512, 768]         590,592\n",
      "       BertSelfOutput-273             [-1, 512, 768]               0\n",
      "               Linear-274             [-1, 512, 768]         590,592\n",
      "              Dropout-275             [-1, 512, 768]               0\n",
      "              Dropout-276             [-1, 512, 768]               0\n",
      "       FusedLayerNorm-277             [-1, 512, 768]           1,536\n",
      "       FusedLayerNorm-278             [-1, 512, 768]           1,536\n",
      "     BertIntermediate-279             [-1, 512, 768]               0\n",
      "               Linear-280             [-1, 512, 768]       2,362,368\n",
      "     BertIntermediate-281             [-1, 512, 768]               0\n",
      "               Linear-282             [-1, 512, 768]       2,362,368\n",
      "           BertOutput-283            [-1, 512, 3072]               0\n",
      "               Linear-284            [-1, 512, 3072]       2,360,064\n",
      "           BertOutput-285            [-1, 512, 3072]               0\n",
      "               Linear-286            [-1, 512, 3072]       2,360,064\n",
      "              Dropout-287             [-1, 512, 768]               0\n",
      "              Dropout-288             [-1, 512, 768]               0\n",
      "       FusedLayerNorm-289             [-1, 512, 768]           1,536\n",
      "       FusedLayerNorm-290             [-1, 512, 768]           1,536\n",
      "            BertLayer-291             [-1, 512, 768]               0\n",
      "        BertAttention-292             [-1, 512, 768]               0\n",
      "    BertSelfAttention-293             [-1, 512, 768]               0\n",
      "               Linear-294             [-1, 512, 768]         590,592\n",
      "            BertLayer-295             [-1, 512, 768]               0\n",
      "        BertAttention-296             [-1, 512, 768]               0\n",
      "    BertSelfAttention-297             [-1, 512, 768]               0\n",
      "               Linear-298             [-1, 512, 768]         590,592\n",
      "               Linear-299             [-1, 512, 768]         590,592\n",
      "               Linear-300             [-1, 512, 768]         590,592\n",
      "               Linear-301             [-1, 512, 768]         590,592\n",
      "               Linear-302             [-1, 512, 768]         590,592\n",
      "              Dropout-303         [-1, 12, 512, 512]               0\n",
      "              Dropout-304         [-1, 12, 512, 512]               0\n",
      "       BertSelfOutput-305             [-1, 512, 768]               0\n",
      "               Linear-306             [-1, 512, 768]         590,592\n",
      "       BertSelfOutput-307             [-1, 512, 768]               0\n",
      "               Linear-308             [-1, 512, 768]         590,592\n",
      "              Dropout-309             [-1, 512, 768]               0\n",
      "              Dropout-310             [-1, 512, 768]               0\n",
      "       FusedLayerNorm-311             [-1, 512, 768]           1,536\n",
      "     BertIntermediate-312             [-1, 512, 768]               0\n",
      "               Linear-313             [-1, 512, 768]       2,362,368\n",
      "       FusedLayerNorm-314             [-1, 512, 768]           1,536\n",
      "     BertIntermediate-315             [-1, 512, 768]               0\n",
      "               Linear-316             [-1, 512, 768]       2,362,368\n",
      "           BertOutput-317            [-1, 512, 3072]               0\n",
      "               Linear-318            [-1, 512, 3072]       2,360,064\n",
      "           BertOutput-319            [-1, 512, 3072]               0\n",
      "               Linear-320            [-1, 512, 3072]       2,360,064\n",
      "              Dropout-321             [-1, 512, 768]               0\n",
      "              Dropout-322             [-1, 512, 768]               0\n",
      "       FusedLayerNorm-323             [-1, 512, 768]           1,536\n",
      "       FusedLayerNorm-324             [-1, 512, 768]           1,536\n",
      "            BertLayer-325             [-1, 512, 768]               0\n",
      "        BertAttention-326             [-1, 512, 768]               0\n",
      "    BertSelfAttention-327             [-1, 512, 768]               0\n",
      "               Linear-328             [-1, 512, 768]         590,592\n",
      "            BertLayer-329             [-1, 512, 768]               0\n",
      "        BertAttention-330             [-1, 512, 768]               0\n",
      "    BertSelfAttention-331             [-1, 512, 768]               0\n",
      "               Linear-332             [-1, 512, 768]         590,592\n",
      "               Linear-333             [-1, 512, 768]         590,592\n",
      "               Linear-334             [-1, 512, 768]         590,592\n",
      "               Linear-335             [-1, 512, 768]         590,592\n",
      "               Linear-336             [-1, 512, 768]         590,592\n",
      "              Dropout-337         [-1, 12, 512, 512]               0\n",
      "              Dropout-338         [-1, 12, 512, 512]               0\n",
      "       BertSelfOutput-339             [-1, 512, 768]               0\n",
      "               Linear-340             [-1, 512, 768]         590,592\n",
      "       BertSelfOutput-341             [-1, 512, 768]               0\n",
      "               Linear-342             [-1, 512, 768]         590,592\n",
      "              Dropout-343             [-1, 512, 768]               0\n",
      "              Dropout-344             [-1, 512, 768]               0\n",
      "       FusedLayerNorm-345             [-1, 512, 768]           1,536\n",
      "       FusedLayerNorm-346             [-1, 512, 768]           1,536\n",
      "     BertIntermediate-347             [-1, 512, 768]               0\n",
      "               Linear-348             [-1, 512, 768]       2,362,368\n",
      "     BertIntermediate-349             [-1, 512, 768]               0\n",
      "               Linear-350             [-1, 512, 768]       2,362,368\n",
      "           BertOutput-351            [-1, 512, 3072]               0\n",
      "               Linear-352            [-1, 512, 3072]       2,360,064\n",
      "           BertOutput-353            [-1, 512, 3072]               0\n",
      "               Linear-354            [-1, 512, 3072]       2,360,064\n",
      "              Dropout-355             [-1, 512, 768]               0\n",
      "              Dropout-356             [-1, 512, 768]               0\n",
      "       FusedLayerNorm-357             [-1, 512, 768]           1,536\n",
      "       FusedLayerNorm-358             [-1, 512, 768]           1,536\n",
      "            BertLayer-359             [-1, 512, 768]               0\n",
      "        BertAttention-360             [-1, 512, 768]               0\n",
      "    BertSelfAttention-361             [-1, 512, 768]               0\n",
      "               Linear-362             [-1, 512, 768]         590,592\n",
      "            BertLayer-363             [-1, 512, 768]               0\n",
      "        BertAttention-364             [-1, 512, 768]               0\n",
      "    BertSelfAttention-365             [-1, 512, 768]               0\n",
      "               Linear-366             [-1, 512, 768]         590,592\n",
      "               Linear-367             [-1, 512, 768]         590,592\n",
      "               Linear-368             [-1, 512, 768]         590,592\n",
      "               Linear-369             [-1, 512, 768]         590,592\n",
      "               Linear-370             [-1, 512, 768]         590,592\n",
      "              Dropout-371         [-1, 12, 512, 512]               0\n",
      "              Dropout-372         [-1, 12, 512, 512]               0\n",
      "       BertSelfOutput-373             [-1, 512, 768]               0\n",
      "               Linear-374             [-1, 512, 768]         590,592\n",
      "       BertSelfOutput-375             [-1, 512, 768]               0\n",
      "               Linear-376             [-1, 512, 768]         590,592\n",
      "              Dropout-377             [-1, 512, 768]               0\n",
      "              Dropout-378             [-1, 512, 768]               0\n",
      "       FusedLayerNorm-379             [-1, 512, 768]           1,536\n",
      "     BertIntermediate-380             [-1, 512, 768]               0\n",
      "               Linear-381             [-1, 512, 768]       2,362,368\n",
      "       FusedLayerNorm-382             [-1, 512, 768]           1,536\n",
      "     BertIntermediate-383             [-1, 512, 768]               0\n",
      "               Linear-384             [-1, 512, 768]       2,362,368\n",
      "           BertOutput-385            [-1, 512, 3072]               0\n",
      "               Linear-386            [-1, 512, 3072]       2,360,064\n",
      "           BertOutput-387            [-1, 512, 3072]               0\n",
      "               Linear-388            [-1, 512, 3072]       2,360,064\n",
      "              Dropout-389             [-1, 512, 768]               0\n",
      "              Dropout-390             [-1, 512, 768]               0\n",
      "       FusedLayerNorm-391             [-1, 512, 768]           1,536\n",
      "       FusedLayerNorm-392             [-1, 512, 768]           1,536\n",
      "            BertLayer-393             [-1, 512, 768]               0\n",
      "        BertAttention-394             [-1, 512, 768]               0\n",
      "    BertSelfAttention-395             [-1, 512, 768]               0\n",
      "               Linear-396             [-1, 512, 768]         590,592\n",
      "            BertLayer-397             [-1, 512, 768]               0\n",
      "        BertAttention-398             [-1, 512, 768]               0\n",
      "    BertSelfAttention-399             [-1, 512, 768]               0\n",
      "               Linear-400             [-1, 512, 768]         590,592\n",
      "               Linear-401             [-1, 512, 768]         590,592\n",
      "               Linear-402             [-1, 512, 768]         590,592\n",
      "               Linear-403             [-1, 512, 768]         590,592\n",
      "               Linear-404             [-1, 512, 768]         590,592\n",
      "              Dropout-405         [-1, 12, 512, 512]               0\n",
      "              Dropout-406         [-1, 12, 512, 512]               0\n",
      "       BertSelfOutput-407             [-1, 512, 768]               0\n",
      "               Linear-408             [-1, 512, 768]         590,592\n",
      "       BertSelfOutput-409             [-1, 512, 768]               0\n",
      "               Linear-410             [-1, 512, 768]         590,592\n",
      "              Dropout-411             [-1, 512, 768]               0\n",
      "              Dropout-412             [-1, 512, 768]               0\n",
      "       FusedLayerNorm-413             [-1, 512, 768]           1,536\n",
      "       FusedLayerNorm-414             [-1, 512, 768]           1,536\n",
      "     BertIntermediate-415             [-1, 512, 768]               0\n",
      "               Linear-416             [-1, 512, 768]       2,362,368\n",
      "     BertIntermediate-417             [-1, 512, 768]               0\n",
      "               Linear-418             [-1, 512, 768]       2,362,368\n",
      "           BertOutput-419            [-1, 512, 3072]               0\n",
      "               Linear-420            [-1, 512, 3072]       2,360,064\n",
      "           BertOutput-421            [-1, 512, 3072]               0\n",
      "               Linear-422            [-1, 512, 3072]       2,360,064\n",
      "              Dropout-423             [-1, 512, 768]               0\n",
      "              Dropout-424             [-1, 512, 768]               0\n",
      "       FusedLayerNorm-425             [-1, 512, 768]           1,536\n",
      "           BertPooler-426             [-1, 512, 768]               0\n",
      "               Linear-427                  [-1, 768]         590,592\n",
      "       FusedLayerNorm-428             [-1, 512, 768]           1,536\n",
      "           BertPooler-429             [-1, 512, 768]               0\n",
      "               Linear-430                  [-1, 768]         590,592\n",
      "                 Tanh-431                  [-1, 768]               0\n",
      "                 Tanh-432                  [-1, 768]               0\n",
      "              Dropout-433                  [-1, 768]               0\n",
      "              Dropout-434                  [-1, 768]               0\n",
      "               Linear-435                  [-1, 768]             769\n",
      "               Linear-436                  [-1, 768]             769\n",
      "=======================================================================\n",
      "Total params: 218,966,018\n",
      "Trainable params: 218,966,018\n",
      "Non-trainable params: 0\n",
      "-----------------------------------------------------------------------\n",
      "--------------------Epoch {epoch}/{self.epochs}------------------------\n",
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>]100% -0.5s/step  accuracy: 0.1538 - loss: 0.7218 \n",
      "------------------------- train result ------------------------------\n",
      "Best thresh: 0.3500 - F1 Score: 0.6683\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-06-12 07:45:52]: bert trainer.py[line:198] INFO  \n",
      "Epoch: 1 -  loss: 0.6779 - f1: 0.6683 - valid_loss: 0.6713 - valid_f1: 0.7042 - valid_threshold_f1: 0.4800 \n",
      "[2019-06-12 07:45:52]: bert modelcheckpoint.py[line:48] INFO  \n",
      "Epoch 1: valid_loss improved from inf to 0.67130\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------------valid result ------------------------------\n",
      "Best thresh: 0.4800 - F1 Score: 0.7042\n",
      "{'valid_loss': 0.671301007270813, 'valid_f1': 0.704225352112676, 'valid_threshold_f1': 0.48}\n",
      "{'loss': 0.6778594255447388, 'f1': 0.6682692307692307, 'valid_loss': 0.671301007270813, 'valid_f1': 0.704225352112676, 'valid_threshold_f1': 0.48}\n",
      "-----------------------------------------------------------------------\n",
      "--------------------Epoch {epoch}/{self.epochs}------------------------\n",
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>]100% -0.5s/step  accuracy: 0.5385 - loss: 0.6860 \n",
      "------------------------- train result ------------------------------\n",
      "Best thresh: 0.4100 - F1 Score: 0.6795\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-06-12 07:46:48]: bert trainer.py[line:198] INFO  \n",
      "Epoch: 2 -  loss: 0.6577 - f1: 0.6795 - valid_loss: 0.5002 - valid_f1: 0.8384 - valid_threshold_f1: 0.5100 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------------valid result ------------------------------\n",
      "Best thresh: 0.5100 - F1 Score: 0.8384\n",
      "{'valid_loss': 0.5001815557479858, 'valid_f1': 0.8383838383838385, 'valid_threshold_f1': 0.51}\n",
      "{'loss': 0.657708466053009, 'f1': 0.6795366795366796, 'valid_loss': 0.5001815557479858, 'valid_f1': 0.8383838383838385, 'valid_threshold_f1': 0.51}\n",
      "-----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-06-12 07:46:49]: bert modelcheckpoint.py[line:48] INFO  \n",
      "Epoch 2: valid_loss improved from 0.67130 to 0.50018\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------Epoch {epoch}/{self.epochs}------------------------\n",
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>]100% -0.5s/step  accuracy: 0.8462 - loss: 0.3781 \n",
      "------------------------- train result ------------------------------\n",
      "Best thresh: 0.3500 - F1 Score: 0.7864\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-06-12 07:47:47]: bert trainer.py[line:198] INFO  \n",
      "Epoch: 3 -  loss: 0.5039 - f1: 0.7864 - valid_loss: 0.4841 - valid_f1: 0.8440 - valid_threshold_f1: 0.5300 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------------valid result ------------------------------\n",
      "Best thresh: 0.5300 - F1 Score: 0.8440\n",
      "{'valid_loss': 0.4840683043003082, 'valid_f1': 0.8440366972477064, 'valid_threshold_f1': 0.53}\n",
      "{'loss': 0.5038641691207886, 'f1': 0.7864406779661016, 'valid_loss': 0.4840683043003082, 'valid_f1': 0.8440366972477064, 'valid_threshold_f1': 0.53}\n",
      "-----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-06-12 07:47:49]: bert modelcheckpoint.py[line:48] INFO  \n",
      "Epoch 3: valid_loss improved from 0.50018 to 0.48407\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------Epoch {epoch}/{self.epochs}------------------------\n",
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>]100% -0.5s/step  accuracy: 0.7692 - loss: 0.5059 \n",
      "------------------------- train result ------------------------------\n",
      "Best thresh: 0.3400 - F1 Score: 0.8541\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-06-12 07:48:55]: bert trainer.py[line:198] INFO  \n",
      "Epoch: 4 -  loss: 0.3959 - f1: 0.8541 - valid_loss: 0.4358 - valid_f1: 0.8063 - valid_threshold_f1: 0.4600 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------------valid result ------------------------------\n",
      "Best thresh: 0.4600 - F1 Score: 0.8063\n",
      "{'valid_loss': 0.43582046031951904, 'valid_f1': 0.8062827225130889, 'valid_threshold_f1': 0.46}\n",
      "{'loss': 0.395877867937088, 'f1': 0.8541300527240773, 'valid_loss': 0.43582046031951904, 'valid_f1': 0.8062827225130889, 'valid_threshold_f1': 0.46}\n",
      "-----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-06-12 07:48:55]: bert modelcheckpoint.py[line:48] INFO  \n",
      "Epoch 4: valid_loss improved from 0.48407 to 0.43582\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------Epoch {epoch}/{self.epochs}------------------------\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 96.00 MiB (GPU 1; 15.89 GiB total capacity; 2.15 GiB already allocated; 56.25 MiB free; 920.50 KiB cached)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/workspace/AA_DPH/bert_recipe_lonepatient/train_bert_multi_label.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/workspace/AA_DPH/bert_recipe_lonepatient/train_bert_multi_label.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_configs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_configs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m     \u001b[0;31m# 拟合模型\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m     \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalid_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalid_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m     \u001b[0;31m# 释放显存\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'n_gpu'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/workspace/AA_DPH/bert_recipe_lonepatient/pybert/train/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, train_data, valid_data)\u001b[0m\n\u001b[1;32m    187\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_epoch\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"--------------------Epoch {epoch}/{self.epochs}------------------------\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 189\u001b[0;31m             \u001b[0mtrain_log\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    190\u001b[0m             \u001b[0mvalid_log\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_valid_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/workspace/AA_DPH/bert_recipe_lonepatient/pybert/train/trainer.py\u001b[0m in \u001b[0;36m_train_epoch\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    126\u001b[0m             \u001b[0msegment_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msegment_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m             \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabel_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m             \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msegment_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minput_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m             \u001b[0;31m# 计算batch loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/parallel/data_parallel.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    141\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m         \u001b[0mreplicas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplicate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallel_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplicas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    144\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/parallel/data_parallel.py\u001b[0m in \u001b[0;36mparallel_apply\u001b[0;34m(self, replicas, inputs, kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mparallel_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplicas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mparallel_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplicas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplicas\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/parallel/parallel_apply.py\u001b[0m in \u001b[0;36mparallel_apply\u001b[0;34m(modules, inputs, kwargs_tup, devices)\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m         \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/parallel/parallel_apply.py\u001b[0m in \u001b[0;36m_worker\u001b[0;34m(i, module, input, kwargs, device)\u001b[0m\n\u001b[1;32m     57\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m                     \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mlock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m                 \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/workspace/AA_DPH/bert_recipe_lonepatient/pybert/model/nn/bert_fine.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, token_type_ids, attention_mask, label_ids, output_all_encoded_layers)\u001b[0m\n\u001b[1;32m     26\u001b[0m                                      \u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m                                      \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                                      output_all_encoded_layers=output_all_encoded_layers)\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0mpooled_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpooled_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpooled_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pytorch_pretrained_bert/modeling.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, token_type_ids, attention_mask, output_all_encoded_layers)\u001b[0m\n\u001b[1;32m    712\u001b[0m         encoded_layers = self.encoder(embedding_output,\n\u001b[1;32m    713\u001b[0m                                       \u001b[0mextended_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 714\u001b[0;31m                                       output_all_encoded_layers=output_all_encoded_layers)\n\u001b[0m\u001b[1;32m    715\u001b[0m         \u001b[0msequence_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoded_layers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    716\u001b[0m         \u001b[0mpooled_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpooler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequence_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pytorch_pretrained_bert/modeling.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, output_all_encoded_layers)\u001b[0m\n\u001b[1;32m    394\u001b[0m         \u001b[0mall_encoder_layers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer_module\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 396\u001b[0;31m             \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    397\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0moutput_all_encoded_layers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m                 \u001b[0mall_encoder_layers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pytorch_pretrained_bert/modeling.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask)\u001b[0m\n\u001b[1;32m    379\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 381\u001b[0;31m         \u001b[0mattention_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    382\u001b[0m         \u001b[0mintermediate_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintermediate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m         \u001b[0mlayer_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mintermediate_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pytorch_pretrained_bert/modeling.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_tensor, attention_mask)\u001b[0m\n\u001b[1;32m    337\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 339\u001b[0;31m         \u001b[0mself_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    340\u001b[0m         \u001b[0mattention_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pytorch_pretrained_bert/modeling.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask)\u001b[0m\n\u001b[1;32m    300\u001b[0m         \u001b[0mattention_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattention_scores\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention_head_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m         \u001b[0;31m# Apply the attention mask is (precomputed for all layers in BertModel forward() function)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 302\u001b[0;31m         \u001b[0mattention_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattention_scores\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m         \u001b[0;31m# Normalize the attention scores to probabilities.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 96.00 MiB (GPU 1; 15.89 GiB total capacity; 2.15 GiB already allocated; 56.25 MiB free; 920.50 KiB cached)"
     ]
    }
   ],
   "source": [
    "### batch size 1000\n",
    "train = \"bert_recipe_lonepatient/train_bert_multi_label.py\"\n",
    "%run $train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-06-12 05:57:18]: bert inference.py[line:23] INFO  seed is 2018\n",
      "[2019-06-12 05:57:18]: bert inference.py[line:26] INFO  starting load data from disk\n",
      "100%|██████████| 248/248 [00:00<00:00, 9463.65it/s]\n",
      "[2019-06-12 05:57:20]: bert inference.py[line:54] INFO  initializing model\n",
      "[2019-06-12 05:57:31]: bert inference.py[line:59] INFO  model predicting....\n",
      "[2019-06-12 05:57:31]: bert utils.py[line:43] INFO  current 2 GPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[predict]1/1[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>]100%F1 0.6773\n",
      "F1 0.9147\n",
      "Best thresh: 0.5600 - F1 Score: 0.9147\n",
      "tensor([127.])\n",
      "F1 0.9147\n"
     ]
    }
   ],
   "source": [
    "# require to re tart to load the model\n",
    "test = \"bert_recipe_lonepatient/inference.py\"\n",
    "%run $test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresh = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-06-12 07:31:23]: bert inference.py[line:25] INFO  seed is 2018\n",
      "[2019-06-12 07:31:23]: bert inference.py[line:28] INFO  starting load data from disk\n",
      "100%|██████████| 248/248 [00:00<00:00, 9816.70it/s]\n",
      "[2019-06-12 07:31:24]: bert inference.py[line:56] INFO  initializing model\n",
      "[2019-06-12 07:31:36]: bert inference.py[line:61] INFO  model predicting....\n",
      "[2019-06-12 07:31:36]: bert utils.py[line:43] INFO  current 2 GPUs\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: out of memory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/workspace/AA_DPH/bert_recipe_lonepatient/inference.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/workspace/AA_DPH/bert_recipe_lonepatient/inference.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     63\u001b[0m                          \u001b[0mlogger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogger\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m                          \u001b[0mn_gpu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'n_gpu'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m                          \u001b[0mmodel_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'output'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'checkpoint_dir'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34mf\"{config['best_model']}_model.pth\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m                          )\n\u001b[1;32m     67\u001b[0m     \u001b[0;31m# 拟合模型\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/workspace/AA_DPH/bert_recipe_lonepatient/pybert/test/predicter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model, logger, n_gpu, model_path)\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwidth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m30\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_gpu\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mn_gpu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogger\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogger\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mloads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_bert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloads\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/workspace/AA_DPH/bert_recipe_lonepatient/pybert/utils/utils.py\u001b[0m in \u001b[0;36mload_bert\u001b[0;34m(model_path, model, optimizer)\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mPath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0mmodel_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m     \u001b[0mcheckpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m     \u001b[0mstate_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheckpoint\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'state_dict'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0;31m# new_state_dict = {}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module)\u001b[0m\n\u001b[1;32m    365\u001b[0m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 367\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    368\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnew_fd\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_load\u001b[0;34m(f, map_location, pickle_module)\u001b[0m\n\u001b[1;32m    536\u001b[0m     \u001b[0munpickler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnpickler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    537\u001b[0m     \u001b[0munpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpersistent_load\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpersistent_load\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 538\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    539\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m     \u001b[0mdeserialized_storage_keys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mpersistent_load\u001b[0;34m(saved_id)\u001b[0m\n\u001b[1;32m    502\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mroot_key\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdeserialized_objects\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    503\u001b[0m                 deserialized_objects[root_key] = restore_location(\n\u001b[0;32m--> 504\u001b[0;31m                     data_type(size), location)\n\u001b[0m\u001b[1;32m    505\u001b[0m             \u001b[0mstorage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeserialized_objects\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mroot_key\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    506\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mview_metadata\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mdefault_restore_location\u001b[0;34m(storage, location)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdefault_restore_location\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_package_registry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_cuda_deserialize\u001b[0;34m(obj, location)\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cuda'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate_cuda_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/_utils.py\u001b[0m in \u001b[0;36m_cuda\u001b[0;34m(self, device, non_blocking, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0mnew_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mnew_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36m_lazy_new\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    494\u001b[0m     \u001b[0;31m# We need this method only for lazy init, so we can remove it\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m     \u001b[0;32mdel\u001b[0m \u001b[0m_CudaBase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__new__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 496\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_CudaBase\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__new__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    497\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    498\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: out of memory"
     ]
    }
   ],
   "source": [
    "# require to re tart to load the model\n",
    "test = \"bert_recipe_lonepatient/inference.py\"\n",
    "%run $test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
