{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Because the field of description looks noisy, we decides to conduct 3 fields generation instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 5.72 Âµs\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "%load_ext autotime\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "\n",
    "# if cannot import the modules, add the parent directory to system path might help\n",
    "\n",
    "import os\n",
    "import sys\n",
    "parent_dir = os.path.abspath(os.getcwd()+'/..')+'/'\n",
    "sys.path.append(parent_dir)\n",
    "\n",
    "from utils.path import dir_HugeFiles\n",
    "from utils.preprocessing import load\n",
    "from sklearn.model_selection import train_test_split\n",
    "from utils.save import make_dir, save_pickle, load_pickle, save\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "from utils.evaluation import spacy_extension\n",
    "import tqdm\n",
    "\n",
    "#dir_save = os.path.normpath(dir_HugeFiles+'dph/dic_20190607.pickle')\n",
    "#dic = load(dir_save)\n",
    "random_seed = 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exist ../big_data/dic_20190927.pickle\n",
      "drop 46 recipes with less than 2 ingredients\n",
      "furthur drop 1026 recipes with less than 2 instructions\n",
      "drop 0 recipes with no description\n",
      "now we are using recipe54k 54076\n",
      "time: 4.7 s\n"
     ]
    }
   ],
   "source": [
    "dic = load(dir_save = '../big_data/dic_20190927.pickle')\n",
    "\n",
    "ls = [i for i,v in dic.items() if len(v['ingredients'])>1]\n",
    "print('drop %d recipes with less than 2 ingredients' %(len(dic)-len(ls)))\n",
    "ls = [i for i in ls if len(dic[i]['directions'])>1]\n",
    "print('furthur drop %d recipes with less than 2 instructions' %(len(dic)-len(ls)))\n",
    "desc = [i for i in ls if len(dic[i]['description'])<1]\n",
    "print('drop %d recipes with no description' %(len(desc)))\n",
    "print('now we are using recipe54k %d' % len(ls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 32.3 ms\n"
     ]
    }
   ],
   "source": [
    "ls = range(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 29.9 ms\n"
     ]
    }
   ],
   "source": [
    "### STEP3 sent to the NYtimes\n",
    "### assign indices to each ingredient <---> NYtimes\n",
    "class ny_ingredients:\n",
    "    def __init__(self, fields):\n",
    "        # this function will take the global variable ls and dic\n",
    "        # static & reuseable\n",
    "        self.ny_ingred = '../../NYtime-parser2/ingred.txt'\n",
    "        self.ny_result = '../../NYtime-parser2/result.json'\n",
    "        \n",
    "        self.fields = fields #['ingredients', 'generated_ingred']\n",
    "        # self.sp = spacy_extension() \n",
    "\n",
    "    def to_ny(self):\n",
    "        '''\n",
    "        using global variables data and ls\n",
    "        '''\n",
    "        to_write = []\n",
    "        for i, v in tqdm.tqdm(enumerate(data)):\n",
    "            # assing index\n",
    "            for field in self.fields:\n",
    "                line_ids = []\n",
    "                for line in v[field]:\n",
    "                    if line in to_write:\n",
    "                        ny_id = to_write.index(line)\n",
    "                    else:\n",
    "                        ny_id = len(to_write)\n",
    "                        to_write.append(line)\n",
    "                    line_ids.append(ny_id)\n",
    "                data[i]['ny_%s'%(field)] = line_ids\n",
    "\n",
    "        # save the file to the folder under NYtime-parser2\n",
    "        save(filename = self.ny_ingred, \n",
    "             to_write = '\\n'.join(to_write),\n",
    "             overwrite = True, \n",
    "             print_=True)\n",
    "\n",
    "        self.to_write = to_write\n",
    "        \n",
    "    # step 3\n",
    "    def to_ingred(self):\n",
    "        '''\n",
    "        using global variables dic and ls\n",
    "        '''\n",
    "        ny_result = pd.read_json(self.ny_result)\n",
    "        to_write = []\n",
    "        for i, v in tqdm.tqdm(enumerate(data)):\n",
    "            if i in ls:\n",
    "                # assing index\n",
    "                for field in self.fields:\n",
    "                    temp = [ny_result.loc[ny_id]['name'] for ny_id in v['ny_%s'%(field)]]\n",
    "                    exact, root = self.extract(temp)\n",
    "                    data[i]['ny_full_%s'%(field)] = ny_result.loc[ny_id].to_dict()\n",
    "                    data[i]['ny_%s'%(field)] = {'ny':temp, 'exact':exact, 'root':root}\n",
    "        # step 3\n",
    "    def to_ingred2(self):\n",
    "        '''\n",
    "        using global variables dic and ls\n",
    "        '''\n",
    "        self.df_result = pd.read_json(self.ny_result)\n",
    "        to_write = []\n",
    "        for i, v in tqdm.tqdm(enumerate(data)):\n",
    "            if i in ls:\n",
    "                # assing index\n",
    "                for field in self.fields:\n",
    "                    data[i]['inv_%s'%(field)] = self.inverse_ingr(v['ny_%s'%(field)])\n",
    "                    \n",
    "    def inverse_ingr(self, x):\n",
    "        '''\n",
    "        x: something like data[0]['ny_ingredients']\n",
    "        '''\n",
    "        output = []\n",
    "        for ny_id in x:   \n",
    "            series = self.df_result.loc[ny_id]\n",
    "            answer = []\n",
    "            if type(series['name'])==str:\n",
    "                answer.append(series['name'])\n",
    "            if type(series['unit'])==str:\n",
    "                answer.append(series['unit'])\n",
    "            if type(series['qty'])==str:\n",
    "                answer.append(series['qty'])\n",
    "            # if type(series['name'])== float: # should change\n",
    "            #    answer.append(series['input'])\n",
    "            answer = ' '.join(answer)\n",
    "            output.append(answer)\n",
    "        return output\n",
    "                    \n",
    "    #def extract(self, ny_ingred):\n",
    "    #    '''\n",
    "    #    Args: ny_ingred: a list of ingredient names\n",
    "    #    '''\n",
    "    #    return self.sp.ny_ingred(ny_ingred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 51.6 ms\n"
     ]
    }
   ],
   "source": [
    "data = [v for i, v in dic.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "55102it [06:31, 140.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved ../../NYtime-parser2/ingred.txt\n",
      "time: 6min 31s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "### start                    \n",
    "ny_ingr = ny_ingredients(fields = ['ingredients'])\n",
    "### step 3-1 save it as ingred.txt\n",
    "ny_ingr.to_ny()\n",
    "### step 3-2 go to python2 and run NLG_notebooks/Control Nytimes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "55102it [03:03, 300.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 3min 4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "### step 3-3 load the result.json back to dic\n",
    "ny_ingr.to_ingred2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 2.17 s\n"
     ]
    }
   ],
   "source": [
    "save_pickle('../big_data/data55_inv.pickle', data, overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 68.4 ms\n"
     ]
    }
   ],
   "source": [
    "def lowerTuples(aTup):\n",
    "    result = ()\n",
    "    for i in aTup:\n",
    "        result += (i.lower(),) \n",
    "    return result\n",
    "\n",
    "def int_to_roman(input):\n",
    "    \"\"\" Convert an integer to a Roman numeral. \"\"\"\n",
    "\n",
    "    if not isinstance(input, type(1)):\n",
    "        raise (TypeError, \"expected integer, got %s\" % type(input))\n",
    "    if not 0 < input < 4000:\n",
    "        raise (ValueError, \"Argument must be between 1 and 3999\")\n",
    "    ints = (1000, 900,  500, 400, 100, 90, 50,  40, 10,  9,   5,  4,   1)\n",
    "    nums = ('M',  'CM', 'D', 'CD','C', 'XC','L','XL','X','IX','V','IV','I')\n",
    "    nums = lowerTuples(nums)\n",
    "    result = []\n",
    "    for i in range(len(ints)):\n",
    "        count = int(input / ints[i])\n",
    "        result.append(nums[i] * count)\n",
    "        input -= ints[i] * count\n",
    "    return ''.join(result)\n",
    "\n",
    "romans = [int_to_roman(i) for i in range(1, 1000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 28.9 ms\n"
     ]
    }
   ],
   "source": [
    "def remove_number(name):\n",
    "    '''\n",
    "    name: data[6008]['name'][0]\n",
    "    '''\n",
    "    return ' '.join([w for w in name[0].split(' ') if w not in romans])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 3.11 s\n"
     ]
    }
   ],
   "source": [
    "for i, v in enumerate(data):\n",
    "    data[i]['title'] = [remove_number(data[i]['name'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['chinese chicken salad iii'] ['chinese chicken salad']\n",
      "['no bake peanut butter cookies ii'] ['no bake peanut butter cookies']\n",
      "time: 59.6 ms\n"
     ]
    }
   ],
   "source": [
    "# check\n",
    "for i in ls:\n",
    "    if 'ii' in data[i]['name'][0] and i <50:\n",
    "        print(data[i]['name'], data[i]['title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 2.22 s\n"
     ]
    }
   ],
   "source": [
    "save_pickle('../big_data/data55_inv.pickle', data, overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 34.9 ms\n"
     ]
    }
   ],
   "source": [
    "def reverse(x): return x\n",
    "def txt(v, fields, mode = 'train'):\n",
    "    '''\n",
    "    fields: an order list, the last is the field to predict\n",
    "    mode: test/train, return string X, y or X+y\n",
    "    '''\n",
    "    to_write = ''\n",
    "    for field in fields:\n",
    "        if field == 'title':\n",
    "            name = [reverse(sent) for sent in v['title']]\n",
    "            to_write += ' <start-title>'+' '.join(name)+' <end-title>'\n",
    "        if field == 'ingredients':\n",
    "            ingredients = [reverse(sent) for sent in v['inv_ingredients']]\n",
    "            to_write += ' <start-ingredients>'+'$'.join(ingredients)+'$ <end-ingredients>'\n",
    "        if field == 'directions':\n",
    "            directions = [reverse(sent) for sent in v['directions']]\n",
    "            to_write += ' <start-directions>'+' '.join(directions)+' <end-directions>'\n",
    "                                                     \n",
    "    if mode == 'train':\n",
    "        return to_write\n",
    "                                                     \n",
    "    elif mode == 'test':\n",
    "        field_to_predict = '<start-%s>'%fields[-1]\n",
    "        to_X, to_y = to_write.split(field_to_predict)\n",
    "        return to_X+field_to_predict, to_y\n",
    "\n",
    "class to_gpt2:\n",
    "    def __init__(self, data, ls):\n",
    "        ls_train, self.ls_test, _, __ = train_test_split(ls, ls, \n",
    "                                                         test_size = 0.2, \n",
    "                                                         random_state = random_seed, \n",
    "                                                         shuffle = True)\n",
    "        \n",
    "        self.ls_train, self.ls_val, _, __ = train_test_split(ls_train, \n",
    "                                                             ls_train, \n",
    "                                                             test_size = 0.25, \n",
    "                                                             random_state = random_seed , \n",
    "                                                             shuffle = True)\n",
    "        self.data = data\n",
    "        \n",
    "    def train(self, ls, filename, overwrite = False, is_val = False):\n",
    "        to_write = ''\n",
    "        for i, v in enumerate(self.data):\n",
    "            if i in ls:      \n",
    "                self.save(filename+'%d'%(i)+'d.txt', txt(v, ['title','ingredients','directions']), overwrite)\n",
    "                self.save(filename+'%d'%(i)+'i.txt', txt(v, ['title','directions','ingredients']), overwrite)\n",
    "                self.save(filename+'%d'%(i)+'t.txt', txt(v, ['ingredients','directions','title']), overwrite)\n",
    "                \n",
    "    def train_reduce(self, ls, filename, overwrite = False, is_val = False):\n",
    "        to_write = ''\n",
    "        for i, v in enumerate(self.data):\n",
    "            if i in ls:      \n",
    "                self.save(filename+'%d'%(i)+'d.txt', txt(v, ['title','ingredients','directions']), overwrite)\n",
    "                \n",
    "    def test(self, ls, filename, overwrite = False, is_val = False):\n",
    "        to_write = ''\n",
    "        for i, v in enumerate(self.data):\n",
    "            if i in ls:\n",
    "                self.save(filename+'X/%d'%(i)+'d.txt', txt(v, ['title','ingredients','directions'], mode = 'test')[0], overwrite)\n",
    "                self.save(filename+'X/%d'%(i)+'i.txt', txt(v, ['title','directions','ingredients'], mode = 'test')[0], overwrite)\n",
    "                self.save(filename+'X/%d'%(i)+'t.txt', txt(v, ['ingredients','directions','title'], mode = 'test')[0], overwrite)\n",
    "                \n",
    "                self.save(filename+'y/%d'%(i)+'d.txt', txt(v, ['title','ingredients','directions'], mode = 'test')[1], overwrite)\n",
    "                self.save(filename+'y/%d'%(i)+'i.txt', txt(v, ['title','directions','ingredients'], mode = 'test')[1], overwrite)\n",
    "                self.save(filename+'y/%d'%(i)+'t.txt', txt(v, ['ingredients','directions','title'], mode = 'test')[1], overwrite)\n",
    "                                \n",
    "    def fast_export(self, filename, overwrite = False):\n",
    "        prefix = 'sample_train/'\n",
    "        self.train_reduce(self.ls_train[:500], filename+prefix, overwrite = overwrite)\n",
    "        prefix = 'sample_val/'\n",
    "        self.train_reduce(self.ls_val[:50], filename+prefix,overwrite = overwrite)\n",
    "        prefix = 'sample_test/'\n",
    "        self.test(self.ls_test[:50], filename+prefix, overwrite = overwrite)\n",
    "        \n",
    "        prefix = 'train/'\n",
    "        self.train_reduce(self.ls_train, filename+prefix, overwrite = overwrite)\n",
    "        prefix = 'val/'\n",
    "        self.train_reduce(self.ls_val, filename+prefix,overwrite = overwrite)\n",
    "        prefix = 'test/'\n",
    "        self.test(self.ls_test, filename+prefix, overwrite = overwrite)\n",
    "        \n",
    "        prefix = 'train_500/'\n",
    "        self.test(self.ls_train[:500], filename+prefix, overwrite = overwrite)\n",
    "        prefix = 'test_500/'\n",
    "        self.test(self.ls_test[:500], filename+prefix, overwrite = overwrite)\n",
    "        \n",
    "    def save(self, filename, to_write, overwrite = False):\n",
    "        make_dir(filename)\n",
    "        if os.path.isfile(filename) == True and overwrite == False:\n",
    "            print('already exists'+filename)\n",
    "        else:    \n",
    "            with open(filename,'w') as f:\n",
    "                f.write('%s' % to_write)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 104 ms\n"
     ]
    }
   ],
   "source": [
    "model = to_gpt2(data, ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "make dir\n",
      "make dir\n",
      "make dir\n",
      "make dir\n",
      "make dir\n",
      "make dir\n",
      "make dir\n",
      "make dir\n",
      "make dir\n",
      "make dir\n",
      "make dir\n",
      "make dir\n",
      "time: 1min 15s\n"
     ]
    }
   ],
   "source": [
    "model.fast_export('../../to_gpt2/recipe54k_1008/',overwrite = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.28 s\n"
     ]
    }
   ],
   "source": [
    "# rm -rf '../../to_gpt2/recipe54k_1008/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Options: <br>\n",
    "1 Zip everything and use scp to transfer the files <br>\n",
    "2 run on DGX server (faster)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
