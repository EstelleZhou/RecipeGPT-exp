{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 5.72 Âµs\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "%load_ext autotime\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "\n",
    "# if cannot import the modules, add the parent directory to system path might help\n",
    "\n",
    "import os, tqdm, sys\n",
    "parent_dir = os.path.abspath(os.getcwd()+'/..')+'/'\n",
    "sys.path.append(parent_dir)\n",
    "\n",
    "from utils.save import make_dir, save_pickle, load_pickle, save\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import tqdm\n",
    "random_seed = 2019\n",
    "\n",
    "data = load_pickle('../big_data/recipe1M_cleaned.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make data loading faster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/yueliu/RecipeAnalytics_201906\n",
      "time: 20.6 ms\n"
     ]
    }
   ],
   "source": [
    "cd /data/yueliu/RecipeAnalytics_201906/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved gpt-2/src/path.py\n",
      "time: 20.7 ms\n"
     ]
    }
   ],
   "source": [
    "import os, importlib\n",
    "save = importlib.import_module(\"gpt-2.src.save\")\n",
    "to_write = \"path = '/data/yueliu/RecipeAnalytics_201906/gpt-2/'\"+'\\n'+\"path_to_model = path + 'models/'\"\n",
    "save.save('gpt-2/src/path.py', to_write, overwrite = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/yueliu/RecipeAnalytics_201906/gpt-2\n",
      "time: 15.9 ms\n"
     ]
    }
   ],
   "source": [
    "cd gpt-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 30.5 ms\n"
     ]
    }
   ],
   "source": [
    "from src import encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 100 ms\n"
     ]
    }
   ],
   "source": [
    "enc = encoder.get_encoder('117M')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 31.2 ms\n"
     ]
    }
   ],
   "source": [
    "def reverse(x): return x\n",
    "def txt(v, fields, mode = 'train'):\n",
    "    '''\n",
    "    fields: an order list, the last is the field to predict\n",
    "    mode: test/train, return string X, y or X+y\n",
    "    '''\n",
    "    to_write = ''\n",
    "    for field in fields:\n",
    "        if field == 'title':\n",
    "            name = reverse(v['title'])\n",
    "            to_write += ' <start-title>'+name+' <end-title>'\n",
    "        if field == 'ingredients':\n",
    "            ingredients = [reverse(sent) for sent in v['ingredients']]\n",
    "            to_write += ' <start-ingredients>'+'$'.join(ingredients)+'$ <end-ingredients>'\n",
    "        if field == 'directions':\n",
    "            directions = [reverse(sent) for sent in v['instructions']]\n",
    "            to_write += ' <start-directions>'+' '.join(directions)+' <end-directions>'\n",
    "            \n",
    "    to_write= to_write.replace('..','.')\n",
    "                                                     \n",
    "    if mode == 'train':\n",
    "        return to_write\n",
    "                                                     \n",
    "    elif mode == 'test':\n",
    "        field_to_predict = '<start-%s>'%fields[-1]\n",
    "        to_X, to_y = to_write.split(field_to_predict)\n",
    "        return to_X+field_to_predict, to_y\n",
    "\n",
    "import random\n",
    "\n",
    "class to_gpt2:\n",
    "    def __init__(self, data, ls = None):\n",
    "        if not ls:\n",
    "            ls = list(range(len(data)))\n",
    "        random.seed(random_seed)\n",
    "        random.shuffle(ls)\n",
    "        self.ls_test = ls[:10000] \n",
    "        self.ls_val = ls[10000:20000]\n",
    "        self.ls_train = ls[20000:]\n",
    "        self.data = data\n",
    "        \n",
    "    def train(self, ls, filename, overwrite = False, is_val = False):\n",
    "        to_write = ''\n",
    "        for i, v in tqdm.tqdm(enumerate(self.data)):\n",
    "            if i in ls:      \n",
    "                self.save(filename+'%d'%(i)+'d.txt', txt(v, ['title','ingredients','directions']), overwrite)\n",
    "                self.save(filename+'%d'%(i)+'i.txt', txt(v, ['title','directions','ingredients']), overwrite)\n",
    "                self.save(filename+'%d'%(i)+'t.txt', txt(v, ['ingredients','directions','title']), overwrite)\n",
    "                \n",
    "    def train_reduce(self, ls, filename, overwrite = False, is_val = False):\n",
    "        to_write = ''\n",
    "        for i, v in tqdm.tqdm(enumerate(self.data)):\n",
    "            if i in ls:      \n",
    "                self.save(filename+'%d'%(i)+'d.txt', txt(v, ['title','ingredients','directions']), overwrite)\n",
    "                \n",
    "    def test(self, ls, filename, overwrite = False, is_val = False):\n",
    "        to_write = ''\n",
    "        for i, v in tqdm.tqdm(enumerate(self.data)):\n",
    "            if i in ls:\n",
    "                self.save(filename+'X/%d'%(i)+'d.txt', txt(v, ['title','ingredients','directions'], mode = 'test')[0], overwrite)\n",
    "                self.save(filename+'X/%d'%(i)+'i.txt', txt(v, ['title','directions','ingredients'], mode = 'test')[0], overwrite)\n",
    "                self.save(filename+'X/%d'%(i)+'t.txt', txt(v, ['ingredients','directions','title'], mode = 'test')[0], overwrite)\n",
    "                \n",
    "                self.save(filename+'y/%d'%(i)+'d.txt', txt(v, ['title','ingredients','directions'], mode = 'test')[1], overwrite)\n",
    "                self.save(filename+'y/%d'%(i)+'i.txt', txt(v, ['title','directions','ingredients'], mode = 'test')[1], overwrite)\n",
    "                self.save(filename+'y/%d'%(i)+'t.txt', txt(v, ['ingredients','directions','title'], mode = 'test')[1], overwrite)\n",
    "        \n",
    "    def save(self, filename, to_write, overwrite = False):\n",
    "        make_dir(filename)\n",
    "        if os.path.isfile(filename) == True and overwrite == False:\n",
    "            print('already exists'+filename)\n",
    "        else:    \n",
    "            with open(filename,'w') as f:\n",
    "                f.write('%s' % to_write)\n",
    "                \n",
    "    def make_chunk(self, ls, filename, tag, overwrite = False):\n",
    "        chunk = []\n",
    "        for i, v in tqdm.tqdm(enumerate(self.data)):\n",
    "            if i in ls:      \n",
    "                chunk.append(self.encode_recipe(self.data[i]))\n",
    "        make_dir(filename)\n",
    "        print(filename+tag)\n",
    "        save_pickle(filename+tag, chunk)\n",
    "        \n",
    "    def encode_recipe(self,recipe):\n",
    "        return enc.encode(txt(recipe, ['title','ingredients','directions']))\n",
    "    \n",
    "    def fast_chunk(self, filename, overwrite = False):\n",
    "        self.make_chunk(self.ls_train, filename, tag = 'chunk.train')\n",
    "        self.make_chunk(self.ls_val, filename, tag = 'chunk.val')\n",
    "        self.make_chunk(self.ls_test, filename, tag = 'chunk.test')\n",
    "        \n",
    "    def fast_export(self, filename, overwrite = False):\n",
    "        prefix = 'sample_train/'\n",
    "        self.train_reduce(self.ls_train[:500], filename+prefix, overwrite = overwrite)\n",
    "        prefix = 'sample_val/'\n",
    "        self.train_reduce(self.ls_val[:50], filename+prefix,overwrite = overwrite)\n",
    "        prefix = 'sample_test/'\n",
    "        self.test(self.ls_test[:50], filename+prefix, overwrite = overwrite)\n",
    "        \n",
    "        prefix = 'test/'\n",
    "        self.test(self.ls_test, filename+prefix, overwrite = overwrite)\n",
    "        \n",
    "        prefix = 'train_500/'\n",
    "        self.test(self.ls_train[:500], filename+prefix, overwrite = overwrite)\n",
    "        prefix = 'test_500/'\n",
    "        self.test(self.ls_test[:500], filename+prefix, overwrite = overwrite)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/data/yueliu/RecipeAnalytics_201906/gpt-2'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 24 ms\n"
     ]
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23801it [00:00, 119205.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "make dir\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1029720it [00:09, 107964.40it/s]\n",
      "163437it [00:00, 814272.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "make dir\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1029720it [00:01, 900503.47it/s]\n",
      "178616it [00:00, 893911.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "make dir\n",
      "make dir\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1029720it [00:01, 932739.47it/s]\n",
      "867it [00:00, 4279.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "make dir\n",
      "make dir\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1029720it [04:33, 3761.81it/s]\n",
      "22959it [00:00, 113963.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "make dir\n",
      "make dir\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1029720it [00:09, 105725.30it/s]\n",
      "23686it [00:00, 118534.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "make dir\n",
      "make dir\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1029720it [00:09, 107209.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 5min 6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model = to_gpt2(data)\n",
    "model.fast_export('../to_gpt2/recipe54k_1010s/', overwrite = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1029720it [14:48, 1158.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../to_gpt2/recipe54k_1010s/chunk.train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1029720it [04:33, 3764.16it/s]\n",
      "425it [00:00, 4240.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../to_gpt2/recipe54k_1010s/chunk.val\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1029720it [04:34, 3757.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../to_gpt2/recipe54k_1010s/chunk.test\n",
      "time: 23min 56s\n"
     ]
    }
   ],
   "source": [
    "filename = '../to_gpt2/recipe54k_1010s/'\n",
    "model.make_chunk(model.ls_train[:32400], filename, tag = 'chunk.train')\n",
    "model.make_chunk(model.ls_val, filename, tag = 'chunk.val')\n",
    "model.make_chunk(model.ls_test, filename, tag = 'chunk.test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
