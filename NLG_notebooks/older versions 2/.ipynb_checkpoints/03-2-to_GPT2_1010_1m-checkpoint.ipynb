{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 11 Âµs\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "%load_ext autotime\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "\n",
    "# if cannot import the modules, add the parent directory to system path might help\n",
    "\n",
    "import os, tqdm, sys\n",
    "parent_dir = os.path.abspath(os.getcwd()+'/..')+'/'\n",
    "sys.path.append(parent_dir)\n",
    "\n",
    "from utils.save import make_dir, save_pickle, load_pickle, save\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import tqdm\n",
    "random_seed = 2019\n",
    "\n",
    "data = load_pickle('../big_data/recipe1M_cleaned.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### old version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 68.9 ms\n"
     ]
    }
   ],
   "source": [
    "def reverse(x): return x\n",
    "def txt(v, fields, mode = 'train'):\n",
    "    '''\n",
    "    fields: an order list, the last is the field to predict\n",
    "    mode: test/train, return string X, y or X+y\n",
    "    '''\n",
    "    to_write = ''\n",
    "    for field in fields:\n",
    "        if field == 'title':\n",
    "            name = reverse(v['title'])\n",
    "            to_write += ' <start-title>'+name+' <end-title>'\n",
    "        if field == 'ingredients':\n",
    "            ingredients = [reverse(sent) for sent in v['ingredients']]\n",
    "            to_write += ' <start-ingredients>'+'$'.join(ingredients)+'$ <end-ingredients>'\n",
    "        if field == 'directions':\n",
    "            directions = [reverse(sent) for sent in v['instructions']]\n",
    "            to_write += ' <start-directions>'+' '.join(directions)+' <end-directions>'\n",
    "                                                     \n",
    "    if mode == 'train':\n",
    "        return to_write\n",
    "                                                     \n",
    "    elif mode == 'test':\n",
    "        field_to_predict = '<start-%s>'%fields[-1]\n",
    "        to_X, to_y = to_write.split(field_to_predict)\n",
    "        return to_X+field_to_predict, to_y\n",
    "\n",
    "class to_gpt2:\n",
    "    def __init__(self, data, ls):\n",
    "        ls_train, self.ls_test, _, __ = train_test_split(ls, ls, \n",
    "                                                         test_size = 0.2, \n",
    "                                                         random_state = random_seed, \n",
    "                                                         shuffle = True)\n",
    "        \n",
    "        self.ls_train, self.ls_val, _, __ = train_test_split(ls_train, \n",
    "                                                             ls_train, \n",
    "                                                             test_size = 0.25, \n",
    "                                                             random_state = random_seed , \n",
    "                                                             shuffle = True)\n",
    "        self.data = data\n",
    "        \n",
    "    def train(self, ls, filename, overwrite = False, is_val = False):\n",
    "        to_write = ''\n",
    "        for i, v in enumerate(self.data):\n",
    "            if i in ls:      \n",
    "                self.save(filename+'%d'%(i)+'d.txt', txt(v, ['title','ingredients','directions']), overwrite)\n",
    "                self.save(filename+'%d'%(i)+'i.txt', txt(v, ['title','directions','ingredients']), overwrite)\n",
    "                self.save(filename+'%d'%(i)+'t.txt', txt(v, ['ingredients','directions','title']), overwrite)\n",
    "                \n",
    "    def train_reduce(self, ls, filename, overwrite = False, is_val = False):\n",
    "        to_write = ''\n",
    "        for i, v in enumerate(self.data):\n",
    "            if i in ls:      \n",
    "                self.save(filename+'%d'%(i)+'d.txt', txt(v, ['title','ingredients','directions']), overwrite)\n",
    "                \n",
    "    def test(self, ls, filename, overwrite = False, is_val = False):\n",
    "        to_write = ''\n",
    "        for i, v in enumerate(self.data):\n",
    "            if i in ls:\n",
    "                self.save(filename+'X/%d'%(i)+'d.txt', txt(v, ['title','ingredients','directions'], mode = 'test')[0], overwrite)\n",
    "                self.save(filename+'X/%d'%(i)+'i.txt', txt(v, ['title','directions','ingredients'], mode = 'test')[0], overwrite)\n",
    "                self.save(filename+'X/%d'%(i)+'t.txt', txt(v, ['ingredients','directions','title'], mode = 'test')[0], overwrite)\n",
    "                \n",
    "                self.save(filename+'y/%d'%(i)+'d.txt', txt(v, ['title','ingredients','directions'], mode = 'test')[1], overwrite)\n",
    "                self.save(filename+'y/%d'%(i)+'i.txt', txt(v, ['title','directions','ingredients'], mode = 'test')[1], overwrite)\n",
    "                self.save(filename+'y/%d'%(i)+'t.txt', txt(v, ['ingredients','directions','title'], mode = 'test')[1], overwrite)\n",
    "                                \n",
    "    def fast_export(self, filename, overwrite = False):\n",
    "        prefix = 'sample_train/'\n",
    "        self.train_reduce(self.ls_train[:500], filename+prefix, overwrite = overwrite)\n",
    "        prefix = 'sample_val/'\n",
    "        self.train_reduce(self.ls_val[:50], filename+prefix,overwrite = overwrite)\n",
    "        prefix = 'sample_test/'\n",
    "        self.test(self.ls_test[:50], filename+prefix, overwrite = overwrite)\n",
    "        \n",
    "        prefix = 'train/'\n",
    "        self.train_reduce(self.ls_train, filename+prefix, overwrite = overwrite)\n",
    "        prefix = 'val/'\n",
    "        self.train_reduce(self.ls_val, filename+prefix,overwrite = overwrite)\n",
    "        prefix = 'test/'\n",
    "        self.test(self.ls_test, filename+prefix, overwrite = overwrite)\n",
    "        \n",
    "        prefix = 'train_500/'\n",
    "        self.test(self.ls_train[:500], filename+prefix, overwrite = overwrite)\n",
    "        prefix = 'test_500/'\n",
    "        self.test(self.ls_test[:500], filename+prefix, overwrite = overwrite)\n",
    "        \n",
    "    def save(self, filename, to_write, overwrite = False):\n",
    "        make_dir(filename)\n",
    "        if os.path.isfile(filename) == True and overwrite == False:\n",
    "            print('already exists'+filename)\n",
    "        else:    \n",
    "            with open(filename,'w') as f:\n",
    "                f.write('%s' % to_write)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### export a smaller version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 24.3 ms\n"
     ]
    }
   ],
   "source": [
    "ls = range(len(data[:54000]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 63.6 ms\n"
     ]
    }
   ],
   "source": [
    "model = to_gpt2(data[:54000], ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "make dir\n",
      "make dir\n",
      "make dir\n",
      "make dir\n",
      "make dir\n",
      "make dir\n",
      "make dir\n",
      "make dir\n",
      "make dir\n",
      "make dir\n",
      "make dir\n",
      "make dir\n",
      "time: 3.98 s\n"
     ]
    }
   ],
   "source": [
    "model.fast_export('../../to_gpt2/recipe54k_1010s/',overwrite = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 31.4 ms\n"
     ]
    }
   ],
   "source": [
    "ls = range(100)\n",
    "y, x, _, __ = train_test_split(ls, ls, test_size = 0.2, random_state = random_seed, shuffle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### export and a full version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls = range(len(data))\n",
    "model = to_gpt2(data, ls)\n",
    "model.fast_export('../../to_gpt2/recipe54k_1010/',overwrite = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make data loading faster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/yueliu/RecipeAnalytics_201906\n",
      "time: 17.5 ms\n"
     ]
    }
   ],
   "source": [
    "cd /data/yueliu/RecipeAnalytics_201906/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved gpt-2/src/path.py\n",
      "time: 29.5 ms\n"
     ]
    }
   ],
   "source": [
    "import os, importlib\n",
    "save = importlib.import_module(\"gpt-2.src.save\")\n",
    "to_write = \"path = '/data/yueliu/RecipeAnalytics_201906/gpt-2/'\"+'\\n'+\"path_to_model = path + 'models/'\"\n",
    "save.save('gpt-2/src/path.py', to_write, overwrite = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/yueliu/RecipeAnalytics_201906/gpt-2\n",
      "time: 27.3 ms\n"
     ]
    }
   ],
   "source": [
    "cd gpt-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 50.4 ms\n"
     ]
    }
   ],
   "source": [
    "from src import encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 106 ms\n"
     ]
    }
   ],
   "source": [
    "enc = encoder.get_encoder('117M')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 64.7 ms\n"
     ]
    }
   ],
   "source": [
    "def reverse(x): return x\n",
    "def txt(v, fields, mode = 'train'):\n",
    "    '''\n",
    "    fields: an order list, the last is the field to predict\n",
    "    mode: test/train, return string X, y or X+y\n",
    "    '''\n",
    "    to_write = ''\n",
    "    for field in fields:\n",
    "        if field == 'title':\n",
    "            name = reverse(v['title'])\n",
    "            to_write += ' <start-title>'+name+' <end-title>'\n",
    "        if field == 'ingredients':\n",
    "            ingredients = [reverse(sent) for sent in v['ingredients']]\n",
    "            to_write += ' <start-ingredients>'+'$'.join(ingredients)+'$ <end-ingredients>'\n",
    "        if field == 'directions':\n",
    "            directions = [reverse(sent) for sent in v['instructions']]\n",
    "            to_write += ' <start-directions>'+' '.join(directions)+' <end-directions>'\n",
    "            \n",
    "    to_write= to_write.replace('..','.')\n",
    "                                                     \n",
    "    if mode == 'train':\n",
    "        return to_write\n",
    "                                                     \n",
    "    elif mode == 'test':\n",
    "        field_to_predict = '<start-%s>'%fields[-1]\n",
    "        to_X, to_y = to_write.split(field_to_predict)\n",
    "        return to_X+field_to_predict, to_y\n",
    "\n",
    "import random\n",
    "\n",
    "class to_gpt2:\n",
    "    def __init__(self, data, ls = None):\n",
    "        if not ls:\n",
    "            ls = list(range(len(data)))\n",
    "        random.seed(random_seed)\n",
    "        random.shuffle(ls)\n",
    "        self.ls_test = ls[:10000] \n",
    "        self.ls_val = ls[10000:20000]\n",
    "        self.ls_train = ls[20000:]\n",
    "        self.data = data\n",
    "        \n",
    "    def train(self, ls, filename, overwrite = False, is_val = False):\n",
    "        to_write = ''\n",
    "        for i, v in tqdm.tqdm(enumerate(self.data)):\n",
    "            if i in ls:      \n",
    "                self.save(filename+'%d'%(i)+'d.txt', txt(v, ['title','ingredients','directions']), overwrite)\n",
    "                self.save(filename+'%d'%(i)+'i.txt', txt(v, ['title','directions','ingredients']), overwrite)\n",
    "                self.save(filename+'%d'%(i)+'t.txt', txt(v, ['ingredients','directions','title']), overwrite)\n",
    "                \n",
    "    def train_reduce(self, ls, filename, overwrite = False, is_val = False):\n",
    "        to_write = ''\n",
    "        for i, v in tqdm.tqdm(enumerate(self.data)):\n",
    "            if i in ls:      \n",
    "                self.save(filename+'%d'%(i)+'d.txt', txt(v, ['title','ingredients','directions']), overwrite)\n",
    "                \n",
    "    def test(self, ls, filename, overwrite = False, is_val = False):\n",
    "        to_write = ''\n",
    "        for i, v in tqdm.tqdm(enumerate(self.data)):\n",
    "            if i in ls:\n",
    "                self.save(filename+'X/%d'%(i)+'d.txt', txt(v, ['title','ingredients','directions'], mode = 'test')[0], overwrite)\n",
    "                self.save(filename+'X/%d'%(i)+'i.txt', txt(v, ['title','directions','ingredients'], mode = 'test')[0], overwrite)\n",
    "                self.save(filename+'X/%d'%(i)+'t.txt', txt(v, ['ingredients','directions','title'], mode = 'test')[0], overwrite)\n",
    "                \n",
    "                self.save(filename+'y/%d'%(i)+'d.txt', txt(v, ['title','ingredients','directions'], mode = 'test')[1], overwrite)\n",
    "                self.save(filename+'y/%d'%(i)+'i.txt', txt(v, ['title','directions','ingredients'], mode = 'test')[1], overwrite)\n",
    "                self.save(filename+'y/%d'%(i)+'t.txt', txt(v, ['ingredients','directions','title'], mode = 'test')[1], overwrite)\n",
    "        \n",
    "    def save(self, filename, to_write, overwrite = False):\n",
    "        make_dir(filename)\n",
    "        if os.path.isfile(filename) == True and overwrite == False:\n",
    "            print('already exists'+filename)\n",
    "        else:    \n",
    "            with open(filename,'w') as f:\n",
    "                f.write('%s' % to_write)\n",
    "                \n",
    "    def make_chunk(self, ls, filename, tag, overwrite = False):\n",
    "        chunk = []\n",
    "        for i, v in tqdm.tqdm(enumerate(self.data)):\n",
    "            if i in ls:      \n",
    "                chunk.append(self.encode_recipe(self.data[i]))\n",
    "        make_dir(filename)\n",
    "        print(filename+tag)\n",
    "        save_pickle(filename+tag, chunk)\n",
    "        \n",
    "    def encode_recipe(self,recipe):\n",
    "        return enc.encode(txt(recipe, ['title','ingredients','directions']))\n",
    "    \n",
    "    def fast_chunk(self, filename, overwrite = False):\n",
    "        self.make_chunk(self.ls_train, filename, tag = 'chunk.train')\n",
    "        self.make_chunk(self.ls_val, filename, tag = 'chunk.val')\n",
    "        self.make_chunk(self.ls_test, filename, tag = 'chunk.test')\n",
    "        \n",
    "    def fast_export(self, filename, overwrite = False):\n",
    "        prefix = 'sample_train/'\n",
    "        self.train_reduce(self.ls_train[:500], filename+prefix, overwrite = overwrite)\n",
    "        prefix = 'sample_val/'\n",
    "        self.train_reduce(self.ls_val[:50], filename+prefix,overwrite = overwrite)\n",
    "        prefix = 'sample_test/'\n",
    "        self.test(self.ls_test[:50], filename+prefix, overwrite = overwrite)\n",
    "        \n",
    "        prefix = 'test/'\n",
    "        self.test(self.ls_test, filename+prefix, overwrite = overwrite)\n",
    "        \n",
    "        prefix = 'train_500/'\n",
    "        self.test(self.ls_train[:500], filename+prefix, overwrite = overwrite)\n",
    "        prefix = 'test_500/'\n",
    "        self.test(self.ls_test[:500], filename+prefix, overwrite = overwrite)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/data/yueliu/RecipeAnalytics_201906/gpt-2'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 48.7 ms\n"
     ]
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1029720it [9:16:37, 30.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "make dir\n",
      "../to_gpt2/recipe54k_1010/chunk.train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1029720it [04:36, 3724.34it/s]\n",
      "423it [00:00, 4222.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../to_gpt2/recipe54k_1010/chunk.val\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1029720it [04:34, 3754.54it/s]\n",
      "12330it [00:00, 123290.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../to_gpt2/recipe54k_1010/chunk.test\n",
      "make dir\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1029720it [00:09, 110822.18it/s]\n",
      "177021it [00:00, 886939.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "make dir\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1029720it [00:01, 928635.78it/s]\n",
      "178120it [00:00, 888856.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "make dir\n",
      "make dir\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1029720it [00:01, 906957.67it/s]\n",
      "915it [00:00, 4550.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "make dir\n",
      "make dir\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1029720it [04:31, 3793.85it/s]\n",
      "20535it [00:00, 93242.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "make dir\n",
      "make dir\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1029720it [00:09, 105602.90it/s]\n",
      "25398it [00:00, 127010.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "make dir\n",
      "make dir\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1029720it [00:09, 107800.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 9h 31min 1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model = to_gpt2(data)\n",
    "model.fast_chunk('../to_gpt2/recipe1M_1010/', overwrite = True)\n",
    "model.fast_export('../to_gpt2/recipe1M_1010/', overwrite = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
